{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 75 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 07:24:05.867321: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 07:24:05.898426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 07:24:05.898455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 07:24:05.899512: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 07:24:05.904863: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 07:24:05.905723: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 07:24:09.027425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files2/S001R04.edf',\n",
       " 'files2/S002R04.edf',\n",
       " 'files2/S003R04.edf',\n",
       " 'files2/S004R04.edf',\n",
       " 'files2/S005R04.edf',\n",
       " 'files2/S006R04.edf',\n",
       " 'files2/S007R04.edf',\n",
       " 'files2/S008R04.edf',\n",
       " 'files2/S009R04.edf',\n",
       " 'files2/S010R04.edf',\n",
       " 'files2/S011R04.edf',\n",
       " 'files2/S012R04.edf',\n",
       " 'files2/S013R04.edf',\n",
       " 'files2/S014R04.edf',\n",
       " 'files2/S015R04.edf',\n",
       " 'files2/S016R04.edf',\n",
       " 'files2/S017R04.edf',\n",
       " 'files2/S018R04.edf',\n",
       " 'files2/S019R04.edf',\n",
       " 'files2/S020R04.edf',\n",
       " 'files2/S021R04.edf',\n",
       " 'files2/S022R04.edf',\n",
       " 'files2/S023R04.edf',\n",
       " 'files2/S024R04.edf',\n",
       " 'files2/S025R04.edf',\n",
       " 'files2/S026R04.edf',\n",
       " 'files2/S027R04.edf',\n",
       " 'files2/S028R04.edf',\n",
       " 'files2/S029R04.edf',\n",
       " 'files2/S030R04.edf',\n",
       " 'files2/S031R04.edf',\n",
       " 'files2/S032R04.edf',\n",
       " 'files2/S033R04.edf',\n",
       " 'files2/S034R04.edf',\n",
       " 'files2/S035R04.edf',\n",
       " 'files2/S036R04.edf',\n",
       " 'files2/S037R04.edf',\n",
       " 'files2/S038R04.edf',\n",
       " 'files2/S039R04.edf',\n",
       " 'files2/S040R04.edf',\n",
       " 'files2/S041R04.edf',\n",
       " 'files2/S042R04.edf',\n",
       " 'files2/S043R04.edf',\n",
       " 'files2/S044R04.edf',\n",
       " 'files2/S045R04.edf',\n",
       " 'files2/S046R04.edf',\n",
       " 'files2/S047R04.edf',\n",
       " 'files2/S048R04.edf',\n",
       " 'files2/S049R04.edf',\n",
       " 'files2/S050R04.edf',\n",
       " 'files2/S051R04.edf',\n",
       " 'files2/S052R04.edf',\n",
       " 'files2/S053R04.edf',\n",
       " 'files2/S054R04.edf',\n",
       " 'files2/S055R04.edf',\n",
       " 'files2/S056R04.edf',\n",
       " 'files2/S057R04.edf',\n",
       " 'files2/S058R04.edf',\n",
       " 'files2/S059R04.edf',\n",
       " 'files2/S060R04.edf',\n",
       " 'files2/S061R04.edf',\n",
       " 'files2/S062R04.edf',\n",
       " 'files2/S063R04.edf',\n",
       " 'files2/S064R04.edf',\n",
       " 'files2/S065R04.edf',\n",
       " 'files2/S066R04.edf',\n",
       " 'files2/S067R04.edf',\n",
       " 'files2/S068R04.edf',\n",
       " 'files2/S069R04.edf',\n",
       " 'files2/S070R04.edf',\n",
       " 'files2/S071R04.edf',\n",
       " 'files2/S072R04.edf',\n",
       " 'files2/S073R04.edf',\n",
       " 'files2/S074R04.edf',\n",
       " 'files2/S075R04.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files2/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ca7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 112425, 900000, 112425)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0e-05  1.5e-05  6.0e-06 ... -1.6e-05 -1.3e-05 -6.0e-06]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1       -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2       -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3       -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4       -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000055   0.000013  -0.000036  -0.000092  -0.000044  -0.000073   \n",
       "899996  -0.000050  -0.000065  -0.000089  -0.000119  -0.000064  -0.000116   \n",
       "899997   0.000001  -0.000021  -0.000052  -0.000108  -0.000053  -0.000102   \n",
       "899998   0.000011   0.000051   0.000003  -0.000079  -0.000023  -0.000072   \n",
       "899999   0.000031   0.000022  -0.000004  -0.000061   0.000002  -0.000045   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1       -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2       -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3       -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4       -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000019   0.000028   0.000032  -0.000043  ...    0.000005   \n",
       "899996  -0.000026  -0.000044  -0.000070  -0.000080  ...   -0.000007   \n",
       "899997  -0.000020  -0.000034  -0.000003  -0.000039  ...   -0.000044   \n",
       "899998   0.000031  -0.000010   0.000079   0.000008  ...   -0.000064   \n",
       "899999   0.000037   0.000059   0.000023   0.000003  ...   -0.000048   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1        -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2        -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3        -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4        -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995    0.000019   -0.000023   -0.000062   -0.000021   -0.000005   \n",
       "899996   -0.000014   -0.000038   -0.000075   -0.000048   -0.000031   \n",
       "899997    0.000030    0.000022   -0.000021   -0.000030   -0.000039   \n",
       "899998    0.000006    0.000007   -0.000009   -0.000026   -0.000063   \n",
       "899999    0.000002    0.000010    0.000023    0.000015   -0.000032   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1        -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2        -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3        -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4        -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000022    0.000022   -0.000018    0.000014  \n",
       "899996   -0.000008    0.000012   -0.000035   -0.000030  \n",
       "899997    0.000026    0.000058    0.000017    0.000050  \n",
       "899998    0.000028    0.000044   -0.000009    0.000039  \n",
       "899999    0.000033    0.000045    0.000033    0.000042  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78467/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_78467/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_78467/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1       -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2       -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3       -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4       -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000055   0.000013  -0.000036  -0.000092  -0.000044  -0.000073   \n",
       "899996  -0.000050  -0.000065  -0.000089  -0.000119  -0.000064  -0.000116   \n",
       "899997   0.000001  -0.000021  -0.000052  -0.000108  -0.000053  -0.000102   \n",
       "899998   0.000011   0.000051   0.000003  -0.000079  -0.000023  -0.000072   \n",
       "899999   0.000031   0.000022  -0.000004  -0.000061   0.000002  -0.000045   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1       -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2       -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3       -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4       -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000019   0.000028   0.000032  -0.000043  ...    0.000005   \n",
       "899996  -0.000026  -0.000044  -0.000070  -0.000080  ...   -0.000007   \n",
       "899997  -0.000020  -0.000034  -0.000003  -0.000039  ...   -0.000044   \n",
       "899998   0.000031  -0.000010   0.000079   0.000008  ...   -0.000064   \n",
       "899999   0.000037   0.000059   0.000023   0.000003  ...   -0.000048   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1        -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2        -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3        -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4        -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995    0.000019   -0.000023   -0.000062   -0.000021   -0.000005   \n",
       "899996   -0.000014   -0.000038   -0.000075   -0.000048   -0.000031   \n",
       "899997    0.000030    0.000022   -0.000021   -0.000030   -0.000039   \n",
       "899998    0.000006    0.000007   -0.000009   -0.000026   -0.000063   \n",
       "899999    0.000002    0.000010    0.000023    0.000015   -0.000032   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1        -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2        -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3        -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4        -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000022    0.000022   -0.000018    0.000014  \n",
       "899996   -0.000008    0.000012   -0.000035   -0.000030  \n",
       "899997    0.000026    0.000058    0.000017    0.000050  \n",
       "899998    0.000028    0.000044   -0.000009    0.000039  \n",
       "899999    0.000033    0.000045    0.000033    0.000042  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:4.03047\n",
      "[1]\tvalidation_0-mlogloss:3.91937\n",
      "[2]\tvalidation_0-mlogloss:3.82976\n",
      "[3]\tvalidation_0-mlogloss:3.76401\n",
      "[4]\tvalidation_0-mlogloss:3.70940\n",
      "[5]\tvalidation_0-mlogloss:3.66650\n",
      "[6]\tvalidation_0-mlogloss:3.62728\n",
      "[7]\tvalidation_0-mlogloss:3.59370\n",
      "[8]\tvalidation_0-mlogloss:3.56536\n",
      "[9]\tvalidation_0-mlogloss:3.54245\n",
      "[10]\tvalidation_0-mlogloss:3.52024\n",
      "[11]\tvalidation_0-mlogloss:3.50060\n",
      "[12]\tvalidation_0-mlogloss:3.48300\n",
      "[13]\tvalidation_0-mlogloss:3.46688\n",
      "[14]\tvalidation_0-mlogloss:3.45414\n",
      "[15]\tvalidation_0-mlogloss:3.43916\n",
      "[16]\tvalidation_0-mlogloss:3.42744\n",
      "[17]\tvalidation_0-mlogloss:3.41602\n",
      "[18]\tvalidation_0-mlogloss:3.40528\n",
      "[19]\tvalidation_0-mlogloss:3.39415\n",
      "[20]\tvalidation_0-mlogloss:3.38442\n",
      "[21]\tvalidation_0-mlogloss:3.37375\n",
      "[22]\tvalidation_0-mlogloss:3.36319\n",
      "[23]\tvalidation_0-mlogloss:3.35296\n",
      "[24]\tvalidation_0-mlogloss:3.34590\n",
      "[25]\tvalidation_0-mlogloss:3.33921\n",
      "[26]\tvalidation_0-mlogloss:3.32994\n",
      "[27]\tvalidation_0-mlogloss:3.32020\n",
      "[28]\tvalidation_0-mlogloss:3.31245\n",
      "[29]\tvalidation_0-mlogloss:3.30462\n",
      "[30]\tvalidation_0-mlogloss:3.29691\n",
      "[31]\tvalidation_0-mlogloss:3.28908\n",
      "[32]\tvalidation_0-mlogloss:3.28195\n",
      "[33]\tvalidation_0-mlogloss:3.27413\n",
      "[34]\tvalidation_0-mlogloss:3.26928\n",
      "[35]\tvalidation_0-mlogloss:3.26371\n",
      "[36]\tvalidation_0-mlogloss:3.25808\n",
      "[37]\tvalidation_0-mlogloss:3.25051\n",
      "[38]\tvalidation_0-mlogloss:3.24527\n",
      "[39]\tvalidation_0-mlogloss:3.23744\n",
      "[40]\tvalidation_0-mlogloss:3.23155\n",
      "[41]\tvalidation_0-mlogloss:3.22450\n",
      "[42]\tvalidation_0-mlogloss:3.21973\n",
      "[43]\tvalidation_0-mlogloss:3.21585\n",
      "[44]\tvalidation_0-mlogloss:3.20971\n",
      "[45]\tvalidation_0-mlogloss:3.20515\n",
      "[46]\tvalidation_0-mlogloss:3.19915\n",
      "[47]\tvalidation_0-mlogloss:3.19374\n",
      "[48]\tvalidation_0-mlogloss:3.18878\n",
      "[49]\tvalidation_0-mlogloss:3.18364\n",
      "[50]\tvalidation_0-mlogloss:3.17943\n",
      "[51]\tvalidation_0-mlogloss:3.17406\n",
      "[52]\tvalidation_0-mlogloss:3.16967\n",
      "[53]\tvalidation_0-mlogloss:3.16478\n",
      "[54]\tvalidation_0-mlogloss:3.16015\n",
      "[55]\tvalidation_0-mlogloss:3.15523\n",
      "[56]\tvalidation_0-mlogloss:3.15186\n",
      "[57]\tvalidation_0-mlogloss:3.14878\n",
      "[58]\tvalidation_0-mlogloss:3.14530\n",
      "[59]\tvalidation_0-mlogloss:3.14200\n",
      "[60]\tvalidation_0-mlogloss:3.13764\n",
      "[61]\tvalidation_0-mlogloss:3.13377\n",
      "[62]\tvalidation_0-mlogloss:3.13099\n",
      "[63]\tvalidation_0-mlogloss:3.12652\n",
      "[64]\tvalidation_0-mlogloss:3.12295\n",
      "[65]\tvalidation_0-mlogloss:3.11859\n",
      "[66]\tvalidation_0-mlogloss:3.11403\n",
      "[67]\tvalidation_0-mlogloss:3.10956\n",
      "[68]\tvalidation_0-mlogloss:3.10756\n",
      "[69]\tvalidation_0-mlogloss:3.10332\n",
      "[70]\tvalidation_0-mlogloss:3.09967\n",
      "[71]\tvalidation_0-mlogloss:3.09706\n",
      "[72]\tvalidation_0-mlogloss:3.09511\n",
      "[73]\tvalidation_0-mlogloss:3.09293\n",
      "[74]\tvalidation_0-mlogloss:3.08943\n",
      "[75]\tvalidation_0-mlogloss:3.08668\n",
      "[76]\tvalidation_0-mlogloss:3.08318\n",
      "[77]\tvalidation_0-mlogloss:3.08036\n",
      "[78]\tvalidation_0-mlogloss:3.07891\n",
      "[79]\tvalidation_0-mlogloss:3.07557\n",
      "[80]\tvalidation_0-mlogloss:3.07208\n",
      "[81]\tvalidation_0-mlogloss:3.06943\n",
      "[82]\tvalidation_0-mlogloss:3.06637\n",
      "[83]\tvalidation_0-mlogloss:3.06318\n",
      "[84]\tvalidation_0-mlogloss:3.06065\n",
      "[85]\tvalidation_0-mlogloss:3.05817\n",
      "[86]\tvalidation_0-mlogloss:3.05602\n",
      "[87]\tvalidation_0-mlogloss:3.05205\n",
      "[88]\tvalidation_0-mlogloss:3.04783\n",
      "[89]\tvalidation_0-mlogloss:3.04456\n",
      "[90]\tvalidation_0-mlogloss:3.04079\n",
      "[91]\tvalidation_0-mlogloss:3.03798\n",
      "[92]\tvalidation_0-mlogloss:3.03539\n",
      "[93]\tvalidation_0-mlogloss:3.03202\n",
      "[94]\tvalidation_0-mlogloss:3.03028\n",
      "[95]\tvalidation_0-mlogloss:3.02843\n",
      "[96]\tvalidation_0-mlogloss:3.02635\n",
      "[97]\tvalidation_0-mlogloss:3.02405\n",
      "[98]\tvalidation_0-mlogloss:3.02264\n",
      "[99]\tvalidation_0-mlogloss:3.02074\n",
      "[100]\tvalidation_0-mlogloss:3.01751\n",
      "[101]\tvalidation_0-mlogloss:3.01433\n",
      "[102]\tvalidation_0-mlogloss:3.01129\n",
      "[103]\tvalidation_0-mlogloss:3.00954\n",
      "[104]\tvalidation_0-mlogloss:3.00674\n",
      "[105]\tvalidation_0-mlogloss:3.00513\n",
      "[106]\tvalidation_0-mlogloss:3.00366\n",
      "[107]\tvalidation_0-mlogloss:3.00159\n",
      "[108]\tvalidation_0-mlogloss:3.00011\n",
      "[109]\tvalidation_0-mlogloss:2.99936\n",
      "[110]\tvalidation_0-mlogloss:2.99636\n",
      "[111]\tvalidation_0-mlogloss:2.99449\n",
      "[112]\tvalidation_0-mlogloss:2.99234\n",
      "[113]\tvalidation_0-mlogloss:2.99038\n",
      "[114]\tvalidation_0-mlogloss:2.98809\n",
      "[115]\tvalidation_0-mlogloss:2.98702\n",
      "[116]\tvalidation_0-mlogloss:2.98467\n",
      "[117]\tvalidation_0-mlogloss:2.98334\n",
      "[118]\tvalidation_0-mlogloss:2.98118\n",
      "[119]\tvalidation_0-mlogloss:2.98010\n",
      "[120]\tvalidation_0-mlogloss:2.97861\n",
      "[121]\tvalidation_0-mlogloss:2.97811\n",
      "[122]\tvalidation_0-mlogloss:2.97779\n",
      "[123]\tvalidation_0-mlogloss:2.97566\n",
      "[124]\tvalidation_0-mlogloss:2.97376\n",
      "[125]\tvalidation_0-mlogloss:2.97153\n",
      "[126]\tvalidation_0-mlogloss:2.97035\n",
      "[127]\tvalidation_0-mlogloss:2.96874\n",
      "[128]\tvalidation_0-mlogloss:2.96692\n",
      "[129]\tvalidation_0-mlogloss:2.96497\n",
      "[130]\tvalidation_0-mlogloss:2.96272\n",
      "[131]\tvalidation_0-mlogloss:2.96104\n",
      "[132]\tvalidation_0-mlogloss:2.95945\n",
      "[133]\tvalidation_0-mlogloss:2.95760\n",
      "[134]\tvalidation_0-mlogloss:2.95557\n",
      "[135]\tvalidation_0-mlogloss:2.95561\n",
      "[136]\tvalidation_0-mlogloss:2.95589\n",
      "[137]\tvalidation_0-mlogloss:2.95504\n",
      "[138]\tvalidation_0-mlogloss:2.95368\n",
      "[139]\tvalidation_0-mlogloss:2.95194\n",
      "[140]\tvalidation_0-mlogloss:2.95062\n",
      "[141]\tvalidation_0-mlogloss:2.94983\n",
      "[142]\tvalidation_0-mlogloss:2.94812\n",
      "[143]\tvalidation_0-mlogloss:2.94595\n",
      "[144]\tvalidation_0-mlogloss:2.94393\n",
      "[145]\tvalidation_0-mlogloss:2.94234\n",
      "[146]\tvalidation_0-mlogloss:2.94089\n",
      "[147]\tvalidation_0-mlogloss:2.93965\n",
      "[148]\tvalidation_0-mlogloss:2.93869\n",
      "[149]\tvalidation_0-mlogloss:2.93787\n",
      "[150]\tvalidation_0-mlogloss:2.93697\n",
      "[151]\tvalidation_0-mlogloss:2.93564\n",
      "[152]\tvalidation_0-mlogloss:2.93588\n",
      "[153]\tvalidation_0-mlogloss:2.93528\n",
      "[154]\tvalidation_0-mlogloss:2.93417\n",
      "[155]\tvalidation_0-mlogloss:2.93304\n",
      "[156]\tvalidation_0-mlogloss:2.93165\n",
      "[157]\tvalidation_0-mlogloss:2.93061\n",
      "[158]\tvalidation_0-mlogloss:2.92954\n",
      "[159]\tvalidation_0-mlogloss:2.92867\n",
      "[160]\tvalidation_0-mlogloss:2.92751\n",
      "[161]\tvalidation_0-mlogloss:2.92713\n",
      "[162]\tvalidation_0-mlogloss:2.92654\n",
      "[163]\tvalidation_0-mlogloss:2.92513\n",
      "[164]\tvalidation_0-mlogloss:2.92437\n",
      "[165]\tvalidation_0-mlogloss:2.92426\n",
      "[166]\tvalidation_0-mlogloss:2.92372\n",
      "[167]\tvalidation_0-mlogloss:2.92240\n",
      "[168]\tvalidation_0-mlogloss:2.92063\n",
      "[169]\tvalidation_0-mlogloss:2.92036\n",
      "[170]\tvalidation_0-mlogloss:2.91891\n",
      "[171]\tvalidation_0-mlogloss:2.91779\n",
      "[172]\tvalidation_0-mlogloss:2.91632\n",
      "[173]\tvalidation_0-mlogloss:2.91538\n",
      "[174]\tvalidation_0-mlogloss:2.91419\n",
      "[175]\tvalidation_0-mlogloss:2.91288\n",
      "[176]\tvalidation_0-mlogloss:2.91193\n",
      "[177]\tvalidation_0-mlogloss:2.91057\n",
      "[178]\tvalidation_0-mlogloss:2.90991\n",
      "[179]\tvalidation_0-mlogloss:2.90876\n",
      "[180]\tvalidation_0-mlogloss:2.90829\n",
      "[181]\tvalidation_0-mlogloss:2.90817\n",
      "[182]\tvalidation_0-mlogloss:2.90676\n",
      "[183]\tvalidation_0-mlogloss:2.90578\n",
      "[184]\tvalidation_0-mlogloss:2.90511\n",
      "[185]\tvalidation_0-mlogloss:2.90525\n",
      "[186]\tvalidation_0-mlogloss:2.90445\n",
      "[187]\tvalidation_0-mlogloss:2.90387\n",
      "[188]\tvalidation_0-mlogloss:2.90292\n",
      "[189]\tvalidation_0-mlogloss:2.90167\n",
      "[190]\tvalidation_0-mlogloss:2.90071\n",
      "[191]\tvalidation_0-mlogloss:2.90085\n",
      "[192]\tvalidation_0-mlogloss:2.90064\n",
      "[193]\tvalidation_0-mlogloss:2.90011\n",
      "[194]\tvalidation_0-mlogloss:2.90012\n",
      "[195]\tvalidation_0-mlogloss:2.90058\n",
      "[196]\tvalidation_0-mlogloss:2.90034\n",
      "[197]\tvalidation_0-mlogloss:2.89992\n",
      "[198]\tvalidation_0-mlogloss:2.89971\n",
      "[199]\tvalidation_0-mlogloss:2.89884\n",
      "[200]\tvalidation_0-mlogloss:2.89788\n",
      "[201]\tvalidation_0-mlogloss:2.89718\n",
      "[202]\tvalidation_0-mlogloss:2.89630\n",
      "[203]\tvalidation_0-mlogloss:2.89576\n",
      "[204]\tvalidation_0-mlogloss:2.89498\n",
      "[205]\tvalidation_0-mlogloss:2.89404\n",
      "[206]\tvalidation_0-mlogloss:2.89385\n",
      "[207]\tvalidation_0-mlogloss:2.89346\n",
      "[208]\tvalidation_0-mlogloss:2.89270\n",
      "[209]\tvalidation_0-mlogloss:2.89203\n",
      "[210]\tvalidation_0-mlogloss:2.89181\n",
      "[211]\tvalidation_0-mlogloss:2.89124\n",
      "[212]\tvalidation_0-mlogloss:2.89138\n",
      "[213]\tvalidation_0-mlogloss:2.89086\n",
      "[214]\tvalidation_0-mlogloss:2.89066\n",
      "[215]\tvalidation_0-mlogloss:2.89020\n",
      "[216]\tvalidation_0-mlogloss:2.88979\n",
      "[217]\tvalidation_0-mlogloss:2.88988\n",
      "[218]\tvalidation_0-mlogloss:2.88907\n",
      "[219]\tvalidation_0-mlogloss:2.88904\n",
      "[220]\tvalidation_0-mlogloss:2.88829\n",
      "[221]\tvalidation_0-mlogloss:2.88791\n",
      "[222]\tvalidation_0-mlogloss:2.88827\n",
      "[223]\tvalidation_0-mlogloss:2.88843\n",
      "[224]\tvalidation_0-mlogloss:2.88868\n",
      "[225]\tvalidation_0-mlogloss:2.88928\n",
      "[226]\tvalidation_0-mlogloss:2.88864\n",
      "[227]\tvalidation_0-mlogloss:2.88755\n",
      "[228]\tvalidation_0-mlogloss:2.88716\n",
      "[229]\tvalidation_0-mlogloss:2.88665\n",
      "[230]\tvalidation_0-mlogloss:2.88769\n",
      "[231]\tvalidation_0-mlogloss:2.88746\n",
      "[232]\tvalidation_0-mlogloss:2.88825\n",
      "[233]\tvalidation_0-mlogloss:2.88772\n",
      "[234]\tvalidation_0-mlogloss:2.88800\n",
      "[235]\tvalidation_0-mlogloss:2.88821\n",
      "[236]\tvalidation_0-mlogloss:2.88870\n",
      "[237]\tvalidation_0-mlogloss:2.88810\n",
      "[238]\tvalidation_0-mlogloss:2.88740\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.12      0.14      1499\n",
      "           1       0.19      0.23      0.21      1499\n",
      "           2       0.23      0.22      0.23      1499\n",
      "           3       0.35      0.33      0.34      1499\n",
      "           4       0.66      0.69      0.67      1499\n",
      "           5       0.20      0.20      0.20      1499\n",
      "           6       0.19      0.31      0.23      1499\n",
      "           7       0.28      0.27      0.27      1499\n",
      "           8       0.27      0.41      0.33      1499\n",
      "           9       0.46      0.47      0.46      1499\n",
      "          10       0.19      0.18      0.19      1499\n",
      "          11       0.12      0.04      0.06      1499\n",
      "          12       0.08      0.04      0.05      1499\n",
      "          13       0.13      0.09      0.10      1499\n",
      "          14       0.14      0.06      0.08      1499\n",
      "          15       0.18      0.17      0.18      1499\n",
      "          16       0.23      0.25      0.24      1499\n",
      "          17       0.30      0.47      0.36      1499\n",
      "          18       0.11      0.04      0.06      1499\n",
      "          19       0.15      0.12      0.13      1499\n",
      "          20       0.53      0.85      0.65      1499\n",
      "          21       0.20      0.24      0.22      1499\n",
      "          22       0.19      0.10      0.13      1499\n",
      "          23       0.33      0.58      0.42      1499\n",
      "          24       0.17      0.19      0.18      1499\n",
      "          25       0.25      0.31      0.28      1499\n",
      "          26       0.24      0.22      0.23      1499\n",
      "          27       0.09      0.10      0.09      1499\n",
      "          28       0.15      0.14      0.14      1499\n",
      "          29       0.26      0.27      0.26      1499\n",
      "          30       0.15      0.14      0.14      1499\n",
      "          31       0.23      0.30      0.26      1499\n",
      "          32       0.10      0.10      0.10      1499\n",
      "          33       0.06      0.04      0.04      1499\n",
      "          34       0.34      0.28      0.31      1499\n",
      "          35       0.36      0.46      0.40      1499\n",
      "          36       0.29      0.44      0.35      1499\n",
      "          37       0.16      0.17      0.17      1499\n",
      "          38       0.18      0.21      0.19      1499\n",
      "          39       0.26      0.23      0.24      1499\n",
      "          40       0.25      0.30      0.27      1499\n",
      "          41       0.35      0.46      0.40      1499\n",
      "          42       0.20      0.13      0.16      1499\n",
      "          43       0.20      0.19      0.19      1499\n",
      "          44       0.30      0.44      0.36      1499\n",
      "          45       0.13      0.09      0.11      1499\n",
      "          46       0.27      0.28      0.27      1499\n",
      "          47       0.43      0.39      0.41      1499\n",
      "          48       0.42      0.56      0.48      1499\n",
      "          49       0.22      0.13      0.16      1499\n",
      "          50       0.27      0.45      0.34      1499\n",
      "          51       0.16      0.13      0.14      1499\n",
      "          52       0.14      0.08      0.10      1499\n",
      "          53       0.14      0.09      0.11      1499\n",
      "          54       0.18      0.18      0.18      1499\n",
      "          55       0.27      0.35      0.30      1499\n",
      "          56       0.12      0.12      0.12      1499\n",
      "          57       0.31      0.49      0.38      1499\n",
      "          58       0.86      0.73      0.79      1499\n",
      "          59       0.13      0.12      0.12      1499\n",
      "          60       0.28      0.24      0.26      1499\n",
      "          61       0.11      0.08      0.09      1499\n",
      "          62       0.40      0.59      0.48      1499\n",
      "          63       0.15      0.08      0.10      1499\n",
      "          64       0.12      0.09      0.10      1499\n",
      "          65       0.12      0.10      0.11      1499\n",
      "          66       0.56      0.57      0.56      1499\n",
      "          67       0.28      0.24      0.26      1499\n",
      "          68       0.38      0.43      0.41      1499\n",
      "          69       0.10      0.07      0.08      1499\n",
      "          70       0.24      0.22      0.23      1499\n",
      "          71       0.14      0.15      0.15      1499\n",
      "          72       0.08      0.05      0.06      1499\n",
      "          73       0.28      0.24      0.26      1499\n",
      "          74       0.15      0.24      0.18      1499\n",
      "\n",
      "    accuracy                           0.26    112425\n",
      "   macro avg       0.24      0.26      0.24    112425\n",
      "weighted avg       0.24      0.26      0.24    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b181cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 79s 3ms/step - loss: 2.7215 - val_loss: 2.8924\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 84s 3ms/step - loss: 2.2078 - val_loss: 2.9173\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 85s 3ms/step - loss: 2.0909 - val_loss: 2.9269\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 75s 3ms/step - loss: 2.0340 - val_loss: 3.0103\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 60s 2ms/step - loss: 1.9979 - val_loss: 3.0350\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 62s 2ms/step - loss: 1.9729 - val_loss: 2.9166\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 63s 2ms/step - loss: 1.9533 - val_loss: 3.0732\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 61s 2ms/step - loss: 1.9357 - val_loss: 3.0248\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 63s 2ms/step - loss: 1.9230 - val_loss: 3.1574\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 62s 2ms/step - loss: 1.9132 - val_loss: 3.0501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f31555dbe50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 5s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 13:47:17.275148: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 33727500 exceeds 10% of free system memory.\n",
      "2023-10-04 13:47:21.076335: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 33727500 exceeds 10% of free system memory.\n",
      "2023-10-04 13:47:21.083214: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 33727500 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.23      0.17      1499\n",
      "           1       0.21      0.18      0.20      1499\n",
      "           2       0.21      0.23      0.22      1499\n",
      "           3       0.42      0.32      0.36      1499\n",
      "           4       0.67      0.67      0.67      1499\n",
      "           5       0.23      0.25      0.24      1499\n",
      "           6       0.19      0.36      0.25      1499\n",
      "           7       0.36      0.28      0.31      1499\n",
      "           8       0.31      0.37      0.34      1499\n",
      "           9       0.67      0.41      0.51      1499\n",
      "          10       0.17      0.13      0.15      1499\n",
      "          11       0.09      0.08      0.08      1499\n",
      "          12       0.06      0.02      0.03      1499\n",
      "          13       0.12      0.08      0.10      1499\n",
      "          14       0.15      0.04      0.07      1499\n",
      "          15       0.18      0.19      0.18      1499\n",
      "          16       0.16      0.14      0.15      1499\n",
      "          17       0.42      0.36      0.39      1499\n",
      "          18       0.11      0.05      0.07      1499\n",
      "          19       0.14      0.08      0.10      1499\n",
      "          20       0.80      0.46      0.59      1499\n",
      "          21       0.19      0.28      0.22      1499\n",
      "          22       0.21      0.06      0.10      1499\n",
      "          23       0.30      0.63      0.40      1499\n",
      "          24       0.16      0.19      0.18      1499\n",
      "          25       0.36      0.18      0.24      1499\n",
      "          26       0.19      0.09      0.12      1499\n",
      "          27       0.06      0.09      0.07      1499\n",
      "          28       0.18      0.18      0.18      1499\n",
      "          29       0.23      0.39      0.29      1499\n",
      "          30       0.16      0.18      0.17      1499\n",
      "          31       0.17      0.22      0.19      1499\n",
      "          32       0.10      0.08      0.09      1499\n",
      "          33       0.05      0.03      0.04      1499\n",
      "          34       0.33      0.14      0.19      1499\n",
      "          35       0.38      0.32      0.35      1499\n",
      "          36       0.45      0.24      0.31      1499\n",
      "          37       0.14      0.19      0.16      1499\n",
      "          38       0.16      0.29      0.20      1499\n",
      "          39       0.23      0.24      0.23      1499\n",
      "          40       0.26      0.33      0.29      1499\n",
      "          41       0.47      0.50      0.48      1499\n",
      "          42       0.18      0.19      0.19      1499\n",
      "          43       0.20      0.36      0.25      1499\n",
      "          44       0.29      0.38      0.33      1499\n",
      "          45       0.09      0.09      0.09      1499\n",
      "          46       0.34      0.23      0.27      1499\n",
      "          47       0.45      0.36      0.40      1499\n",
      "          48       0.42      0.55      0.47      1499\n",
      "          49       0.17      0.10      0.12      1499\n",
      "          50       0.33      0.39      0.36      1499\n",
      "          51       0.13      0.06      0.09      1499\n",
      "          52       0.11      0.10      0.11      1499\n",
      "          53       0.13      0.12      0.13      1499\n",
      "          54       0.18      0.15      0.17      1499\n",
      "          55       0.22      0.42      0.29      1499\n",
      "          56       0.10      0.14      0.12      1499\n",
      "          57       0.36      0.45      0.40      1499\n",
      "          58       0.90      0.71      0.80      1499\n",
      "          59       0.09      0.09      0.09      1499\n",
      "          60       0.24      0.18      0.20      1499\n",
      "          61       0.11      0.06      0.08      1499\n",
      "          62       0.46      0.45      0.46      1499\n",
      "          63       0.17      0.14      0.15      1499\n",
      "          64       0.08      0.04      0.05      1499\n",
      "          65       0.11      0.13      0.12      1499\n",
      "          66       0.63      0.47      0.54      1499\n",
      "          67       0.19      0.25      0.22      1499\n",
      "          68       0.44      0.30      0.36      1499\n",
      "          69       0.07      0.13      0.09      1499\n",
      "          70       0.22      0.18      0.20      1499\n",
      "          71       0.13      0.18      0.15      1499\n",
      "          72       0.08      0.06      0.07      1499\n",
      "          73       0.29      0.24      0.27      1499\n",
      "          74       0.12      0.23      0.16      1499\n",
      "\n",
      "    accuracy                           0.24    112425\n",
      "   macro avg       0.25      0.24      0.23    112425\n",
      "weighted avg       0.25      0.24      0.23    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78467/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_78467/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_78467/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1       -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2       -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3       -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4       -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000055   0.000013  -0.000036  -0.000092  -0.000044  -0.000073   \n",
       "899996  -0.000050  -0.000065  -0.000089  -0.000119  -0.000064  -0.000116   \n",
       "899997   0.000001  -0.000021  -0.000052  -0.000108  -0.000053  -0.000102   \n",
       "899998   0.000011   0.000051   0.000003  -0.000079  -0.000023  -0.000072   \n",
       "899999   0.000031   0.000022  -0.000004  -0.000061   0.000002  -0.000045   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1       -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2       -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3       -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4       -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000019   0.000028   0.000032  -0.000043  ...    0.000005   \n",
       "899996  -0.000026  -0.000044  -0.000070  -0.000080  ...   -0.000007   \n",
       "899997  -0.000020  -0.000034  -0.000003  -0.000039  ...   -0.000044   \n",
       "899998   0.000031  -0.000010   0.000079   0.000008  ...   -0.000064   \n",
       "899999   0.000037   0.000059   0.000023   0.000003  ...   -0.000048   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1        -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2        -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3        -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4        -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995    0.000019   -0.000023   -0.000062   -0.000021   -0.000005   \n",
       "899996   -0.000014   -0.000038   -0.000075   -0.000048   -0.000031   \n",
       "899997    0.000030    0.000022   -0.000021   -0.000030   -0.000039   \n",
       "899998    0.000006    0.000007   -0.000009   -0.000026   -0.000063   \n",
       "899999    0.000002    0.000010    0.000023    0.000015   -0.000032   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1        -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2        -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3        -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4        -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000022    0.000022   -0.000018    0.000014  \n",
       "899996   -0.000008    0.000012   -0.000035   -0.000030  \n",
       "899997    0.000026    0.000058    0.000017    0.000050  \n",
       "899998    0.000028    0.000044   -0.000009    0.000039  \n",
       "899999    0.000033    0.000045    0.000033    0.000042  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.95584\n",
      "[1]\tvalidation_0-mlogloss:3.81972\n",
      "[2]\tvalidation_0-mlogloss:3.71704\n",
      "[3]\tvalidation_0-mlogloss:3.63810\n",
      "[4]\tvalidation_0-mlogloss:3.57185\n",
      "[5]\tvalidation_0-mlogloss:3.51312\n",
      "[6]\tvalidation_0-mlogloss:3.46801\n",
      "[7]\tvalidation_0-mlogloss:3.42156\n",
      "[8]\tvalidation_0-mlogloss:3.38207\n",
      "[9]\tvalidation_0-mlogloss:3.34842\n",
      "[10]\tvalidation_0-mlogloss:3.31633\n",
      "[11]\tvalidation_0-mlogloss:3.28849\n",
      "[12]\tvalidation_0-mlogloss:3.26423\n",
      "[13]\tvalidation_0-mlogloss:3.24068\n",
      "[14]\tvalidation_0-mlogloss:3.22118\n",
      "[15]\tvalidation_0-mlogloss:3.20052\n",
      "[16]\tvalidation_0-mlogloss:3.18306\n",
      "[17]\tvalidation_0-mlogloss:3.16494\n",
      "[18]\tvalidation_0-mlogloss:3.14867\n",
      "[19]\tvalidation_0-mlogloss:3.13299\n",
      "[20]\tvalidation_0-mlogloss:3.11670\n",
      "[21]\tvalidation_0-mlogloss:3.10015\n",
      "[22]\tvalidation_0-mlogloss:3.08734\n",
      "[23]\tvalidation_0-mlogloss:3.07533\n",
      "[24]\tvalidation_0-mlogloss:3.06128\n",
      "[25]\tvalidation_0-mlogloss:3.04646\n",
      "[26]\tvalidation_0-mlogloss:3.03483\n",
      "[27]\tvalidation_0-mlogloss:3.02275\n",
      "[28]\tvalidation_0-mlogloss:3.01246\n",
      "[29]\tvalidation_0-mlogloss:2.99956\n",
      "[30]\tvalidation_0-mlogloss:2.98825\n",
      "[31]\tvalidation_0-mlogloss:2.97625\n",
      "[32]\tvalidation_0-mlogloss:2.96638\n",
      "[33]\tvalidation_0-mlogloss:2.95589\n",
      "[34]\tvalidation_0-mlogloss:2.94688\n",
      "[35]\tvalidation_0-mlogloss:2.93776\n",
      "[36]\tvalidation_0-mlogloss:2.92940\n",
      "[37]\tvalidation_0-mlogloss:2.91757\n",
      "[38]\tvalidation_0-mlogloss:2.90931\n",
      "[39]\tvalidation_0-mlogloss:2.90087\n",
      "[40]\tvalidation_0-mlogloss:2.89007\n",
      "[41]\tvalidation_0-mlogloss:2.88125\n",
      "[42]\tvalidation_0-mlogloss:2.87344\n",
      "[43]\tvalidation_0-mlogloss:2.86374\n",
      "[44]\tvalidation_0-mlogloss:2.85479\n",
      "[45]\tvalidation_0-mlogloss:2.84631\n",
      "[46]\tvalidation_0-mlogloss:2.83822\n",
      "[47]\tvalidation_0-mlogloss:2.82984\n",
      "[48]\tvalidation_0-mlogloss:2.82188\n",
      "[49]\tvalidation_0-mlogloss:2.81326\n",
      "[50]\tvalidation_0-mlogloss:2.80600\n",
      "[51]\tvalidation_0-mlogloss:2.79835\n",
      "[52]\tvalidation_0-mlogloss:2.79182\n",
      "[53]\tvalidation_0-mlogloss:2.78562\n",
      "[54]\tvalidation_0-mlogloss:2.77929\n",
      "[55]\tvalidation_0-mlogloss:2.77388\n",
      "[56]\tvalidation_0-mlogloss:2.76750\n",
      "[57]\tvalidation_0-mlogloss:2.76016\n",
      "[58]\tvalidation_0-mlogloss:2.75433\n",
      "[59]\tvalidation_0-mlogloss:2.74842\n",
      "[60]\tvalidation_0-mlogloss:2.74240\n",
      "[61]\tvalidation_0-mlogloss:2.73686\n",
      "[62]\tvalidation_0-mlogloss:2.73140\n",
      "[63]\tvalidation_0-mlogloss:2.72355\n",
      "[64]\tvalidation_0-mlogloss:2.71879\n",
      "[65]\tvalidation_0-mlogloss:2.71426\n",
      "[66]\tvalidation_0-mlogloss:2.70796\n",
      "[67]\tvalidation_0-mlogloss:2.70170\n",
      "[68]\tvalidation_0-mlogloss:2.69786\n",
      "[69]\tvalidation_0-mlogloss:2.69400\n",
      "[70]\tvalidation_0-mlogloss:2.68946\n",
      "[71]\tvalidation_0-mlogloss:2.68469\n",
      "[72]\tvalidation_0-mlogloss:2.67917\n",
      "[73]\tvalidation_0-mlogloss:2.67366\n",
      "[74]\tvalidation_0-mlogloss:2.66882\n",
      "[75]\tvalidation_0-mlogloss:2.66369\n",
      "[76]\tvalidation_0-mlogloss:2.66013\n",
      "[77]\tvalidation_0-mlogloss:2.65631\n",
      "[78]\tvalidation_0-mlogloss:2.65121\n",
      "[79]\tvalidation_0-mlogloss:2.64611\n",
      "[80]\tvalidation_0-mlogloss:2.64254\n",
      "[81]\tvalidation_0-mlogloss:2.63856\n",
      "[82]\tvalidation_0-mlogloss:2.63578\n",
      "[83]\tvalidation_0-mlogloss:2.63060\n",
      "[84]\tvalidation_0-mlogloss:2.62544\n",
      "[85]\tvalidation_0-mlogloss:2.62139\n",
      "[86]\tvalidation_0-mlogloss:2.61685\n",
      "[87]\tvalidation_0-mlogloss:2.61271\n",
      "[88]\tvalidation_0-mlogloss:2.60846\n",
      "[89]\tvalidation_0-mlogloss:2.60379\n",
      "[90]\tvalidation_0-mlogloss:2.60110\n",
      "[91]\tvalidation_0-mlogloss:2.59895\n",
      "[92]\tvalidation_0-mlogloss:2.59534\n",
      "[93]\tvalidation_0-mlogloss:2.59163\n",
      "[94]\tvalidation_0-mlogloss:2.58730\n",
      "[95]\tvalidation_0-mlogloss:2.58334\n",
      "[96]\tvalidation_0-mlogloss:2.57868\n",
      "[97]\tvalidation_0-mlogloss:2.57517\n",
      "[98]\tvalidation_0-mlogloss:2.57148\n",
      "[99]\tvalidation_0-mlogloss:2.56794\n",
      "[100]\tvalidation_0-mlogloss:2.56494\n",
      "[101]\tvalidation_0-mlogloss:2.56151\n",
      "[102]\tvalidation_0-mlogloss:2.55799\n",
      "[103]\tvalidation_0-mlogloss:2.55484\n",
      "[104]\tvalidation_0-mlogloss:2.55240\n",
      "[105]\tvalidation_0-mlogloss:2.54832\n",
      "[106]\tvalidation_0-mlogloss:2.54465\n",
      "[107]\tvalidation_0-mlogloss:2.54066\n",
      "[108]\tvalidation_0-mlogloss:2.53874\n",
      "[109]\tvalidation_0-mlogloss:2.53653\n",
      "[110]\tvalidation_0-mlogloss:2.53359\n",
      "[111]\tvalidation_0-mlogloss:2.53111\n",
      "[112]\tvalidation_0-mlogloss:2.52875\n",
      "[113]\tvalidation_0-mlogloss:2.52524\n",
      "[114]\tvalidation_0-mlogloss:2.52281\n",
      "[115]\tvalidation_0-mlogloss:2.51992\n",
      "[116]\tvalidation_0-mlogloss:2.51676\n",
      "[117]\tvalidation_0-mlogloss:2.51352\n",
      "[118]\tvalidation_0-mlogloss:2.51049\n",
      "[119]\tvalidation_0-mlogloss:2.50741\n",
      "[120]\tvalidation_0-mlogloss:2.50480\n",
      "[121]\tvalidation_0-mlogloss:2.50213\n",
      "[122]\tvalidation_0-mlogloss:2.49888\n",
      "[123]\tvalidation_0-mlogloss:2.49536\n",
      "[124]\tvalidation_0-mlogloss:2.49220\n",
      "[125]\tvalidation_0-mlogloss:2.48756\n",
      "[126]\tvalidation_0-mlogloss:2.48437\n",
      "[127]\tvalidation_0-mlogloss:2.48262\n",
      "[128]\tvalidation_0-mlogloss:2.47997\n",
      "[129]\tvalidation_0-mlogloss:2.47899\n",
      "[130]\tvalidation_0-mlogloss:2.47698\n",
      "[131]\tvalidation_0-mlogloss:2.47451\n",
      "[132]\tvalidation_0-mlogloss:2.47082\n",
      "[133]\tvalidation_0-mlogloss:2.46867\n",
      "[134]\tvalidation_0-mlogloss:2.46625\n",
      "[135]\tvalidation_0-mlogloss:2.46331\n",
      "[136]\tvalidation_0-mlogloss:2.46101\n",
      "[137]\tvalidation_0-mlogloss:2.45865\n",
      "[138]\tvalidation_0-mlogloss:2.45676\n",
      "[139]\tvalidation_0-mlogloss:2.45399\n",
      "[140]\tvalidation_0-mlogloss:2.45170\n",
      "[141]\tvalidation_0-mlogloss:2.44959\n",
      "[142]\tvalidation_0-mlogloss:2.44769\n",
      "[143]\tvalidation_0-mlogloss:2.44553\n",
      "[144]\tvalidation_0-mlogloss:2.44402\n",
      "[145]\tvalidation_0-mlogloss:2.44228\n",
      "[146]\tvalidation_0-mlogloss:2.44050\n",
      "[147]\tvalidation_0-mlogloss:2.43876\n",
      "[148]\tvalidation_0-mlogloss:2.43786\n",
      "[149]\tvalidation_0-mlogloss:2.43702\n",
      "[150]\tvalidation_0-mlogloss:2.43517\n",
      "[151]\tvalidation_0-mlogloss:2.43300\n",
      "[152]\tvalidation_0-mlogloss:2.43089\n",
      "[153]\tvalidation_0-mlogloss:2.42880\n",
      "[154]\tvalidation_0-mlogloss:2.42656\n",
      "[155]\tvalidation_0-mlogloss:2.42445\n",
      "[156]\tvalidation_0-mlogloss:2.42259\n",
      "[157]\tvalidation_0-mlogloss:2.42073\n",
      "[158]\tvalidation_0-mlogloss:2.41933\n",
      "[159]\tvalidation_0-mlogloss:2.41814\n",
      "[160]\tvalidation_0-mlogloss:2.41725\n",
      "[161]\tvalidation_0-mlogloss:2.41556\n",
      "[162]\tvalidation_0-mlogloss:2.41472\n",
      "[163]\tvalidation_0-mlogloss:2.41342\n",
      "[164]\tvalidation_0-mlogloss:2.41204\n",
      "[165]\tvalidation_0-mlogloss:2.40974\n",
      "[166]\tvalidation_0-mlogloss:2.40810\n",
      "[167]\tvalidation_0-mlogloss:2.40666\n",
      "[168]\tvalidation_0-mlogloss:2.40567\n",
      "[169]\tvalidation_0-mlogloss:2.40415\n",
      "[170]\tvalidation_0-mlogloss:2.40235\n",
      "[171]\tvalidation_0-mlogloss:2.40063\n",
      "[172]\tvalidation_0-mlogloss:2.39875\n",
      "[173]\tvalidation_0-mlogloss:2.39751\n",
      "[174]\tvalidation_0-mlogloss:2.39635\n",
      "[175]\tvalidation_0-mlogloss:2.39545\n",
      "[176]\tvalidation_0-mlogloss:2.39436\n",
      "[177]\tvalidation_0-mlogloss:2.39357\n",
      "[178]\tvalidation_0-mlogloss:2.39266\n",
      "[179]\tvalidation_0-mlogloss:2.39136\n",
      "[180]\tvalidation_0-mlogloss:2.38929\n",
      "[181]\tvalidation_0-mlogloss:2.38803\n",
      "[182]\tvalidation_0-mlogloss:2.38639\n",
      "[183]\tvalidation_0-mlogloss:2.38610\n",
      "[184]\tvalidation_0-mlogloss:2.38459\n",
      "[185]\tvalidation_0-mlogloss:2.38307\n",
      "[186]\tvalidation_0-mlogloss:2.38252\n",
      "[187]\tvalidation_0-mlogloss:2.38124\n",
      "[188]\tvalidation_0-mlogloss:2.37971\n",
      "[189]\tvalidation_0-mlogloss:2.37819\n",
      "[190]\tvalidation_0-mlogloss:2.37712\n",
      "[191]\tvalidation_0-mlogloss:2.37617\n",
      "[192]\tvalidation_0-mlogloss:2.37603\n",
      "[193]\tvalidation_0-mlogloss:2.37501\n",
      "[194]\tvalidation_0-mlogloss:2.37382\n",
      "[195]\tvalidation_0-mlogloss:2.37258\n",
      "[196]\tvalidation_0-mlogloss:2.37129\n",
      "[197]\tvalidation_0-mlogloss:2.36970\n",
      "[198]\tvalidation_0-mlogloss:2.36892\n",
      "[199]\tvalidation_0-mlogloss:2.36808\n",
      "[200]\tvalidation_0-mlogloss:2.36680\n",
      "[201]\tvalidation_0-mlogloss:2.36600\n",
      "[202]\tvalidation_0-mlogloss:2.36488\n",
      "[203]\tvalidation_0-mlogloss:2.36419\n",
      "[204]\tvalidation_0-mlogloss:2.36308\n",
      "[205]\tvalidation_0-mlogloss:2.36151\n",
      "[206]\tvalidation_0-mlogloss:2.36068\n",
      "[207]\tvalidation_0-mlogloss:2.35974\n",
      "[208]\tvalidation_0-mlogloss:2.35857\n",
      "[209]\tvalidation_0-mlogloss:2.35777\n",
      "[210]\tvalidation_0-mlogloss:2.35682\n",
      "[211]\tvalidation_0-mlogloss:2.35619\n",
      "[212]\tvalidation_0-mlogloss:2.35502\n",
      "[213]\tvalidation_0-mlogloss:2.35386\n",
      "[214]\tvalidation_0-mlogloss:2.35274\n",
      "[215]\tvalidation_0-mlogloss:2.35199\n",
      "[216]\tvalidation_0-mlogloss:2.35071\n",
      "[217]\tvalidation_0-mlogloss:2.35029\n",
      "[218]\tvalidation_0-mlogloss:2.34861\n",
      "[219]\tvalidation_0-mlogloss:2.34804\n",
      "[220]\tvalidation_0-mlogloss:2.34735\n",
      "[221]\tvalidation_0-mlogloss:2.34637\n",
      "[222]\tvalidation_0-mlogloss:2.34587\n",
      "[223]\tvalidation_0-mlogloss:2.34645\n",
      "[224]\tvalidation_0-mlogloss:2.34564\n",
      "[225]\tvalidation_0-mlogloss:2.34528\n",
      "[226]\tvalidation_0-mlogloss:2.34401\n",
      "[227]\tvalidation_0-mlogloss:2.34428\n",
      "[228]\tvalidation_0-mlogloss:2.34291\n",
      "[229]\tvalidation_0-mlogloss:2.34181\n",
      "[230]\tvalidation_0-mlogloss:2.34118\n",
      "[231]\tvalidation_0-mlogloss:2.34018\n",
      "[232]\tvalidation_0-mlogloss:2.33971\n",
      "[233]\tvalidation_0-mlogloss:2.33919\n",
      "[234]\tvalidation_0-mlogloss:2.33845\n",
      "[235]\tvalidation_0-mlogloss:2.33755\n",
      "[236]\tvalidation_0-mlogloss:2.33671\n",
      "[237]\tvalidation_0-mlogloss:2.33606\n",
      "[238]\tvalidation_0-mlogloss:2.33518\n",
      "[239]\tvalidation_0-mlogloss:2.33505\n",
      "[240]\tvalidation_0-mlogloss:2.33447\n",
      "[241]\tvalidation_0-mlogloss:2.33327\n",
      "[242]\tvalidation_0-mlogloss:2.33200\n",
      "[243]\tvalidation_0-mlogloss:2.33112\n",
      "[244]\tvalidation_0-mlogloss:2.32996\n",
      "[245]\tvalidation_0-mlogloss:2.32948\n",
      "[246]\tvalidation_0-mlogloss:2.32837\n",
      "[247]\tvalidation_0-mlogloss:2.32818\n",
      "[248]\tvalidation_0-mlogloss:2.32716\n",
      "[249]\tvalidation_0-mlogloss:2.32622\n",
      "[250]\tvalidation_0-mlogloss:2.32523\n",
      "[251]\tvalidation_0-mlogloss:2.32386\n",
      "[252]\tvalidation_0-mlogloss:2.32282\n",
      "[253]\tvalidation_0-mlogloss:2.32253\n",
      "[254]\tvalidation_0-mlogloss:2.32096\n",
      "[255]\tvalidation_0-mlogloss:2.31984\n",
      "[256]\tvalidation_0-mlogloss:2.31832\n",
      "[257]\tvalidation_0-mlogloss:2.31764\n",
      "[258]\tvalidation_0-mlogloss:2.31694\n",
      "[259]\tvalidation_0-mlogloss:2.31619\n",
      "[260]\tvalidation_0-mlogloss:2.31573\n",
      "[261]\tvalidation_0-mlogloss:2.31617\n",
      "[262]\tvalidation_0-mlogloss:2.31567\n",
      "[263]\tvalidation_0-mlogloss:2.31524\n",
      "[264]\tvalidation_0-mlogloss:2.31539\n",
      "[265]\tvalidation_0-mlogloss:2.31527\n",
      "[266]\tvalidation_0-mlogloss:2.31453\n",
      "[267]\tvalidation_0-mlogloss:2.31408\n",
      "[268]\tvalidation_0-mlogloss:2.31366\n",
      "[269]\tvalidation_0-mlogloss:2.31372\n",
      "[270]\tvalidation_0-mlogloss:2.31359\n",
      "[271]\tvalidation_0-mlogloss:2.31306\n",
      "[272]\tvalidation_0-mlogloss:2.31225\n",
      "[273]\tvalidation_0-mlogloss:2.31231\n",
      "[274]\tvalidation_0-mlogloss:2.31183\n",
      "[275]\tvalidation_0-mlogloss:2.31246\n",
      "[276]\tvalidation_0-mlogloss:2.31256\n",
      "[277]\tvalidation_0-mlogloss:2.31286\n",
      "[278]\tvalidation_0-mlogloss:2.31277\n",
      "[279]\tvalidation_0-mlogloss:2.31227\n",
      "[280]\tvalidation_0-mlogloss:2.31153\n",
      "[281]\tvalidation_0-mlogloss:2.31066\n",
      "[282]\tvalidation_0-mlogloss:2.31016\n",
      "[283]\tvalidation_0-mlogloss:2.30939\n",
      "[284]\tvalidation_0-mlogloss:2.30933\n",
      "[285]\tvalidation_0-mlogloss:2.30865\n",
      "[286]\tvalidation_0-mlogloss:2.30820\n",
      "[287]\tvalidation_0-mlogloss:2.30794\n",
      "[288]\tvalidation_0-mlogloss:2.30712\n",
      "[289]\tvalidation_0-mlogloss:2.30664\n",
      "[290]\tvalidation_0-mlogloss:2.30609\n",
      "[291]\tvalidation_0-mlogloss:2.30557\n",
      "[292]\tvalidation_0-mlogloss:2.30549\n",
      "[293]\tvalidation_0-mlogloss:2.30635\n",
      "[294]\tvalidation_0-mlogloss:2.30657\n",
      "[295]\tvalidation_0-mlogloss:2.30611\n",
      "[296]\tvalidation_0-mlogloss:2.30548\n",
      "[297]\tvalidation_0-mlogloss:2.30551\n",
      "[298]\tvalidation_0-mlogloss:2.30518\n",
      "[299]\tvalidation_0-mlogloss:2.30492\n",
      "[300]\tvalidation_0-mlogloss:2.30429\n",
      "[301]\tvalidation_0-mlogloss:2.30391\n",
      "[302]\tvalidation_0-mlogloss:2.30286\n",
      "[303]\tvalidation_0-mlogloss:2.30322\n",
      "[304]\tvalidation_0-mlogloss:2.30263\n",
      "[305]\tvalidation_0-mlogloss:2.30314\n",
      "[306]\tvalidation_0-mlogloss:2.30333\n",
      "[307]\tvalidation_0-mlogloss:2.30336\n",
      "[308]\tvalidation_0-mlogloss:2.30293\n",
      "[309]\tvalidation_0-mlogloss:2.30239\n",
      "[310]\tvalidation_0-mlogloss:2.30213\n",
      "[311]\tvalidation_0-mlogloss:2.30211\n",
      "[312]\tvalidation_0-mlogloss:2.30180\n",
      "[313]\tvalidation_0-mlogloss:2.30178\n",
      "[314]\tvalidation_0-mlogloss:2.30162\n",
      "[315]\tvalidation_0-mlogloss:2.30139\n",
      "[316]\tvalidation_0-mlogloss:2.30098\n",
      "[317]\tvalidation_0-mlogloss:2.30151\n",
      "[318]\tvalidation_0-mlogloss:2.30184\n",
      "[319]\tvalidation_0-mlogloss:2.30143\n",
      "[320]\tvalidation_0-mlogloss:2.30102\n",
      "[321]\tvalidation_0-mlogloss:2.30102\n",
      "[322]\tvalidation_0-mlogloss:2.30061\n",
      "[323]\tvalidation_0-mlogloss:2.30036\n",
      "[324]\tvalidation_0-mlogloss:2.29989\n",
      "[325]\tvalidation_0-mlogloss:2.29948\n",
      "[326]\tvalidation_0-mlogloss:2.29942\n",
      "[327]\tvalidation_0-mlogloss:2.29859\n",
      "[328]\tvalidation_0-mlogloss:2.29771\n",
      "[329]\tvalidation_0-mlogloss:2.29721\n",
      "[330]\tvalidation_0-mlogloss:2.29748\n",
      "[331]\tvalidation_0-mlogloss:2.29684\n",
      "[332]\tvalidation_0-mlogloss:2.29672\n",
      "[333]\tvalidation_0-mlogloss:2.29666\n",
      "[334]\tvalidation_0-mlogloss:2.29662\n",
      "[335]\tvalidation_0-mlogloss:2.29584\n",
      "[336]\tvalidation_0-mlogloss:2.29606\n",
      "[337]\tvalidation_0-mlogloss:2.29646\n",
      "[338]\tvalidation_0-mlogloss:2.29656\n",
      "[339]\tvalidation_0-mlogloss:2.29654\n",
      "[340]\tvalidation_0-mlogloss:2.29607\n",
      "[341]\tvalidation_0-mlogloss:2.29613\n",
      "[342]\tvalidation_0-mlogloss:2.29639\n",
      "[343]\tvalidation_0-mlogloss:2.29629\n",
      "[344]\tvalidation_0-mlogloss:2.29626\n",
      "[345]\tvalidation_0-mlogloss:2.29629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.24      0.25      1499\n",
      "           1       0.31      0.44      0.37      1499\n",
      "           2       0.40      0.40      0.40      1499\n",
      "           3       0.56      0.33      0.42      1499\n",
      "           4       0.86      0.87      0.86      1499\n",
      "           5       0.41      0.32      0.36      1499\n",
      "           6       0.49      0.56      0.53      1499\n",
      "           7       0.71      0.46      0.56      1499\n",
      "           8       0.29      0.43      0.34      1499\n",
      "           9       0.57      0.66      0.61      1499\n",
      "          10       0.36      0.18      0.24      1499\n",
      "          11       0.29      0.29      0.29      1499\n",
      "          12       0.28      0.35      0.31      1499\n",
      "          13       0.26      0.32      0.28      1499\n",
      "          14       0.21      0.20      0.20      1499\n",
      "          15       0.57      0.43      0.49      1499\n",
      "          16       0.44      0.32      0.37      1499\n",
      "          17       0.47      0.44      0.45      1499\n",
      "          18       0.33      0.41      0.37      1499\n",
      "          19       0.40      0.37      0.39      1499\n",
      "          20       0.80      0.93      0.86      1499\n",
      "          21       0.28      0.31      0.30      1499\n",
      "          22       0.31      0.22      0.26      1499\n",
      "          23       0.60      0.75      0.67      1499\n",
      "          24       0.51      0.38      0.44      1499\n",
      "          25       0.62      0.49      0.55      1499\n",
      "          26       0.24      0.19      0.21      1499\n",
      "          27       0.24      0.31      0.27      1499\n",
      "          28       0.32      0.31      0.31      1499\n",
      "          29       0.53      0.48      0.50      1499\n",
      "          30       0.33      0.27      0.30      1499\n",
      "          31       0.30      0.56      0.39      1499\n",
      "          32       0.28      0.25      0.26      1499\n",
      "          33       0.29      0.27      0.28      1499\n",
      "          34       0.65      0.62      0.64      1499\n",
      "          35       0.53      0.53      0.53      1499\n",
      "          36       0.71      0.81      0.76      1499\n",
      "          37       0.32      0.33      0.32      1499\n",
      "          38       0.37      0.32      0.34      1499\n",
      "          39       0.49      0.36      0.41      1499\n",
      "          40       0.80      0.60      0.68      1499\n",
      "          41       0.66      0.60      0.63      1499\n",
      "          42       0.37      0.27      0.31      1499\n",
      "          43       0.23      0.27      0.25      1499\n",
      "          44       0.47      0.62      0.53      1499\n",
      "          45       0.32      0.31      0.31      1499\n",
      "          46       0.47      0.44      0.45      1499\n",
      "          47       0.58      0.46      0.51      1499\n",
      "          48       0.60      0.68      0.64      1499\n",
      "          49       0.41      0.31      0.36      1499\n",
      "          50       0.42      0.41      0.41      1499\n",
      "          51       0.23      0.27      0.25      1499\n",
      "          52       0.25      0.27      0.26      1499\n",
      "          53       0.22      0.22      0.22      1499\n",
      "          54       0.44      0.41      0.43      1499\n",
      "          55       0.34      0.46      0.40      1499\n",
      "          56       0.17      0.15      0.16      1499\n",
      "          57       0.38      0.57      0.46      1499\n",
      "          58       0.94      0.86      0.90      1499\n",
      "          59       0.26      0.19      0.22      1499\n",
      "          60       0.38      0.30      0.34      1499\n",
      "          61       0.24      0.18      0.21      1499\n",
      "          62       0.60      0.78      0.68      1499\n",
      "          63       0.30      0.32      0.31      1499\n",
      "          64       0.24      0.16      0.19      1499\n",
      "          65       0.30      0.34      0.32      1499\n",
      "          66       0.76      0.66      0.71      1499\n",
      "          67       0.29      0.14      0.19      1499\n",
      "          68       0.52      0.60      0.56      1499\n",
      "          69       0.17      0.16      0.17      1499\n",
      "          70       0.56      0.24      0.34      1499\n",
      "          71       0.22      0.19      0.21      1499\n",
      "          72       0.26      0.25      0.25      1499\n",
      "          73       0.40      0.53      0.46      1499\n",
      "          74       0.18      0.47      0.26      1499\n",
      "\n",
      "    accuracy                           0.41    112425\n",
      "   macro avg       0.42      0.41      0.40    112425\n",
      "weighted avg       0.42      0.41      0.40    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7980b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46f7c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 13:47:22.263777: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 57600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7773/28125 [=======>......................] - ETA: 41s - loss: 2.7295"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28125/28125 [==============================] - 61s 2ms/step - loss: 2.0060 - val_loss: 2.6217\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 62s 2ms/step - loss: 1.3554 - val_loss: 2.7408\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 62s 2ms/step - loss: 1.2092 - val_loss: 2.8221\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 62s 2ms/step - loss: 1.1365 - val_loss: 2.8277\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 62s 2ms/step - loss: 1.0877 - val_loss: 2.8175\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 62s 2ms/step - loss: 1.0532 - val_loss: 2.9038\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 61s 2ms/step - loss: 1.0269 - val_loss: 2.9518\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 63s 2ms/step - loss: 1.0060 - val_loss: 2.7804\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 61s 2ms/step - loss: 0.9892 - val_loss: 2.9476\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 64s 2ms/step - loss: 0.9738 - val_loss: 2.8907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f312fe727d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/3514 [..............................] - ETA: 4:20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 5s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 13:57:49.589696: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 33727500 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.32      0.24      1499\n",
      "           1       0.42      0.29      0.34      1499\n",
      "           2       0.29      0.42      0.34      1499\n",
      "           3       0.57      0.31      0.40      1499\n",
      "           4       0.88      0.76      0.82      1499\n",
      "           5       0.44      0.39      0.41      1499\n",
      "           6       0.49      0.45      0.47      1499\n",
      "           7       0.59      0.28      0.38      1499\n",
      "           8       0.30      0.42      0.35      1499\n",
      "           9       0.53      0.66      0.58      1499\n",
      "          10       0.18      0.13      0.15      1499\n",
      "          11       0.39      0.40      0.39      1499\n",
      "          12       0.29      0.27      0.28      1499\n",
      "          13       0.20      0.21      0.20      1499\n",
      "          14       0.18      0.15      0.17      1499\n",
      "          15       0.51      0.25      0.33      1499\n",
      "          16       0.21      0.17      0.19      1499\n",
      "          17       0.51      0.20      0.29      1499\n",
      "          18       0.29      0.44      0.35      1499\n",
      "          19       0.18      0.09      0.12      1499\n",
      "          20       0.85      0.58      0.69      1499\n",
      "          21       0.26      0.37      0.30      1499\n",
      "          22       0.31      0.21      0.25      1499\n",
      "          23       0.60      0.74      0.66      1499\n",
      "          24       0.46      0.19      0.27      1499\n",
      "          25       0.59      0.45      0.51      1499\n",
      "          26       0.15      0.11      0.13      1499\n",
      "          27       0.23      0.35      0.28      1499\n",
      "          28       0.31      0.43      0.36      1499\n",
      "          29       0.63      0.26      0.37      1499\n",
      "          30       0.41      0.39      0.40      1499\n",
      "          31       0.30      0.39      0.34      1499\n",
      "          32       0.28      0.17      0.21      1499\n",
      "          33       0.18      0.22      0.19      1499\n",
      "          34       0.89      0.28      0.43      1499\n",
      "          35       0.62      0.36      0.45      1499\n",
      "          36       0.76      0.54      0.63      1499\n",
      "          37       0.27      0.31      0.29      1499\n",
      "          38       0.22      0.39      0.28      1499\n",
      "          39       0.31      0.39      0.35      1499\n",
      "          40       0.62      0.31      0.41      1499\n",
      "          41       0.91      0.48      0.63      1499\n",
      "          42       0.29      0.21      0.24      1499\n",
      "          43       0.21      0.33      0.26      1499\n",
      "          44       0.48      0.51      0.49      1499\n",
      "          45       0.18      0.19      0.19      1499\n",
      "          46       0.35      0.37      0.36      1499\n",
      "          47       0.56      0.34      0.42      1499\n",
      "          48       0.44      0.64      0.52      1499\n",
      "          49       0.26      0.30      0.28      1499\n",
      "          50       0.38      0.42      0.40      1499\n",
      "          51       0.12      0.07      0.09      1499\n",
      "          52       0.19      0.25      0.22      1499\n",
      "          53       0.18      0.31      0.23      1499\n",
      "          54       0.46      0.22      0.30      1499\n",
      "          55       0.29      0.42      0.34      1499\n",
      "          56       0.13      0.14      0.14      1499\n",
      "          57       0.42      0.54      0.47      1499\n",
      "          58       0.91      0.66      0.76      1499\n",
      "          59       0.24      0.10      0.14      1499\n",
      "          60       0.31      0.16      0.21      1499\n",
      "          61       0.27      0.19      0.23      1499\n",
      "          62       0.71      0.47      0.56      1499\n",
      "          63       0.27      0.34      0.30      1499\n",
      "          64       0.10      0.14      0.11      1499\n",
      "          65       0.27      0.39      0.32      1499\n",
      "          66       0.68      0.58      0.63      1499\n",
      "          67       0.23      0.27      0.25      1499\n",
      "          68       0.31      0.59      0.41      1499\n",
      "          69       0.11      0.25      0.15      1499\n",
      "          70       0.36      0.15      0.22      1499\n",
      "          71       0.25      0.09      0.14      1499\n",
      "          72       0.24      0.31      0.27      1499\n",
      "          73       0.39      0.51      0.44      1499\n",
      "          74       0.19      0.38      0.25      1499\n",
      "\n",
      "    accuracy                           0.34    112425\n",
      "   macro avg       0.38      0.34      0.34    112425\n",
      "weighted avg       0.38      0.34      0.34    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78467/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_78467/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_78467/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.79791\n",
      "[1]\tvalidation_0-mlogloss:3.57443\n",
      "[2]\tvalidation_0-mlogloss:3.41813\n",
      "[3]\tvalidation_0-mlogloss:3.29974\n",
      "[4]\tvalidation_0-mlogloss:3.20704\n",
      "[5]\tvalidation_0-mlogloss:3.12124\n",
      "[6]\tvalidation_0-mlogloss:3.05021\n",
      "[7]\tvalidation_0-mlogloss:2.98905\n",
      "[8]\tvalidation_0-mlogloss:2.93709\n",
      "[9]\tvalidation_0-mlogloss:2.88638\n",
      "[10]\tvalidation_0-mlogloss:2.84592\n",
      "[11]\tvalidation_0-mlogloss:2.80431\n",
      "[12]\tvalidation_0-mlogloss:2.76730\n",
      "[13]\tvalidation_0-mlogloss:2.73201\n",
      "[14]\tvalidation_0-mlogloss:2.69951\n",
      "[15]\tvalidation_0-mlogloss:2.66902\n",
      "[16]\tvalidation_0-mlogloss:2.63944\n",
      "[17]\tvalidation_0-mlogloss:2.61035\n",
      "[18]\tvalidation_0-mlogloss:2.58382\n",
      "[19]\tvalidation_0-mlogloss:2.55811\n",
      "[20]\tvalidation_0-mlogloss:2.53627\n",
      "[21]\tvalidation_0-mlogloss:2.51469\n",
      "[22]\tvalidation_0-mlogloss:2.49290\n",
      "[23]\tvalidation_0-mlogloss:2.47358\n",
      "[24]\tvalidation_0-mlogloss:2.45639\n",
      "[25]\tvalidation_0-mlogloss:2.43897\n",
      "[26]\tvalidation_0-mlogloss:2.41846\n",
      "[27]\tvalidation_0-mlogloss:2.39989\n",
      "[28]\tvalidation_0-mlogloss:2.38230\n",
      "[29]\tvalidation_0-mlogloss:2.36607\n",
      "[30]\tvalidation_0-mlogloss:2.34910\n",
      "[31]\tvalidation_0-mlogloss:2.33250\n",
      "[32]\tvalidation_0-mlogloss:2.31844\n",
      "[33]\tvalidation_0-mlogloss:2.30244\n",
      "[34]\tvalidation_0-mlogloss:2.28954\n",
      "[35]\tvalidation_0-mlogloss:2.27359\n",
      "[36]\tvalidation_0-mlogloss:2.25892\n",
      "[37]\tvalidation_0-mlogloss:2.24547\n",
      "[38]\tvalidation_0-mlogloss:2.23290\n",
      "[39]\tvalidation_0-mlogloss:2.21818\n",
      "[40]\tvalidation_0-mlogloss:2.20544\n",
      "[41]\tvalidation_0-mlogloss:2.19322\n",
      "[42]\tvalidation_0-mlogloss:2.17942\n",
      "[43]\tvalidation_0-mlogloss:2.16596\n",
      "[44]\tvalidation_0-mlogloss:2.15429\n",
      "[45]\tvalidation_0-mlogloss:2.14305\n",
      "[46]\tvalidation_0-mlogloss:2.13326\n",
      "[47]\tvalidation_0-mlogloss:2.12395\n",
      "[48]\tvalidation_0-mlogloss:2.11304\n",
      "[49]\tvalidation_0-mlogloss:2.10243\n",
      "[50]\tvalidation_0-mlogloss:2.09238\n",
      "[51]\tvalidation_0-mlogloss:2.08320\n",
      "[52]\tvalidation_0-mlogloss:2.07352\n",
      "[53]\tvalidation_0-mlogloss:2.06489\n",
      "[54]\tvalidation_0-mlogloss:2.05432\n",
      "[55]\tvalidation_0-mlogloss:2.04510\n",
      "[56]\tvalidation_0-mlogloss:2.03481\n",
      "[57]\tvalidation_0-mlogloss:2.02642\n",
      "[58]\tvalidation_0-mlogloss:2.01533\n",
      "[59]\tvalidation_0-mlogloss:2.00603\n",
      "[60]\tvalidation_0-mlogloss:1.99707\n",
      "[61]\tvalidation_0-mlogloss:1.98650\n",
      "[62]\tvalidation_0-mlogloss:1.97764\n",
      "[63]\tvalidation_0-mlogloss:1.97069\n",
      "[64]\tvalidation_0-mlogloss:1.96339\n",
      "[65]\tvalidation_0-mlogloss:1.95444\n",
      "[66]\tvalidation_0-mlogloss:1.94715\n",
      "[67]\tvalidation_0-mlogloss:1.94005\n",
      "[68]\tvalidation_0-mlogloss:1.93301\n",
      "[69]\tvalidation_0-mlogloss:1.92529\n",
      "[70]\tvalidation_0-mlogloss:1.91748\n",
      "[71]\tvalidation_0-mlogloss:1.91003\n",
      "[72]\tvalidation_0-mlogloss:1.90468\n",
      "[73]\tvalidation_0-mlogloss:1.89599\n",
      "[74]\tvalidation_0-mlogloss:1.89066\n",
      "[75]\tvalidation_0-mlogloss:1.88502\n",
      "[76]\tvalidation_0-mlogloss:1.87808\n",
      "[77]\tvalidation_0-mlogloss:1.87125\n",
      "[78]\tvalidation_0-mlogloss:1.86522\n",
      "[79]\tvalidation_0-mlogloss:1.85940\n",
      "[80]\tvalidation_0-mlogloss:1.85256\n",
      "[81]\tvalidation_0-mlogloss:1.84574\n",
      "[82]\tvalidation_0-mlogloss:1.83960\n",
      "[83]\tvalidation_0-mlogloss:1.83274\n",
      "[84]\tvalidation_0-mlogloss:1.82856\n",
      "[85]\tvalidation_0-mlogloss:1.82301\n",
      "[86]\tvalidation_0-mlogloss:1.81763\n",
      "[87]\tvalidation_0-mlogloss:1.81311\n",
      "[88]\tvalidation_0-mlogloss:1.80899\n",
      "[89]\tvalidation_0-mlogloss:1.80210\n",
      "[90]\tvalidation_0-mlogloss:1.79604\n",
      "[91]\tvalidation_0-mlogloss:1.79086\n",
      "[92]\tvalidation_0-mlogloss:1.78551\n",
      "[93]\tvalidation_0-mlogloss:1.77876\n",
      "[94]\tvalidation_0-mlogloss:1.77367\n",
      "[95]\tvalidation_0-mlogloss:1.76863\n",
      "[96]\tvalidation_0-mlogloss:1.76297\n",
      "[97]\tvalidation_0-mlogloss:1.75876\n",
      "[98]\tvalidation_0-mlogloss:1.75482\n",
      "[99]\tvalidation_0-mlogloss:1.74963\n",
      "[100]\tvalidation_0-mlogloss:1.74541\n",
      "[101]\tvalidation_0-mlogloss:1.74061\n",
      "[102]\tvalidation_0-mlogloss:1.73518\n",
      "[103]\tvalidation_0-mlogloss:1.73020\n",
      "[104]\tvalidation_0-mlogloss:1.72648\n",
      "[105]\tvalidation_0-mlogloss:1.72190\n",
      "[106]\tvalidation_0-mlogloss:1.71696\n",
      "[107]\tvalidation_0-mlogloss:1.71303\n",
      "[108]\tvalidation_0-mlogloss:1.70799\n",
      "[109]\tvalidation_0-mlogloss:1.70298\n",
      "[110]\tvalidation_0-mlogloss:1.69893\n",
      "[111]\tvalidation_0-mlogloss:1.69356\n",
      "[112]\tvalidation_0-mlogloss:1.68803\n",
      "[113]\tvalidation_0-mlogloss:1.68290\n",
      "[114]\tvalidation_0-mlogloss:1.67937\n",
      "[115]\tvalidation_0-mlogloss:1.67515\n",
      "[116]\tvalidation_0-mlogloss:1.67153\n",
      "[117]\tvalidation_0-mlogloss:1.66740\n",
      "[118]\tvalidation_0-mlogloss:1.66396\n",
      "[119]\tvalidation_0-mlogloss:1.65959\n",
      "[120]\tvalidation_0-mlogloss:1.65582\n",
      "[121]\tvalidation_0-mlogloss:1.65178\n",
      "[122]\tvalidation_0-mlogloss:1.64769\n",
      "[123]\tvalidation_0-mlogloss:1.64407\n",
      "[124]\tvalidation_0-mlogloss:1.64076\n",
      "[125]\tvalidation_0-mlogloss:1.63709\n",
      "[126]\tvalidation_0-mlogloss:1.63223\n",
      "[127]\tvalidation_0-mlogloss:1.62909\n",
      "[128]\tvalidation_0-mlogloss:1.62536\n",
      "[129]\tvalidation_0-mlogloss:1.62298\n",
      "[130]\tvalidation_0-mlogloss:1.61986\n",
      "[131]\tvalidation_0-mlogloss:1.61695\n",
      "[132]\tvalidation_0-mlogloss:1.61354\n",
      "[133]\tvalidation_0-mlogloss:1.61055\n",
      "[134]\tvalidation_0-mlogloss:1.60722\n",
      "[135]\tvalidation_0-mlogloss:1.60414\n",
      "[136]\tvalidation_0-mlogloss:1.60114\n",
      "[137]\tvalidation_0-mlogloss:1.59798\n",
      "[138]\tvalidation_0-mlogloss:1.59444\n",
      "[139]\tvalidation_0-mlogloss:1.59051\n",
      "[140]\tvalidation_0-mlogloss:1.58815\n",
      "[141]\tvalidation_0-mlogloss:1.58480\n",
      "[142]\tvalidation_0-mlogloss:1.58223\n",
      "[143]\tvalidation_0-mlogloss:1.57901\n",
      "[144]\tvalidation_0-mlogloss:1.57657\n",
      "[145]\tvalidation_0-mlogloss:1.57345\n",
      "[146]\tvalidation_0-mlogloss:1.57079\n",
      "[147]\tvalidation_0-mlogloss:1.56832\n",
      "[148]\tvalidation_0-mlogloss:1.56576\n",
      "[149]\tvalidation_0-mlogloss:1.56284\n",
      "[150]\tvalidation_0-mlogloss:1.56018\n",
      "[151]\tvalidation_0-mlogloss:1.55813\n",
      "[152]\tvalidation_0-mlogloss:1.55588\n",
      "[153]\tvalidation_0-mlogloss:1.55303\n",
      "[154]\tvalidation_0-mlogloss:1.55018\n",
      "[155]\tvalidation_0-mlogloss:1.54737\n",
      "[156]\tvalidation_0-mlogloss:1.54467\n",
      "[157]\tvalidation_0-mlogloss:1.54259\n",
      "[158]\tvalidation_0-mlogloss:1.54010\n",
      "[159]\tvalidation_0-mlogloss:1.53761\n",
      "[160]\tvalidation_0-mlogloss:1.53625\n",
      "[161]\tvalidation_0-mlogloss:1.53283\n",
      "[162]\tvalidation_0-mlogloss:1.52985\n",
      "[163]\tvalidation_0-mlogloss:1.52667\n",
      "[164]\tvalidation_0-mlogloss:1.52453\n",
      "[165]\tvalidation_0-mlogloss:1.52212\n",
      "[166]\tvalidation_0-mlogloss:1.51950\n",
      "[167]\tvalidation_0-mlogloss:1.51703\n",
      "[168]\tvalidation_0-mlogloss:1.51545\n",
      "[169]\tvalidation_0-mlogloss:1.51342\n",
      "[170]\tvalidation_0-mlogloss:1.51094\n",
      "[171]\tvalidation_0-mlogloss:1.50943\n",
      "[172]\tvalidation_0-mlogloss:1.50720\n",
      "[173]\tvalidation_0-mlogloss:1.50483\n",
      "[174]\tvalidation_0-mlogloss:1.50218\n",
      "[175]\tvalidation_0-mlogloss:1.50021\n",
      "[176]\tvalidation_0-mlogloss:1.49803\n",
      "[177]\tvalidation_0-mlogloss:1.49603\n",
      "[178]\tvalidation_0-mlogloss:1.49409\n",
      "[179]\tvalidation_0-mlogloss:1.49145\n",
      "[180]\tvalidation_0-mlogloss:1.48963\n",
      "[181]\tvalidation_0-mlogloss:1.48722\n",
      "[182]\tvalidation_0-mlogloss:1.48488\n",
      "[183]\tvalidation_0-mlogloss:1.48280\n",
      "[184]\tvalidation_0-mlogloss:1.48031\n",
      "[185]\tvalidation_0-mlogloss:1.47872\n",
      "[186]\tvalidation_0-mlogloss:1.47669\n",
      "[187]\tvalidation_0-mlogloss:1.47501\n",
      "[188]\tvalidation_0-mlogloss:1.47326\n",
      "[189]\tvalidation_0-mlogloss:1.47186\n",
      "[190]\tvalidation_0-mlogloss:1.47024\n",
      "[191]\tvalidation_0-mlogloss:1.46835\n",
      "[192]\tvalidation_0-mlogloss:1.46619\n",
      "[193]\tvalidation_0-mlogloss:1.46356\n",
      "[194]\tvalidation_0-mlogloss:1.46215\n",
      "[195]\tvalidation_0-mlogloss:1.46071\n",
      "[196]\tvalidation_0-mlogloss:1.45889\n",
      "[197]\tvalidation_0-mlogloss:1.45750\n",
      "[198]\tvalidation_0-mlogloss:1.45603\n",
      "[199]\tvalidation_0-mlogloss:1.45494\n",
      "[200]\tvalidation_0-mlogloss:1.45380\n",
      "[201]\tvalidation_0-mlogloss:1.45213\n",
      "[202]\tvalidation_0-mlogloss:1.45150\n",
      "[203]\tvalidation_0-mlogloss:1.44886\n",
      "[204]\tvalidation_0-mlogloss:1.44783\n",
      "[205]\tvalidation_0-mlogloss:1.44616\n",
      "[206]\tvalidation_0-mlogloss:1.44523\n",
      "[207]\tvalidation_0-mlogloss:1.44393\n",
      "[208]\tvalidation_0-mlogloss:1.44152\n",
      "[209]\tvalidation_0-mlogloss:1.43988\n",
      "[210]\tvalidation_0-mlogloss:1.43846\n",
      "[211]\tvalidation_0-mlogloss:1.43636\n",
      "[212]\tvalidation_0-mlogloss:1.43453\n",
      "[213]\tvalidation_0-mlogloss:1.43256\n",
      "[214]\tvalidation_0-mlogloss:1.43148\n",
      "[215]\tvalidation_0-mlogloss:1.42972\n",
      "[216]\tvalidation_0-mlogloss:1.42827\n",
      "[217]\tvalidation_0-mlogloss:1.42667\n",
      "[218]\tvalidation_0-mlogloss:1.42538\n",
      "[219]\tvalidation_0-mlogloss:1.42418\n",
      "[220]\tvalidation_0-mlogloss:1.42266\n",
      "[221]\tvalidation_0-mlogloss:1.42195\n",
      "[222]\tvalidation_0-mlogloss:1.42031\n",
      "[223]\tvalidation_0-mlogloss:1.41913\n",
      "[224]\tvalidation_0-mlogloss:1.41805\n",
      "[225]\tvalidation_0-mlogloss:1.41701\n",
      "[226]\tvalidation_0-mlogloss:1.41512\n",
      "[227]\tvalidation_0-mlogloss:1.41370\n",
      "[228]\tvalidation_0-mlogloss:1.41224\n",
      "[229]\tvalidation_0-mlogloss:1.41098\n",
      "[230]\tvalidation_0-mlogloss:1.40955\n",
      "[231]\tvalidation_0-mlogloss:1.40844\n",
      "[232]\tvalidation_0-mlogloss:1.40723\n",
      "[233]\tvalidation_0-mlogloss:1.40534\n",
      "[234]\tvalidation_0-mlogloss:1.40440\n",
      "[235]\tvalidation_0-mlogloss:1.40274\n",
      "[236]\tvalidation_0-mlogloss:1.40200\n",
      "[237]\tvalidation_0-mlogloss:1.40081\n",
      "[238]\tvalidation_0-mlogloss:1.39958\n",
      "[239]\tvalidation_0-mlogloss:1.39912\n",
      "[240]\tvalidation_0-mlogloss:1.39810\n",
      "[241]\tvalidation_0-mlogloss:1.39720\n",
      "[242]\tvalidation_0-mlogloss:1.39600\n",
      "[243]\tvalidation_0-mlogloss:1.39473\n",
      "[244]\tvalidation_0-mlogloss:1.39405\n",
      "[245]\tvalidation_0-mlogloss:1.39217\n",
      "[246]\tvalidation_0-mlogloss:1.39077\n",
      "[247]\tvalidation_0-mlogloss:1.38966\n",
      "[248]\tvalidation_0-mlogloss:1.38846\n",
      "[249]\tvalidation_0-mlogloss:1.38759\n",
      "[250]\tvalidation_0-mlogloss:1.38580\n",
      "[251]\tvalidation_0-mlogloss:1.38479\n",
      "[252]\tvalidation_0-mlogloss:1.38380\n",
      "[253]\tvalidation_0-mlogloss:1.38289\n",
      "[254]\tvalidation_0-mlogloss:1.38185\n",
      "[255]\tvalidation_0-mlogloss:1.38076\n",
      "[256]\tvalidation_0-mlogloss:1.37994\n",
      "[257]\tvalidation_0-mlogloss:1.37906\n",
      "[258]\tvalidation_0-mlogloss:1.37837\n",
      "[259]\tvalidation_0-mlogloss:1.37743\n",
      "[260]\tvalidation_0-mlogloss:1.37685\n",
      "[261]\tvalidation_0-mlogloss:1.37562\n",
      "[262]\tvalidation_0-mlogloss:1.37486\n",
      "[263]\tvalidation_0-mlogloss:1.37365\n",
      "[264]\tvalidation_0-mlogloss:1.37307\n",
      "[265]\tvalidation_0-mlogloss:1.37216\n",
      "[266]\tvalidation_0-mlogloss:1.37101\n",
      "[267]\tvalidation_0-mlogloss:1.36968\n",
      "[268]\tvalidation_0-mlogloss:1.36918\n",
      "[269]\tvalidation_0-mlogloss:1.36796\n",
      "[270]\tvalidation_0-mlogloss:1.36727\n",
      "[271]\tvalidation_0-mlogloss:1.36615\n",
      "[272]\tvalidation_0-mlogloss:1.36522\n",
      "[273]\tvalidation_0-mlogloss:1.36380\n",
      "[274]\tvalidation_0-mlogloss:1.36381\n",
      "[275]\tvalidation_0-mlogloss:1.36322\n",
      "[276]\tvalidation_0-mlogloss:1.36213\n",
      "[277]\tvalidation_0-mlogloss:1.36158\n",
      "[278]\tvalidation_0-mlogloss:1.36144\n",
      "[279]\tvalidation_0-mlogloss:1.36132\n",
      "[280]\tvalidation_0-mlogloss:1.36017\n",
      "[281]\tvalidation_0-mlogloss:1.35962\n",
      "[282]\tvalidation_0-mlogloss:1.35865\n",
      "[283]\tvalidation_0-mlogloss:1.35772\n",
      "[284]\tvalidation_0-mlogloss:1.35664\n",
      "[285]\tvalidation_0-mlogloss:1.35595\n",
      "[286]\tvalidation_0-mlogloss:1.35506\n",
      "[287]\tvalidation_0-mlogloss:1.35421\n",
      "[288]\tvalidation_0-mlogloss:1.35337\n",
      "[289]\tvalidation_0-mlogloss:1.35256\n",
      "[290]\tvalidation_0-mlogloss:1.35152\n",
      "[291]\tvalidation_0-mlogloss:1.35068\n",
      "[292]\tvalidation_0-mlogloss:1.35005\n",
      "[293]\tvalidation_0-mlogloss:1.34950\n",
      "[294]\tvalidation_0-mlogloss:1.34792\n",
      "[295]\tvalidation_0-mlogloss:1.34663\n",
      "[296]\tvalidation_0-mlogloss:1.34590\n",
      "[297]\tvalidation_0-mlogloss:1.34535\n",
      "[298]\tvalidation_0-mlogloss:1.34456\n",
      "[299]\tvalidation_0-mlogloss:1.34376\n",
      "[300]\tvalidation_0-mlogloss:1.34258\n",
      "[301]\tvalidation_0-mlogloss:1.34198\n",
      "[302]\tvalidation_0-mlogloss:1.34093\n",
      "[303]\tvalidation_0-mlogloss:1.34025\n",
      "[304]\tvalidation_0-mlogloss:1.33960\n",
      "[305]\tvalidation_0-mlogloss:1.33952\n",
      "[306]\tvalidation_0-mlogloss:1.33910\n",
      "[307]\tvalidation_0-mlogloss:1.33803\n",
      "[308]\tvalidation_0-mlogloss:1.33794\n",
      "[309]\tvalidation_0-mlogloss:1.33691\n",
      "[310]\tvalidation_0-mlogloss:1.33647\n",
      "[311]\tvalidation_0-mlogloss:1.33598\n",
      "[312]\tvalidation_0-mlogloss:1.33527\n",
      "[313]\tvalidation_0-mlogloss:1.33383\n",
      "[314]\tvalidation_0-mlogloss:1.33283\n",
      "[315]\tvalidation_0-mlogloss:1.33213\n",
      "[316]\tvalidation_0-mlogloss:1.33139\n",
      "[317]\tvalidation_0-mlogloss:1.33079\n",
      "[318]\tvalidation_0-mlogloss:1.33038\n",
      "[319]\tvalidation_0-mlogloss:1.33032\n",
      "[320]\tvalidation_0-mlogloss:1.32983\n",
      "[321]\tvalidation_0-mlogloss:1.32928\n",
      "[322]\tvalidation_0-mlogloss:1.32878\n",
      "[323]\tvalidation_0-mlogloss:1.32770\n",
      "[324]\tvalidation_0-mlogloss:1.32730\n",
      "[325]\tvalidation_0-mlogloss:1.32645\n",
      "[326]\tvalidation_0-mlogloss:1.32571\n",
      "[327]\tvalidation_0-mlogloss:1.32471\n",
      "[328]\tvalidation_0-mlogloss:1.32398\n",
      "[329]\tvalidation_0-mlogloss:1.32313\n",
      "[330]\tvalidation_0-mlogloss:1.32263\n",
      "[331]\tvalidation_0-mlogloss:1.32199\n",
      "[332]\tvalidation_0-mlogloss:1.32129\n",
      "[333]\tvalidation_0-mlogloss:1.32105\n",
      "[334]\tvalidation_0-mlogloss:1.32053\n",
      "[335]\tvalidation_0-mlogloss:1.32002\n",
      "[336]\tvalidation_0-mlogloss:1.31901\n",
      "[337]\tvalidation_0-mlogloss:1.31827\n",
      "[338]\tvalidation_0-mlogloss:1.31787\n",
      "[339]\tvalidation_0-mlogloss:1.31740\n",
      "[340]\tvalidation_0-mlogloss:1.31681\n",
      "[341]\tvalidation_0-mlogloss:1.31643\n",
      "[342]\tvalidation_0-mlogloss:1.31597\n",
      "[343]\tvalidation_0-mlogloss:1.31525\n",
      "[344]\tvalidation_0-mlogloss:1.31525\n",
      "[345]\tvalidation_0-mlogloss:1.31466\n",
      "[346]\tvalidation_0-mlogloss:1.31445\n",
      "[347]\tvalidation_0-mlogloss:1.31342\n",
      "[348]\tvalidation_0-mlogloss:1.31318\n",
      "[349]\tvalidation_0-mlogloss:1.31271\n",
      "[350]\tvalidation_0-mlogloss:1.31198\n",
      "[351]\tvalidation_0-mlogloss:1.31115\n",
      "[352]\tvalidation_0-mlogloss:1.31060\n",
      "[353]\tvalidation_0-mlogloss:1.30978\n",
      "[354]\tvalidation_0-mlogloss:1.30953\n",
      "[355]\tvalidation_0-mlogloss:1.30936\n",
      "[356]\tvalidation_0-mlogloss:1.30951\n",
      "[357]\tvalidation_0-mlogloss:1.30912\n",
      "[358]\tvalidation_0-mlogloss:1.30880\n",
      "[359]\tvalidation_0-mlogloss:1.30862\n",
      "[360]\tvalidation_0-mlogloss:1.30787\n",
      "[361]\tvalidation_0-mlogloss:1.30764\n",
      "[362]\tvalidation_0-mlogloss:1.30713\n",
      "[363]\tvalidation_0-mlogloss:1.30659\n",
      "[364]\tvalidation_0-mlogloss:1.30615\n",
      "[365]\tvalidation_0-mlogloss:1.30545\n",
      "[366]\tvalidation_0-mlogloss:1.30508\n",
      "[367]\tvalidation_0-mlogloss:1.30461\n",
      "[368]\tvalidation_0-mlogloss:1.30441\n",
      "[369]\tvalidation_0-mlogloss:1.30367\n",
      "[370]\tvalidation_0-mlogloss:1.30366\n",
      "[371]\tvalidation_0-mlogloss:1.30359\n",
      "[372]\tvalidation_0-mlogloss:1.30291\n",
      "[373]\tvalidation_0-mlogloss:1.30287\n",
      "[374]\tvalidation_0-mlogloss:1.30242\n",
      "[375]\tvalidation_0-mlogloss:1.30172\n",
      "[376]\tvalidation_0-mlogloss:1.30099\n",
      "[377]\tvalidation_0-mlogloss:1.30079\n",
      "[378]\tvalidation_0-mlogloss:1.30007\n",
      "[379]\tvalidation_0-mlogloss:1.29995\n",
      "[380]\tvalidation_0-mlogloss:1.29961\n",
      "[381]\tvalidation_0-mlogloss:1.29906\n",
      "[382]\tvalidation_0-mlogloss:1.29830\n",
      "[383]\tvalidation_0-mlogloss:1.29811\n",
      "[384]\tvalidation_0-mlogloss:1.29796\n",
      "[385]\tvalidation_0-mlogloss:1.29762\n",
      "[386]\tvalidation_0-mlogloss:1.29731\n",
      "[387]\tvalidation_0-mlogloss:1.29681\n",
      "[388]\tvalidation_0-mlogloss:1.29609\n",
      "[389]\tvalidation_0-mlogloss:1.29549\n",
      "[390]\tvalidation_0-mlogloss:1.29502\n",
      "[391]\tvalidation_0-mlogloss:1.29437\n",
      "[392]\tvalidation_0-mlogloss:1.29413\n",
      "[393]\tvalidation_0-mlogloss:1.29409\n",
      "[394]\tvalidation_0-mlogloss:1.29380\n",
      "[395]\tvalidation_0-mlogloss:1.29352\n",
      "[396]\tvalidation_0-mlogloss:1.29312\n",
      "[397]\tvalidation_0-mlogloss:1.29275\n",
      "[398]\tvalidation_0-mlogloss:1.29264\n",
      "[399]\tvalidation_0-mlogloss:1.29215\n",
      "[400]\tvalidation_0-mlogloss:1.29185\n",
      "[401]\tvalidation_0-mlogloss:1.29095\n",
      "[402]\tvalidation_0-mlogloss:1.29076\n",
      "[403]\tvalidation_0-mlogloss:1.29049\n",
      "[404]\tvalidation_0-mlogloss:1.28987\n",
      "[405]\tvalidation_0-mlogloss:1.28953\n",
      "[406]\tvalidation_0-mlogloss:1.28910\n",
      "[407]\tvalidation_0-mlogloss:1.28858\n",
      "[408]\tvalidation_0-mlogloss:1.28814\n",
      "[409]\tvalidation_0-mlogloss:1.28794\n",
      "[410]\tvalidation_0-mlogloss:1.28721\n",
      "[411]\tvalidation_0-mlogloss:1.28704\n",
      "[412]\tvalidation_0-mlogloss:1.28654\n",
      "[413]\tvalidation_0-mlogloss:1.28626\n",
      "[414]\tvalidation_0-mlogloss:1.28598\n",
      "[415]\tvalidation_0-mlogloss:1.28572\n",
      "[416]\tvalidation_0-mlogloss:1.28535\n",
      "[417]\tvalidation_0-mlogloss:1.28507\n",
      "[418]\tvalidation_0-mlogloss:1.28491\n",
      "[419]\tvalidation_0-mlogloss:1.28461\n",
      "[420]\tvalidation_0-mlogloss:1.28434\n",
      "[421]\tvalidation_0-mlogloss:1.28374\n",
      "[422]\tvalidation_0-mlogloss:1.28338\n",
      "[423]\tvalidation_0-mlogloss:1.28299\n",
      "[424]\tvalidation_0-mlogloss:1.28252\n",
      "[425]\tvalidation_0-mlogloss:1.28245\n",
      "[426]\tvalidation_0-mlogloss:1.28180\n",
      "[427]\tvalidation_0-mlogloss:1.28124\n",
      "[428]\tvalidation_0-mlogloss:1.28096\n",
      "[429]\tvalidation_0-mlogloss:1.28039\n",
      "[430]\tvalidation_0-mlogloss:1.28070\n",
      "[431]\tvalidation_0-mlogloss:1.28050\n",
      "[432]\tvalidation_0-mlogloss:1.28015\n",
      "[433]\tvalidation_0-mlogloss:1.27967\n",
      "[434]\tvalidation_0-mlogloss:1.27956\n",
      "[435]\tvalidation_0-mlogloss:1.27954\n",
      "[436]\tvalidation_0-mlogloss:1.27935\n",
      "[437]\tvalidation_0-mlogloss:1.27894\n",
      "[438]\tvalidation_0-mlogloss:1.27838\n",
      "[439]\tvalidation_0-mlogloss:1.27806\n",
      "[440]\tvalidation_0-mlogloss:1.27752\n",
      "[441]\tvalidation_0-mlogloss:1.27732\n",
      "[442]\tvalidation_0-mlogloss:1.27694\n",
      "[443]\tvalidation_0-mlogloss:1.27665\n",
      "[444]\tvalidation_0-mlogloss:1.27665\n",
      "[445]\tvalidation_0-mlogloss:1.27604\n",
      "[446]\tvalidation_0-mlogloss:1.27550\n",
      "[447]\tvalidation_0-mlogloss:1.27553\n",
      "[448]\tvalidation_0-mlogloss:1.27510\n",
      "[449]\tvalidation_0-mlogloss:1.27490\n",
      "[450]\tvalidation_0-mlogloss:1.27433\n",
      "[451]\tvalidation_0-mlogloss:1.27391\n",
      "[452]\tvalidation_0-mlogloss:1.27370\n",
      "[453]\tvalidation_0-mlogloss:1.27389\n",
      "[454]\tvalidation_0-mlogloss:1.27364\n",
      "[455]\tvalidation_0-mlogloss:1.27374\n",
      "[456]\tvalidation_0-mlogloss:1.27363\n",
      "[457]\tvalidation_0-mlogloss:1.27307\n",
      "[458]\tvalidation_0-mlogloss:1.27272\n",
      "[459]\tvalidation_0-mlogloss:1.27257\n",
      "[460]\tvalidation_0-mlogloss:1.27220\n",
      "[461]\tvalidation_0-mlogloss:1.27170\n",
      "[462]\tvalidation_0-mlogloss:1.27161\n",
      "[463]\tvalidation_0-mlogloss:1.27135\n",
      "[464]\tvalidation_0-mlogloss:1.27127\n",
      "[465]\tvalidation_0-mlogloss:1.27117\n",
      "[466]\tvalidation_0-mlogloss:1.27100\n",
      "[467]\tvalidation_0-mlogloss:1.27098\n",
      "[468]\tvalidation_0-mlogloss:1.27064\n",
      "[469]\tvalidation_0-mlogloss:1.27034\n",
      "[470]\tvalidation_0-mlogloss:1.26974\n",
      "[471]\tvalidation_0-mlogloss:1.26969\n",
      "[472]\tvalidation_0-mlogloss:1.26912\n",
      "[473]\tvalidation_0-mlogloss:1.26912\n",
      "[474]\tvalidation_0-mlogloss:1.26879\n",
      "[475]\tvalidation_0-mlogloss:1.26878\n",
      "[476]\tvalidation_0-mlogloss:1.26843\n",
      "[477]\tvalidation_0-mlogloss:1.26854\n",
      "[478]\tvalidation_0-mlogloss:1.26860\n",
      "[479]\tvalidation_0-mlogloss:1.26847\n",
      "[480]\tvalidation_0-mlogloss:1.26804\n",
      "[481]\tvalidation_0-mlogloss:1.26749\n",
      "[482]\tvalidation_0-mlogloss:1.26684\n",
      "[483]\tvalidation_0-mlogloss:1.26651\n",
      "[484]\tvalidation_0-mlogloss:1.26611\n",
      "[485]\tvalidation_0-mlogloss:1.26592\n",
      "[486]\tvalidation_0-mlogloss:1.26587\n",
      "[487]\tvalidation_0-mlogloss:1.26549\n",
      "[488]\tvalidation_0-mlogloss:1.26531\n",
      "[489]\tvalidation_0-mlogloss:1.26505\n",
      "[490]\tvalidation_0-mlogloss:1.26480\n",
      "[491]\tvalidation_0-mlogloss:1.26438\n",
      "[492]\tvalidation_0-mlogloss:1.26402\n",
      "[493]\tvalidation_0-mlogloss:1.26352\n",
      "[494]\tvalidation_0-mlogloss:1.26326\n",
      "[495]\tvalidation_0-mlogloss:1.26301\n",
      "[496]\tvalidation_0-mlogloss:1.26273\n",
      "[497]\tvalidation_0-mlogloss:1.26278\n",
      "[498]\tvalidation_0-mlogloss:1.26285\n",
      "[499]\tvalidation_0-mlogloss:1.26245\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      1499\n",
      "           1       0.78      0.85      0.81      1499\n",
      "           2       0.59      0.70      0.64      1499\n",
      "           3       0.86      0.55      0.67      1499\n",
      "           4       0.91      0.95      0.93      1499\n",
      "           5       0.94      0.79      0.86      1499\n",
      "           6       0.88      0.92      0.90      1499\n",
      "           7       0.91      0.75      0.82      1499\n",
      "           8       0.47      0.79      0.59      1499\n",
      "           9       0.81      0.74      0.78      1499\n",
      "          10       0.92      0.71      0.80      1499\n",
      "          11       0.96      0.89      0.92      1499\n",
      "          12       0.72      0.77      0.74      1499\n",
      "          13       0.61      0.59      0.60      1499\n",
      "          14       0.55      0.71      0.62      1499\n",
      "          15       0.87      0.59      0.71      1499\n",
      "          16       0.49      0.39      0.44      1499\n",
      "          17       0.66      0.49      0.56      1499\n",
      "          18       0.75      0.78      0.76      1499\n",
      "          19       0.65      0.57      0.61      1499\n",
      "          20       0.95      0.98      0.97      1499\n",
      "          21       0.64      0.72      0.67      1499\n",
      "          22       0.67      0.80      0.73      1499\n",
      "          23       0.95      0.94      0.94      1499\n",
      "          24       0.78      0.49      0.60      1499\n",
      "          25       0.76      0.68      0.72      1499\n",
      "          26       0.77      0.50      0.60      1499\n",
      "          27       0.66      0.54      0.59      1499\n",
      "          28       0.67      0.73      0.70      1499\n",
      "          29       0.74      0.66      0.69      1499\n",
      "          30       0.69      0.77      0.72      1499\n",
      "          31       0.49      0.82      0.61      1499\n",
      "          32       0.84      0.86      0.85      1499\n",
      "          33       0.73      0.60      0.66      1499\n",
      "          34       0.96      0.96      0.96      1499\n",
      "          35       0.77      0.72      0.75      1499\n",
      "          36       0.87      0.86      0.86      1499\n",
      "          37       0.68      0.73      0.70      1499\n",
      "          38       0.45      0.32      0.37      1499\n",
      "          39       0.60      0.52      0.56      1499\n",
      "          40       0.93      0.68      0.79      1499\n",
      "          41       0.80      0.69      0.74      1499\n",
      "          42       0.43      0.36      0.39      1499\n",
      "          43       0.67      0.79      0.72      1499\n",
      "          44       0.78      0.87      0.83      1499\n",
      "          45       0.68      0.53      0.59      1499\n",
      "          46       0.58      0.69      0.63      1499\n",
      "          47       0.68      0.47      0.56      1499\n",
      "          48       0.76      0.86      0.81      1499\n",
      "          49       0.67      0.68      0.67      1499\n",
      "          50       0.61      0.52      0.56      1499\n",
      "          51       0.65      0.55      0.60      1499\n",
      "          52       0.67      0.82      0.74      1499\n",
      "          53       0.54      0.66      0.60      1499\n",
      "          54       0.78      0.72      0.75      1499\n",
      "          55       0.59      0.77      0.67      1499\n",
      "          56       0.47      0.42      0.45      1499\n",
      "          57       0.83      0.83      0.83      1499\n",
      "          58       0.95      0.84      0.89      1499\n",
      "          59       0.57      0.44      0.50      1499\n",
      "          60       0.78      0.83      0.80      1499\n",
      "          61       0.80      0.79      0.80      1499\n",
      "          62       0.89      0.94      0.92      1499\n",
      "          63       0.63      0.63      0.63      1499\n",
      "          64       0.44      0.37      0.40      1499\n",
      "          65       0.65      0.86      0.74      1499\n",
      "          66       0.91      0.87      0.89      1499\n",
      "          67       0.72      0.39      0.51      1499\n",
      "          68       0.75      0.91      0.82      1499\n",
      "          69       0.43      0.60      0.50      1499\n",
      "          70       0.78      0.32      0.46      1499\n",
      "          71       0.59      0.45      0.51      1499\n",
      "          72       0.52      0.62      0.56      1499\n",
      "          73       0.69      0.81      0.74      1499\n",
      "          74       0.32      0.75      0.45      1499\n",
      "\n",
      "    accuracy                           0.69    112425\n",
      "   macro avg       0.71      0.69      0.69    112425\n",
      "weighted avg       0.71      0.69      0.69    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300aa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baab7fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 67s 2ms/step - loss: 1.4385 - val_loss: 2.0294\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 54s 2ms/step - loss: 0.6407 - val_loss: 2.0113\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 47s 2ms/step - loss: 0.4818 - val_loss: 1.8551\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 48s 2ms/step - loss: 0.4102 - val_loss: 1.9687\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 47s 2ms/step - loss: 0.3693 - val_loss: 1.8745\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 0.3384 - val_loss: 1.9664\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 0.3157 - val_loss: 2.0236\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 0.2972 - val_loss: 1.9192\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 0.2825 - val_loss: 2.0627\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 48s 2ms/step - loss: 0.2718 - val_loss: 1.8648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f3128f13430>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 3s 884us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67      1499\n",
      "           1       0.77      0.83      0.80      1499\n",
      "           2       0.54      0.79      0.64      1499\n",
      "           3       0.69      0.27      0.39      1499\n",
      "           4       0.96      0.94      0.95      1499\n",
      "           5       0.98      0.68      0.80      1499\n",
      "           6       0.94      0.70      0.80      1499\n",
      "           7       0.86      0.74      0.80      1499\n",
      "           8       0.55      0.74      0.63      1499\n",
      "           9       0.67      0.79      0.72      1499\n",
      "          10       0.71      0.60      0.65      1499\n",
      "          11       0.98      0.91      0.94      1499\n",
      "          12       0.66      0.80      0.72      1499\n",
      "          13       0.71      0.69      0.70      1499\n",
      "          14       0.58      0.59      0.59      1499\n",
      "          15       0.78      0.31      0.45      1499\n",
      "          16       0.55      0.53      0.54      1499\n",
      "          17       0.71      0.38      0.50      1499\n",
      "          18       0.74      0.79      0.77      1499\n",
      "          19       0.46      0.61      0.52      1499\n",
      "          20       0.98      0.93      0.95      1499\n",
      "          21       0.59      0.58      0.59      1499\n",
      "          22       0.89      0.58      0.70      1499\n",
      "          23       0.83      0.95      0.88      1499\n",
      "          24       0.73      0.31      0.43      1499\n",
      "          25       0.47      0.83      0.60      1499\n",
      "          26       0.82      0.24      0.37      1499\n",
      "          27       0.53      0.56      0.54      1499\n",
      "          28       0.79      0.70      0.75      1499\n",
      "          29       0.91      0.41      0.57      1499\n",
      "          30       0.82      0.74      0.78      1499\n",
      "          31       0.53      0.80      0.64      1499\n",
      "          32       0.95      0.78      0.86      1499\n",
      "          33       0.65      0.71      0.68      1499\n",
      "          34       0.92      0.88      0.90      1499\n",
      "          35       0.87      0.59      0.70      1499\n",
      "          36       0.76      0.95      0.84      1499\n",
      "          37       0.61      0.77      0.68      1499\n",
      "          38       0.46      0.33      0.38      1499\n",
      "          39       0.45      0.63      0.53      1499\n",
      "          40       0.88      0.62      0.73      1499\n",
      "          41       0.83      0.65      0.73      1499\n",
      "          42       0.57      0.30      0.39      1499\n",
      "          43       0.58      0.71      0.64      1499\n",
      "          44       0.85      0.75      0.79      1499\n",
      "          45       0.60      0.44      0.51      1499\n",
      "          46       0.60      0.51      0.55      1499\n",
      "          47       0.60      0.35      0.44      1499\n",
      "          48       0.78      0.81      0.80      1499\n",
      "          49       0.61      0.78      0.69      1499\n",
      "          50       0.61      0.70      0.65      1499\n",
      "          51       0.32      0.63      0.42      1499\n",
      "          52       0.60      0.84      0.70      1499\n",
      "          53       0.62      0.85      0.72      1499\n",
      "          54       0.80      0.48      0.60      1499\n",
      "          55       0.56      0.77      0.65      1499\n",
      "          56       0.43      0.31      0.36      1499\n",
      "          57       0.82      0.71      0.76      1499\n",
      "          58       0.79      0.93      0.85      1499\n",
      "          59       0.43      0.31      0.36      1499\n",
      "          60       0.78      0.68      0.73      1499\n",
      "          61       0.77      0.88      0.82      1499\n",
      "          62       0.73      0.94      0.82      1499\n",
      "          63       0.75      0.65      0.70      1499\n",
      "          64       0.44      0.24      0.31      1499\n",
      "          65       0.79      0.86      0.82      1499\n",
      "          66       0.81      0.90      0.85      1499\n",
      "          67       0.53      0.54      0.54      1499\n",
      "          68       0.76      0.83      0.79      1499\n",
      "          69       0.43      0.49      0.45      1499\n",
      "          70       0.62      0.33      0.43      1499\n",
      "          71       0.56      0.71      0.62      1499\n",
      "          72       0.39      0.26      0.31      1499\n",
      "          73       0.58      0.90      0.70      1499\n",
      "          74       0.28      0.73      0.41      1499\n",
      "\n",
      "    accuracy                           0.65    112425\n",
      "   macro avg       0.68      0.65      0.65    112425\n",
      "weighted avg       0.68      0.65      0.65    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.52774\n",
      "[1]\tvalidation_0-mlogloss:3.26387\n",
      "[2]\tvalidation_0-mlogloss:3.09289\n",
      "[3]\tvalidation_0-mlogloss:2.95280\n",
      "[4]\tvalidation_0-mlogloss:2.83453\n",
      "[5]\tvalidation_0-mlogloss:2.73392\n",
      "[6]\tvalidation_0-mlogloss:2.65230\n",
      "[7]\tvalidation_0-mlogloss:2.57743\n",
      "[8]\tvalidation_0-mlogloss:2.51035\n",
      "[9]\tvalidation_0-mlogloss:2.45098\n",
      "[10]\tvalidation_0-mlogloss:2.39400\n",
      "[11]\tvalidation_0-mlogloss:2.34369\n",
      "[12]\tvalidation_0-mlogloss:2.29405\n",
      "[13]\tvalidation_0-mlogloss:2.25297\n",
      "[14]\tvalidation_0-mlogloss:2.20795\n",
      "[15]\tvalidation_0-mlogloss:2.17071\n",
      "[16]\tvalidation_0-mlogloss:2.13244\n",
      "[17]\tvalidation_0-mlogloss:2.10075\n",
      "[18]\tvalidation_0-mlogloss:2.06592\n",
      "[19]\tvalidation_0-mlogloss:2.03720\n",
      "[20]\tvalidation_0-mlogloss:2.00864\n",
      "[21]\tvalidation_0-mlogloss:1.98550\n",
      "[22]\tvalidation_0-mlogloss:1.96018\n",
      "[23]\tvalidation_0-mlogloss:1.93251\n",
      "[24]\tvalidation_0-mlogloss:1.91045\n",
      "[25]\tvalidation_0-mlogloss:1.89048\n",
      "[26]\tvalidation_0-mlogloss:1.86718\n",
      "[27]\tvalidation_0-mlogloss:1.84779\n",
      "[28]\tvalidation_0-mlogloss:1.82816\n",
      "[29]\tvalidation_0-mlogloss:1.80965\n",
      "[30]\tvalidation_0-mlogloss:1.79184\n",
      "[31]\tvalidation_0-mlogloss:1.77342\n",
      "[32]\tvalidation_0-mlogloss:1.75751\n",
      "[33]\tvalidation_0-mlogloss:1.73951\n",
      "[34]\tvalidation_0-mlogloss:1.72216\n",
      "[35]\tvalidation_0-mlogloss:1.70564\n",
      "[36]\tvalidation_0-mlogloss:1.68771\n",
      "[37]\tvalidation_0-mlogloss:1.67090\n",
      "[38]\tvalidation_0-mlogloss:1.65517\n",
      "[39]\tvalidation_0-mlogloss:1.64142\n",
      "[40]\tvalidation_0-mlogloss:1.62787\n",
      "[41]\tvalidation_0-mlogloss:1.61312\n",
      "[42]\tvalidation_0-mlogloss:1.59869\n",
      "[43]\tvalidation_0-mlogloss:1.58792\n",
      "[44]\tvalidation_0-mlogloss:1.57504\n",
      "[45]\tvalidation_0-mlogloss:1.56064\n",
      "[46]\tvalidation_0-mlogloss:1.54797\n",
      "[47]\tvalidation_0-mlogloss:1.53583\n",
      "[48]\tvalidation_0-mlogloss:1.52424\n",
      "[49]\tvalidation_0-mlogloss:1.51212\n",
      "[50]\tvalidation_0-mlogloss:1.49978\n",
      "[51]\tvalidation_0-mlogloss:1.49055\n",
      "[52]\tvalidation_0-mlogloss:1.48092\n",
      "[53]\tvalidation_0-mlogloss:1.46876\n",
      "[54]\tvalidation_0-mlogloss:1.45712\n",
      "[55]\tvalidation_0-mlogloss:1.44710\n",
      "[56]\tvalidation_0-mlogloss:1.43692\n",
      "[57]\tvalidation_0-mlogloss:1.42800\n",
      "[58]\tvalidation_0-mlogloss:1.41825\n",
      "[59]\tvalidation_0-mlogloss:1.40881\n",
      "[60]\tvalidation_0-mlogloss:1.40003\n",
      "[61]\tvalidation_0-mlogloss:1.38973\n",
      "[62]\tvalidation_0-mlogloss:1.38066\n",
      "[63]\tvalidation_0-mlogloss:1.37265\n",
      "[64]\tvalidation_0-mlogloss:1.36507\n",
      "[65]\tvalidation_0-mlogloss:1.35743\n",
      "[66]\tvalidation_0-mlogloss:1.35055\n",
      "[67]\tvalidation_0-mlogloss:1.34393\n",
      "[68]\tvalidation_0-mlogloss:1.33604\n",
      "[69]\tvalidation_0-mlogloss:1.32845\n",
      "[70]\tvalidation_0-mlogloss:1.32171\n",
      "[71]\tvalidation_0-mlogloss:1.31401\n",
      "[72]\tvalidation_0-mlogloss:1.30616\n",
      "[73]\tvalidation_0-mlogloss:1.29841\n",
      "[74]\tvalidation_0-mlogloss:1.29071\n",
      "[75]\tvalidation_0-mlogloss:1.28384\n",
      "[76]\tvalidation_0-mlogloss:1.27724\n",
      "[77]\tvalidation_0-mlogloss:1.27017\n",
      "[78]\tvalidation_0-mlogloss:1.26476\n",
      "[79]\tvalidation_0-mlogloss:1.25901\n",
      "[80]\tvalidation_0-mlogloss:1.25160\n",
      "[81]\tvalidation_0-mlogloss:1.24536\n",
      "[82]\tvalidation_0-mlogloss:1.23903\n",
      "[83]\tvalidation_0-mlogloss:1.23328\n",
      "[84]\tvalidation_0-mlogloss:1.22791\n",
      "[85]\tvalidation_0-mlogloss:1.22140\n",
      "[86]\tvalidation_0-mlogloss:1.21579\n",
      "[87]\tvalidation_0-mlogloss:1.20981\n",
      "[88]\tvalidation_0-mlogloss:1.20478\n",
      "[89]\tvalidation_0-mlogloss:1.19951\n",
      "[90]\tvalidation_0-mlogloss:1.19436\n",
      "[91]\tvalidation_0-mlogloss:1.19048\n",
      "[92]\tvalidation_0-mlogloss:1.18514\n",
      "[93]\tvalidation_0-mlogloss:1.17894\n",
      "[94]\tvalidation_0-mlogloss:1.17318\n",
      "[95]\tvalidation_0-mlogloss:1.16913\n",
      "[96]\tvalidation_0-mlogloss:1.16354\n",
      "[97]\tvalidation_0-mlogloss:1.15825\n",
      "[98]\tvalidation_0-mlogloss:1.15381\n",
      "[99]\tvalidation_0-mlogloss:1.14814\n",
      "[100]\tvalidation_0-mlogloss:1.14325\n",
      "[101]\tvalidation_0-mlogloss:1.13830\n",
      "[102]\tvalidation_0-mlogloss:1.13486\n",
      "[103]\tvalidation_0-mlogloss:1.13035\n",
      "[104]\tvalidation_0-mlogloss:1.12613\n",
      "[105]\tvalidation_0-mlogloss:1.12155\n",
      "[106]\tvalidation_0-mlogloss:1.11681\n",
      "[107]\tvalidation_0-mlogloss:1.11243\n",
      "[108]\tvalidation_0-mlogloss:1.10760\n",
      "[109]\tvalidation_0-mlogloss:1.10385\n",
      "[110]\tvalidation_0-mlogloss:1.09980\n",
      "[111]\tvalidation_0-mlogloss:1.09690\n",
      "[112]\tvalidation_0-mlogloss:1.09298\n",
      "[113]\tvalidation_0-mlogloss:1.08956\n",
      "[114]\tvalidation_0-mlogloss:1.08582\n",
      "[115]\tvalidation_0-mlogloss:1.08274\n",
      "[116]\tvalidation_0-mlogloss:1.07844\n",
      "[117]\tvalidation_0-mlogloss:1.07459\n",
      "[118]\tvalidation_0-mlogloss:1.07070\n",
      "[119]\tvalidation_0-mlogloss:1.06646\n",
      "[120]\tvalidation_0-mlogloss:1.06273\n",
      "[121]\tvalidation_0-mlogloss:1.05986\n",
      "[122]\tvalidation_0-mlogloss:1.05620\n",
      "[123]\tvalidation_0-mlogloss:1.05292\n",
      "[124]\tvalidation_0-mlogloss:1.04971\n",
      "[125]\tvalidation_0-mlogloss:1.04648\n",
      "[126]\tvalidation_0-mlogloss:1.04342\n",
      "[127]\tvalidation_0-mlogloss:1.04040\n",
      "[128]\tvalidation_0-mlogloss:1.03765\n",
      "[129]\tvalidation_0-mlogloss:1.03450\n",
      "[130]\tvalidation_0-mlogloss:1.03122\n",
      "[131]\tvalidation_0-mlogloss:1.02753\n",
      "[132]\tvalidation_0-mlogloss:1.02472\n",
      "[133]\tvalidation_0-mlogloss:1.02138\n",
      "[134]\tvalidation_0-mlogloss:1.01803\n",
      "[135]\tvalidation_0-mlogloss:1.01469\n",
      "[136]\tvalidation_0-mlogloss:1.01172\n",
      "[137]\tvalidation_0-mlogloss:1.00903\n",
      "[138]\tvalidation_0-mlogloss:1.00650\n",
      "[139]\tvalidation_0-mlogloss:1.00376\n",
      "[140]\tvalidation_0-mlogloss:1.00041\n",
      "[141]\tvalidation_0-mlogloss:0.99772\n",
      "[142]\tvalidation_0-mlogloss:0.99474\n",
      "[143]\tvalidation_0-mlogloss:0.99228\n",
      "[144]\tvalidation_0-mlogloss:0.98999\n",
      "[145]\tvalidation_0-mlogloss:0.98752\n",
      "[146]\tvalidation_0-mlogloss:0.98531\n",
      "[147]\tvalidation_0-mlogloss:0.98213\n",
      "[148]\tvalidation_0-mlogloss:0.97968\n",
      "[149]\tvalidation_0-mlogloss:0.97682\n",
      "[150]\tvalidation_0-mlogloss:0.97448\n",
      "[151]\tvalidation_0-mlogloss:0.97234\n",
      "[152]\tvalidation_0-mlogloss:0.97020\n",
      "[153]\tvalidation_0-mlogloss:0.96796\n",
      "[154]\tvalidation_0-mlogloss:0.96554\n",
      "[155]\tvalidation_0-mlogloss:0.96297\n",
      "[156]\tvalidation_0-mlogloss:0.96067\n",
      "[157]\tvalidation_0-mlogloss:0.95882\n",
      "[158]\tvalidation_0-mlogloss:0.95666\n",
      "[159]\tvalidation_0-mlogloss:0.95489\n",
      "[160]\tvalidation_0-mlogloss:0.95335\n",
      "[161]\tvalidation_0-mlogloss:0.95123\n",
      "[162]\tvalidation_0-mlogloss:0.94935\n",
      "[163]\tvalidation_0-mlogloss:0.94704\n",
      "[164]\tvalidation_0-mlogloss:0.94440\n",
      "[165]\tvalidation_0-mlogloss:0.94291\n",
      "[166]\tvalidation_0-mlogloss:0.94105\n",
      "[167]\tvalidation_0-mlogloss:0.93932\n",
      "[168]\tvalidation_0-mlogloss:0.93713\n",
      "[169]\tvalidation_0-mlogloss:0.93501\n",
      "[170]\tvalidation_0-mlogloss:0.93290\n",
      "[171]\tvalidation_0-mlogloss:0.93106\n",
      "[172]\tvalidation_0-mlogloss:0.92890\n",
      "[173]\tvalidation_0-mlogloss:0.92642\n",
      "[174]\tvalidation_0-mlogloss:0.92513\n",
      "[175]\tvalidation_0-mlogloss:0.92285\n",
      "[176]\tvalidation_0-mlogloss:0.92089\n",
      "[177]\tvalidation_0-mlogloss:0.91840\n",
      "[178]\tvalidation_0-mlogloss:0.91703\n",
      "[179]\tvalidation_0-mlogloss:0.91551\n",
      "[180]\tvalidation_0-mlogloss:0.91318\n",
      "[181]\tvalidation_0-mlogloss:0.91113\n",
      "[182]\tvalidation_0-mlogloss:0.90905\n",
      "[183]\tvalidation_0-mlogloss:0.90713\n",
      "[184]\tvalidation_0-mlogloss:0.90488\n",
      "[185]\tvalidation_0-mlogloss:0.90277\n",
      "[186]\tvalidation_0-mlogloss:0.90159\n",
      "[187]\tvalidation_0-mlogloss:0.89908\n",
      "[188]\tvalidation_0-mlogloss:0.89776\n",
      "[189]\tvalidation_0-mlogloss:0.89570\n",
      "[190]\tvalidation_0-mlogloss:0.89431\n",
      "[191]\tvalidation_0-mlogloss:0.89260\n",
      "[192]\tvalidation_0-mlogloss:0.89090\n",
      "[193]\tvalidation_0-mlogloss:0.88964\n",
      "[194]\tvalidation_0-mlogloss:0.88805\n",
      "[195]\tvalidation_0-mlogloss:0.88662\n",
      "[196]\tvalidation_0-mlogloss:0.88517\n",
      "[197]\tvalidation_0-mlogloss:0.88404\n",
      "[198]\tvalidation_0-mlogloss:0.88233\n",
      "[199]\tvalidation_0-mlogloss:0.88071\n",
      "[200]\tvalidation_0-mlogloss:0.87926\n",
      "[201]\tvalidation_0-mlogloss:0.87792\n",
      "[202]\tvalidation_0-mlogloss:0.87639\n",
      "[203]\tvalidation_0-mlogloss:0.87462\n",
      "[204]\tvalidation_0-mlogloss:0.87310\n",
      "[205]\tvalidation_0-mlogloss:0.87220\n",
      "[206]\tvalidation_0-mlogloss:0.87172\n",
      "[207]\tvalidation_0-mlogloss:0.87020\n",
      "[208]\tvalidation_0-mlogloss:0.86868\n",
      "[209]\tvalidation_0-mlogloss:0.86736\n",
      "[210]\tvalidation_0-mlogloss:0.86579\n",
      "[211]\tvalidation_0-mlogloss:0.86495\n",
      "[212]\tvalidation_0-mlogloss:0.86335\n",
      "[213]\tvalidation_0-mlogloss:0.86213\n",
      "[214]\tvalidation_0-mlogloss:0.86100\n",
      "[215]\tvalidation_0-mlogloss:0.85952\n",
      "[216]\tvalidation_0-mlogloss:0.85858\n",
      "[217]\tvalidation_0-mlogloss:0.85706\n",
      "[218]\tvalidation_0-mlogloss:0.85571\n",
      "[219]\tvalidation_0-mlogloss:0.85491\n",
      "[220]\tvalidation_0-mlogloss:0.85342\n",
      "[221]\tvalidation_0-mlogloss:0.85203\n",
      "[222]\tvalidation_0-mlogloss:0.85051\n",
      "[223]\tvalidation_0-mlogloss:0.84927\n",
      "[224]\tvalidation_0-mlogloss:0.84848\n",
      "[225]\tvalidation_0-mlogloss:0.84727\n",
      "[226]\tvalidation_0-mlogloss:0.84552\n",
      "[227]\tvalidation_0-mlogloss:0.84468\n",
      "[228]\tvalidation_0-mlogloss:0.84371\n",
      "[229]\tvalidation_0-mlogloss:0.84269\n",
      "[230]\tvalidation_0-mlogloss:0.84168\n",
      "[231]\tvalidation_0-mlogloss:0.84030\n",
      "[232]\tvalidation_0-mlogloss:0.83942\n",
      "[233]\tvalidation_0-mlogloss:0.83818\n",
      "[234]\tvalidation_0-mlogloss:0.83661\n",
      "[235]\tvalidation_0-mlogloss:0.83541\n",
      "[236]\tvalidation_0-mlogloss:0.83465\n",
      "[237]\tvalidation_0-mlogloss:0.83332\n",
      "[238]\tvalidation_0-mlogloss:0.83161\n",
      "[239]\tvalidation_0-mlogloss:0.82961\n",
      "[240]\tvalidation_0-mlogloss:0.82817\n",
      "[241]\tvalidation_0-mlogloss:0.82741\n",
      "[242]\tvalidation_0-mlogloss:0.82673\n",
      "[243]\tvalidation_0-mlogloss:0.82552\n",
      "[244]\tvalidation_0-mlogloss:0.82426\n",
      "[245]\tvalidation_0-mlogloss:0.82348\n",
      "[246]\tvalidation_0-mlogloss:0.82220\n",
      "[247]\tvalidation_0-mlogloss:0.82122\n",
      "[248]\tvalidation_0-mlogloss:0.81999\n",
      "[249]\tvalidation_0-mlogloss:0.81904\n",
      "[250]\tvalidation_0-mlogloss:0.81783\n",
      "[251]\tvalidation_0-mlogloss:0.81690\n",
      "[252]\tvalidation_0-mlogloss:0.81582\n",
      "[253]\tvalidation_0-mlogloss:0.81493\n",
      "[254]\tvalidation_0-mlogloss:0.81378\n",
      "[255]\tvalidation_0-mlogloss:0.81263\n",
      "[256]\tvalidation_0-mlogloss:0.81133\n",
      "[257]\tvalidation_0-mlogloss:0.81024\n",
      "[258]\tvalidation_0-mlogloss:0.80963\n",
      "[259]\tvalidation_0-mlogloss:0.80900\n",
      "[260]\tvalidation_0-mlogloss:0.80816\n",
      "[261]\tvalidation_0-mlogloss:0.80703\n",
      "[262]\tvalidation_0-mlogloss:0.80637\n",
      "[263]\tvalidation_0-mlogloss:0.80533\n",
      "[264]\tvalidation_0-mlogloss:0.80389\n",
      "[265]\tvalidation_0-mlogloss:0.80301\n",
      "[266]\tvalidation_0-mlogloss:0.80186\n",
      "[267]\tvalidation_0-mlogloss:0.80108\n",
      "[268]\tvalidation_0-mlogloss:0.79987\n",
      "[269]\tvalidation_0-mlogloss:0.79887\n",
      "[270]\tvalidation_0-mlogloss:0.79793\n",
      "[271]\tvalidation_0-mlogloss:0.79708\n",
      "[272]\tvalidation_0-mlogloss:0.79643\n",
      "[273]\tvalidation_0-mlogloss:0.79550\n",
      "[274]\tvalidation_0-mlogloss:0.79443\n",
      "[275]\tvalidation_0-mlogloss:0.79394\n",
      "[276]\tvalidation_0-mlogloss:0.79280\n",
      "[277]\tvalidation_0-mlogloss:0.79228\n",
      "[278]\tvalidation_0-mlogloss:0.79182\n",
      "[279]\tvalidation_0-mlogloss:0.79115\n",
      "[280]\tvalidation_0-mlogloss:0.79062\n",
      "[281]\tvalidation_0-mlogloss:0.78968\n",
      "[282]\tvalidation_0-mlogloss:0.78927\n",
      "[283]\tvalidation_0-mlogloss:0.78877\n",
      "[284]\tvalidation_0-mlogloss:0.78789\n",
      "[285]\tvalidation_0-mlogloss:0.78707\n",
      "[286]\tvalidation_0-mlogloss:0.78601\n",
      "[287]\tvalidation_0-mlogloss:0.78487\n",
      "[288]\tvalidation_0-mlogloss:0.78405\n",
      "[289]\tvalidation_0-mlogloss:0.78310\n",
      "[290]\tvalidation_0-mlogloss:0.78273\n",
      "[291]\tvalidation_0-mlogloss:0.78199\n",
      "[292]\tvalidation_0-mlogloss:0.78112\n",
      "[293]\tvalidation_0-mlogloss:0.78056\n",
      "[294]\tvalidation_0-mlogloss:0.78006\n",
      "[295]\tvalidation_0-mlogloss:0.77948\n",
      "[296]\tvalidation_0-mlogloss:0.77923\n",
      "[297]\tvalidation_0-mlogloss:0.77846\n",
      "[298]\tvalidation_0-mlogloss:0.77790\n",
      "[299]\tvalidation_0-mlogloss:0.77718\n",
      "[300]\tvalidation_0-mlogloss:0.77600\n",
      "[301]\tvalidation_0-mlogloss:0.77541\n",
      "[302]\tvalidation_0-mlogloss:0.77419\n",
      "[303]\tvalidation_0-mlogloss:0.77336\n",
      "[304]\tvalidation_0-mlogloss:0.77318\n",
      "[305]\tvalidation_0-mlogloss:0.77271\n",
      "[306]\tvalidation_0-mlogloss:0.77189\n",
      "[307]\tvalidation_0-mlogloss:0.77133\n",
      "[308]\tvalidation_0-mlogloss:0.77029\n",
      "[309]\tvalidation_0-mlogloss:0.76965\n",
      "[310]\tvalidation_0-mlogloss:0.76915\n",
      "[311]\tvalidation_0-mlogloss:0.76857\n",
      "[312]\tvalidation_0-mlogloss:0.76793\n",
      "[313]\tvalidation_0-mlogloss:0.76739\n",
      "[314]\tvalidation_0-mlogloss:0.76671\n",
      "[315]\tvalidation_0-mlogloss:0.76589\n",
      "[316]\tvalidation_0-mlogloss:0.76524\n",
      "[317]\tvalidation_0-mlogloss:0.76451\n",
      "[318]\tvalidation_0-mlogloss:0.76362\n",
      "[319]\tvalidation_0-mlogloss:0.76281\n",
      "[320]\tvalidation_0-mlogloss:0.76195\n",
      "[321]\tvalidation_0-mlogloss:0.76139\n",
      "[322]\tvalidation_0-mlogloss:0.76119\n",
      "[323]\tvalidation_0-mlogloss:0.76093\n",
      "[324]\tvalidation_0-mlogloss:0.76042\n",
      "[325]\tvalidation_0-mlogloss:0.76001\n",
      "[326]\tvalidation_0-mlogloss:0.75980\n",
      "[327]\tvalidation_0-mlogloss:0.75919\n",
      "[328]\tvalidation_0-mlogloss:0.75850\n",
      "[329]\tvalidation_0-mlogloss:0.75780\n",
      "[330]\tvalidation_0-mlogloss:0.75719\n",
      "[331]\tvalidation_0-mlogloss:0.75661\n",
      "[332]\tvalidation_0-mlogloss:0.75622\n",
      "[333]\tvalidation_0-mlogloss:0.75536\n",
      "[334]\tvalidation_0-mlogloss:0.75504\n",
      "[335]\tvalidation_0-mlogloss:0.75445\n",
      "[336]\tvalidation_0-mlogloss:0.75405\n",
      "[337]\tvalidation_0-mlogloss:0.75338\n",
      "[338]\tvalidation_0-mlogloss:0.75299\n",
      "[339]\tvalidation_0-mlogloss:0.75217\n",
      "[340]\tvalidation_0-mlogloss:0.75138\n",
      "[341]\tvalidation_0-mlogloss:0.75089\n",
      "[342]\tvalidation_0-mlogloss:0.75048\n",
      "[343]\tvalidation_0-mlogloss:0.75022\n",
      "[344]\tvalidation_0-mlogloss:0.74938\n",
      "[345]\tvalidation_0-mlogloss:0.74844\n",
      "[346]\tvalidation_0-mlogloss:0.74781\n",
      "[347]\tvalidation_0-mlogloss:0.74760\n",
      "[348]\tvalidation_0-mlogloss:0.74714\n",
      "[349]\tvalidation_0-mlogloss:0.74669\n",
      "[350]\tvalidation_0-mlogloss:0.74598\n",
      "[351]\tvalidation_0-mlogloss:0.74558\n",
      "[352]\tvalidation_0-mlogloss:0.74524\n",
      "[353]\tvalidation_0-mlogloss:0.74474\n",
      "[354]\tvalidation_0-mlogloss:0.74420\n",
      "[355]\tvalidation_0-mlogloss:0.74363\n",
      "[356]\tvalidation_0-mlogloss:0.74294\n",
      "[357]\tvalidation_0-mlogloss:0.74252\n",
      "[358]\tvalidation_0-mlogloss:0.74202\n",
      "[359]\tvalidation_0-mlogloss:0.74141\n",
      "[360]\tvalidation_0-mlogloss:0.74084\n",
      "[361]\tvalidation_0-mlogloss:0.74053\n",
      "[362]\tvalidation_0-mlogloss:0.73994\n",
      "[363]\tvalidation_0-mlogloss:0.73952\n",
      "[364]\tvalidation_0-mlogloss:0.73894\n",
      "[365]\tvalidation_0-mlogloss:0.73794\n",
      "[366]\tvalidation_0-mlogloss:0.73712\n",
      "[367]\tvalidation_0-mlogloss:0.73672\n",
      "[368]\tvalidation_0-mlogloss:0.73626\n",
      "[369]\tvalidation_0-mlogloss:0.73555\n",
      "[370]\tvalidation_0-mlogloss:0.73512\n",
      "[371]\tvalidation_0-mlogloss:0.73473\n",
      "[372]\tvalidation_0-mlogloss:0.73429\n",
      "[373]\tvalidation_0-mlogloss:0.73402\n",
      "[374]\tvalidation_0-mlogloss:0.73372\n",
      "[375]\tvalidation_0-mlogloss:0.73286\n",
      "[376]\tvalidation_0-mlogloss:0.73232\n",
      "[377]\tvalidation_0-mlogloss:0.73196\n",
      "[378]\tvalidation_0-mlogloss:0.73150\n",
      "[379]\tvalidation_0-mlogloss:0.73111\n",
      "[380]\tvalidation_0-mlogloss:0.73075\n",
      "[381]\tvalidation_0-mlogloss:0.73033\n",
      "[382]\tvalidation_0-mlogloss:0.72978\n",
      "[383]\tvalidation_0-mlogloss:0.72931\n",
      "[384]\tvalidation_0-mlogloss:0.72894\n",
      "[385]\tvalidation_0-mlogloss:0.72863\n",
      "[386]\tvalidation_0-mlogloss:0.72881\n",
      "[387]\tvalidation_0-mlogloss:0.72829\n",
      "[388]\tvalidation_0-mlogloss:0.72783\n",
      "[389]\tvalidation_0-mlogloss:0.72739\n",
      "[390]\tvalidation_0-mlogloss:0.72668\n",
      "[391]\tvalidation_0-mlogloss:0.72606\n",
      "[392]\tvalidation_0-mlogloss:0.72556\n",
      "[393]\tvalidation_0-mlogloss:0.72519\n",
      "[394]\tvalidation_0-mlogloss:0.72478\n",
      "[395]\tvalidation_0-mlogloss:0.72409\n",
      "[396]\tvalidation_0-mlogloss:0.72372\n",
      "[397]\tvalidation_0-mlogloss:0.72338\n",
      "[398]\tvalidation_0-mlogloss:0.72320\n",
      "[399]\tvalidation_0-mlogloss:0.72271\n",
      "[400]\tvalidation_0-mlogloss:0.72216\n",
      "[401]\tvalidation_0-mlogloss:0.72209\n",
      "[402]\tvalidation_0-mlogloss:0.72177\n",
      "[403]\tvalidation_0-mlogloss:0.72177\n",
      "[404]\tvalidation_0-mlogloss:0.72141\n",
      "[405]\tvalidation_0-mlogloss:0.72110\n",
      "[406]\tvalidation_0-mlogloss:0.72041\n",
      "[407]\tvalidation_0-mlogloss:0.72005\n",
      "[408]\tvalidation_0-mlogloss:0.71973\n",
      "[409]\tvalidation_0-mlogloss:0.71908\n",
      "[410]\tvalidation_0-mlogloss:0.71880\n",
      "[411]\tvalidation_0-mlogloss:0.71861\n",
      "[412]\tvalidation_0-mlogloss:0.71809\n",
      "[413]\tvalidation_0-mlogloss:0.71762\n",
      "[414]\tvalidation_0-mlogloss:0.71712\n",
      "[415]\tvalidation_0-mlogloss:0.71650\n",
      "[416]\tvalidation_0-mlogloss:0.71612\n",
      "[417]\tvalidation_0-mlogloss:0.71610\n",
      "[418]\tvalidation_0-mlogloss:0.71545\n",
      "[419]\tvalidation_0-mlogloss:0.71515\n",
      "[420]\tvalidation_0-mlogloss:0.71470\n",
      "[421]\tvalidation_0-mlogloss:0.71422\n",
      "[422]\tvalidation_0-mlogloss:0.71400\n",
      "[423]\tvalidation_0-mlogloss:0.71351\n",
      "[424]\tvalidation_0-mlogloss:0.71340\n",
      "[425]\tvalidation_0-mlogloss:0.71336\n",
      "[426]\tvalidation_0-mlogloss:0.71312\n",
      "[427]\tvalidation_0-mlogloss:0.71265\n",
      "[428]\tvalidation_0-mlogloss:0.71236\n",
      "[429]\tvalidation_0-mlogloss:0.71175\n",
      "[430]\tvalidation_0-mlogloss:0.71113\n",
      "[431]\tvalidation_0-mlogloss:0.71085\n",
      "[432]\tvalidation_0-mlogloss:0.71051\n",
      "[433]\tvalidation_0-mlogloss:0.71019\n",
      "[434]\tvalidation_0-mlogloss:0.71001\n",
      "[435]\tvalidation_0-mlogloss:0.70952\n",
      "[436]\tvalidation_0-mlogloss:0.70912\n",
      "[437]\tvalidation_0-mlogloss:0.70878\n",
      "[438]\tvalidation_0-mlogloss:0.70875\n",
      "[439]\tvalidation_0-mlogloss:0.70845\n",
      "[440]\tvalidation_0-mlogloss:0.70834\n",
      "[441]\tvalidation_0-mlogloss:0.70815\n",
      "[442]\tvalidation_0-mlogloss:0.70772\n",
      "[443]\tvalidation_0-mlogloss:0.70735\n",
      "[444]\tvalidation_0-mlogloss:0.70685\n",
      "[445]\tvalidation_0-mlogloss:0.70651\n",
      "[446]\tvalidation_0-mlogloss:0.70607\n",
      "[447]\tvalidation_0-mlogloss:0.70584\n",
      "[448]\tvalidation_0-mlogloss:0.70547\n",
      "[449]\tvalidation_0-mlogloss:0.70527\n",
      "[450]\tvalidation_0-mlogloss:0.70510\n",
      "[451]\tvalidation_0-mlogloss:0.70463\n",
      "[452]\tvalidation_0-mlogloss:0.70448\n",
      "[453]\tvalidation_0-mlogloss:0.70418\n",
      "[454]\tvalidation_0-mlogloss:0.70393\n",
      "[455]\tvalidation_0-mlogloss:0.70373\n",
      "[456]\tvalidation_0-mlogloss:0.70378\n",
      "[457]\tvalidation_0-mlogloss:0.70358\n",
      "[458]\tvalidation_0-mlogloss:0.70333\n",
      "[459]\tvalidation_0-mlogloss:0.70348\n",
      "[460]\tvalidation_0-mlogloss:0.70316\n",
      "[461]\tvalidation_0-mlogloss:0.70296\n",
      "[462]\tvalidation_0-mlogloss:0.70279\n",
      "[463]\tvalidation_0-mlogloss:0.70254\n",
      "[464]\tvalidation_0-mlogloss:0.70217\n",
      "[465]\tvalidation_0-mlogloss:0.70191\n",
      "[466]\tvalidation_0-mlogloss:0.70163\n",
      "[467]\tvalidation_0-mlogloss:0.70134\n",
      "[468]\tvalidation_0-mlogloss:0.70102\n",
      "[469]\tvalidation_0-mlogloss:0.70108\n",
      "[470]\tvalidation_0-mlogloss:0.70083\n",
      "[471]\tvalidation_0-mlogloss:0.70053\n",
      "[472]\tvalidation_0-mlogloss:0.69998\n",
      "[473]\tvalidation_0-mlogloss:0.69948\n",
      "[474]\tvalidation_0-mlogloss:0.69933\n",
      "[475]\tvalidation_0-mlogloss:0.69928\n",
      "[476]\tvalidation_0-mlogloss:0.69883\n",
      "[477]\tvalidation_0-mlogloss:0.69871\n",
      "[478]\tvalidation_0-mlogloss:0.69881\n",
      "[479]\tvalidation_0-mlogloss:0.69853\n",
      "[480]\tvalidation_0-mlogloss:0.69842\n",
      "[481]\tvalidation_0-mlogloss:0.69843\n",
      "[482]\tvalidation_0-mlogloss:0.69817\n",
      "[483]\tvalidation_0-mlogloss:0.69795\n",
      "[484]\tvalidation_0-mlogloss:0.69773\n",
      "[485]\tvalidation_0-mlogloss:0.69727\n",
      "[486]\tvalidation_0-mlogloss:0.69693\n",
      "[487]\tvalidation_0-mlogloss:0.69686\n",
      "[488]\tvalidation_0-mlogloss:0.69677\n",
      "[489]\tvalidation_0-mlogloss:0.69665\n",
      "[490]\tvalidation_0-mlogloss:0.69617\n",
      "[491]\tvalidation_0-mlogloss:0.69581\n",
      "[492]\tvalidation_0-mlogloss:0.69558\n",
      "[493]\tvalidation_0-mlogloss:0.69530\n",
      "[494]\tvalidation_0-mlogloss:0.69494\n",
      "[495]\tvalidation_0-mlogloss:0.69440\n",
      "[496]\tvalidation_0-mlogloss:0.69411\n",
      "[497]\tvalidation_0-mlogloss:0.69405\n",
      "[498]\tvalidation_0-mlogloss:0.69391\n",
      "[499]\tvalidation_0-mlogloss:0.69383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.90      0.80      1499\n",
      "           1       0.95      0.97      0.96      1499\n",
      "           2       0.64      0.82      0.72      1499\n",
      "           3       0.92      0.88      0.90      1499\n",
      "           4       0.96      1.00      0.98      1499\n",
      "           5       0.97      0.95      0.96      1499\n",
      "           6       0.95      0.98      0.97      1499\n",
      "           7       0.95      0.89      0.92      1499\n",
      "           8       0.72      0.85      0.78      1499\n",
      "           9       0.89      0.89      0.89      1499\n",
      "          10       0.97      0.74      0.84      1499\n",
      "          11       0.95      0.99      0.97      1499\n",
      "          12       0.83      0.89      0.86      1499\n",
      "          13       0.82      0.85      0.84      1499\n",
      "          14       0.87      0.89      0.88      1499\n",
      "          15       0.94      0.80      0.86      1499\n",
      "          16       0.90      0.76      0.82      1499\n",
      "          17       0.79      0.79      0.79      1499\n",
      "          18       0.89      0.92      0.90      1499\n",
      "          19       0.77      0.56      0.65      1499\n",
      "          20       0.96      1.00      0.98      1499\n",
      "          21       0.80      0.85      0.82      1499\n",
      "          22       0.81      0.91      0.86      1499\n",
      "          23       0.99      0.99      0.99      1499\n",
      "          24       0.93      0.81      0.87      1499\n",
      "          25       0.95      0.83      0.89      1499\n",
      "          26       0.84      0.59      0.69      1499\n",
      "          27       0.68      0.63      0.65      1499\n",
      "          28       0.78      0.89      0.83      1499\n",
      "          29       0.96      0.94      0.95      1499\n",
      "          30       0.88      0.92      0.90      1499\n",
      "          31       0.77      0.94      0.84      1499\n",
      "          32       0.92      0.96      0.94      1499\n",
      "          33       0.88      0.72      0.79      1499\n",
      "          34       0.97      0.99      0.98      1499\n",
      "          35       0.85      0.86      0.86      1499\n",
      "          36       0.89      0.91      0.90      1499\n",
      "          37       0.86      0.93      0.89      1499\n",
      "          38       0.58      0.41      0.48      1499\n",
      "          39       0.91      0.83      0.87      1499\n",
      "          40       0.97      0.89      0.93      1499\n",
      "          41       0.89      0.82      0.85      1499\n",
      "          42       0.91      0.89      0.90      1499\n",
      "          43       0.85      0.88      0.87      1499\n",
      "          44       0.94      0.97      0.95      1499\n",
      "          45       0.83      0.71      0.77      1499\n",
      "          46       0.74      0.89      0.81      1499\n",
      "          47       0.85      0.70      0.77      1499\n",
      "          48       0.87      0.92      0.89      1499\n",
      "          49       0.88      0.89      0.89      1499\n",
      "          50       0.80      0.62      0.70      1499\n",
      "          51       0.67      0.70      0.68      1499\n",
      "          52       0.85      0.93      0.89      1499\n",
      "          53       0.79      0.78      0.79      1499\n",
      "          54       0.81      0.83      0.82      1499\n",
      "          55       0.76      0.92      0.83      1499\n",
      "          56       0.75      0.67      0.70      1499\n",
      "          57       0.97      0.98      0.98      1499\n",
      "          58       0.97      0.79      0.87      1499\n",
      "          59       0.59      0.46      0.52      1499\n",
      "          60       0.83      0.96      0.89      1499\n",
      "          61       0.95      0.91      0.93      1499\n",
      "          62       0.94      0.97      0.96      1499\n",
      "          63       0.78      0.90      0.83      1499\n",
      "          64       0.72      0.75      0.73      1499\n",
      "          65       0.84      0.98      0.90      1499\n",
      "          66       0.97      0.93      0.95      1499\n",
      "          67       0.82      0.48      0.61      1499\n",
      "          68       0.84      0.97      0.90      1499\n",
      "          69       0.72      0.77      0.74      1499\n",
      "          70       0.86      0.37      0.52      1499\n",
      "          71       0.74      0.66      0.70      1499\n",
      "          72       0.85      0.92      0.88      1499\n",
      "          73       0.90      0.94      0.92      1499\n",
      "          74       0.42      0.91      0.58      1499\n",
      "\n",
      "    accuracy                           0.84    112425\n",
      "   macro avg       0.85      0.84      0.84    112425\n",
      "weighted avg       0.85      0.84      0.84    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3361defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a896a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 6915/28125 [======>.......................] - ETA: 31s - loss: 1.8721"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28125/28125 [==============================] - 50s 2ms/step - loss: 1.0110 - val_loss: 1.8567\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 0.3903 - val_loss: 1.7396\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 0.2807 - val_loss: 1.6829\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 52s 2ms/step - loss: 0.2257 - val_loss: 1.7655\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 0.1914 - val_loss: 1.4496\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 0.1671 - val_loss: 1.4852\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.1486 - val_loss: 1.6931\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 0.1344 - val_loss: 1.4802\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.1231 - val_loss: 1.4067\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.1153 - val_loss: 1.5924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f31d025dbd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  91/3514 [..............................] - ETA: 3s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 4s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85      1499\n",
      "           1       0.76      0.93      0.83      1499\n",
      "           2       0.70      0.69      0.70      1499\n",
      "           3       0.93      0.80      0.86      1499\n",
      "           4       0.96      0.99      0.97      1499\n",
      "           5       0.99      0.84      0.91      1499\n",
      "           6       0.91      0.93      0.92      1499\n",
      "           7       0.96      0.77      0.85      1499\n",
      "           8       0.57      0.87      0.69      1499\n",
      "           9       0.72      0.91      0.80      1499\n",
      "          10       0.70      0.62      0.66      1499\n",
      "          11       0.98      0.83      0.90      1499\n",
      "          12       0.89      0.73      0.80      1499\n",
      "          13       0.53      0.78      0.63      1499\n",
      "          14       0.84      0.74      0.79      1499\n",
      "          15       0.88      0.75      0.81      1499\n",
      "          16       0.85      0.77      0.81      1499\n",
      "          17       0.88      0.60      0.71      1499\n",
      "          18       0.83      0.91      0.87      1499\n",
      "          19       0.44      0.34      0.38      1499\n",
      "          20       0.99      0.96      0.98      1499\n",
      "          21       0.81      0.70      0.76      1499\n",
      "          22       0.80      0.90      0.84      1499\n",
      "          23       0.94      0.98      0.96      1499\n",
      "          24       0.66      0.76      0.71      1499\n",
      "          25       0.60      0.74      0.66      1499\n",
      "          26       0.85      0.56      0.67      1499\n",
      "          27       0.75      0.68      0.71      1499\n",
      "          28       0.79      0.75      0.77      1499\n",
      "          29       0.94      0.85      0.89      1499\n",
      "          30       0.88      0.91      0.89      1499\n",
      "          31       0.77      0.95      0.85      1499\n",
      "          32       0.80      0.90      0.85      1499\n",
      "          33       0.69      0.84      0.76      1499\n",
      "          34       0.98      0.98      0.98      1499\n",
      "          35       0.89      0.79      0.84      1499\n",
      "          36       0.74      0.79      0.77      1499\n",
      "          37       0.82      0.90      0.86      1499\n",
      "          38       0.62      0.43      0.51      1499\n",
      "          39       0.54      0.65      0.59      1499\n",
      "          40       0.94      0.48      0.64      1499\n",
      "          41       0.84      0.42      0.56      1499\n",
      "          42       0.63      0.73      0.68      1499\n",
      "          43       0.82      0.73      0.78      1499\n",
      "          44       0.94      0.89      0.92      1499\n",
      "          45       0.61      0.56      0.58      1499\n",
      "          46       0.58      0.77      0.66      1499\n",
      "          47       0.84      0.80      0.82      1499\n",
      "          48       0.86      0.84      0.85      1499\n",
      "          49       0.72      0.83      0.77      1499\n",
      "          50       0.65      0.55      0.59      1499\n",
      "          51       0.46      0.64      0.54      1499\n",
      "          52       0.72      0.92      0.81      1499\n",
      "          53       0.82      0.68      0.74      1499\n",
      "          54       0.87      0.75      0.80      1499\n",
      "          55       0.66      0.93      0.78      1499\n",
      "          56       0.76      0.50      0.60      1499\n",
      "          57       0.89      0.94      0.91      1499\n",
      "          58       0.85      0.98      0.91      1499\n",
      "          59       0.53      0.68      0.60      1499\n",
      "          60       0.76      0.93      0.84      1499\n",
      "          61       0.87      0.56      0.68      1499\n",
      "          62       0.94      0.96      0.95      1499\n",
      "          63       0.65      0.82      0.72      1499\n",
      "          64       0.38      0.45      0.42      1499\n",
      "          65       0.91      0.72      0.80      1499\n",
      "          66       0.82      0.96      0.89      1499\n",
      "          67       0.65      0.46      0.54      1499\n",
      "          68       0.94      0.93      0.94      1499\n",
      "          69       0.63      0.66      0.64      1499\n",
      "          70       0.78      0.22      0.35      1499\n",
      "          71       0.50      0.60      0.54      1499\n",
      "          72       0.87      0.28      0.42      1499\n",
      "          73       0.84      0.75      0.79      1499\n",
      "          74       0.38      0.80      0.52      1499\n",
      "\n",
      "    accuracy                           0.75    112425\n",
      "   macro avg       0.77      0.75      0.75    112425\n",
      "weighted avg       0.77      0.75      0.75    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
