{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 75 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 06:25:04.694665: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 06:25:04.743490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 06:25:04.743547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 06:25:04.744859: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 06:25:04.752870: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 06:25:04.755959: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 06:25:06.518400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files4/S001R06.edf',\n",
       " 'files4/S002R06.edf',\n",
       " 'files4/S003R06.edf',\n",
       " 'files4/S004R06.edf',\n",
       " 'files4/S005R06.edf',\n",
       " 'files4/S006R06.edf',\n",
       " 'files4/S007R06.edf',\n",
       " 'files4/S008R06.edf',\n",
       " 'files4/S009R06.edf',\n",
       " 'files4/S010R06.edf',\n",
       " 'files4/S011R06.edf',\n",
       " 'files4/S012R06.edf',\n",
       " 'files4/S013R06.edf',\n",
       " 'files4/S014R06.edf',\n",
       " 'files4/S015R06.edf',\n",
       " 'files4/S016R06.edf',\n",
       " 'files4/S017R06.edf',\n",
       " 'files4/S018R06.edf',\n",
       " 'files4/S019R06.edf',\n",
       " 'files4/S020R06.edf',\n",
       " 'files4/S021R06.edf',\n",
       " 'files4/S022R06.edf',\n",
       " 'files4/S023R06.edf',\n",
       " 'files4/S024R06.edf',\n",
       " 'files4/S025R06.edf',\n",
       " 'files4/S026R06.edf',\n",
       " 'files4/S027R06.edf',\n",
       " 'files4/S028R06.edf',\n",
       " 'files4/S029R06.edf',\n",
       " 'files4/S030R06.edf',\n",
       " 'files4/S031R06.edf',\n",
       " 'files4/S032R06.edf',\n",
       " 'files4/S033R06.edf',\n",
       " 'files4/S034R06.edf',\n",
       " 'files4/S035R06.edf',\n",
       " 'files4/S036R06.edf',\n",
       " 'files4/S037R06.edf',\n",
       " 'files4/S038R06.edf',\n",
       " 'files4/S039R06.edf',\n",
       " 'files4/S040R06.edf',\n",
       " 'files4/S041R06.edf',\n",
       " 'files4/S042R06.edf',\n",
       " 'files4/S043R06.edf',\n",
       " 'files4/S044R06.edf',\n",
       " 'files4/S045R06.edf',\n",
       " 'files4/S046R06.edf',\n",
       " 'files4/S047R06.edf',\n",
       " 'files4/S048R06.edf',\n",
       " 'files4/S049R06.edf',\n",
       " 'files4/S050R06.edf',\n",
       " 'files4/S051R06.edf',\n",
       " 'files4/S052R06.edf',\n",
       " 'files4/S053R06.edf',\n",
       " 'files4/S054R06.edf',\n",
       " 'files4/S055R06.edf',\n",
       " 'files4/S056R06.edf',\n",
       " 'files4/S057R06.edf',\n",
       " 'files4/S058R06.edf',\n",
       " 'files4/S059R06.edf',\n",
       " 'files4/S060R06.edf',\n",
       " 'files4/S061R06.edf',\n",
       " 'files4/S062R06.edf',\n",
       " 'files4/S063R06.edf',\n",
       " 'files4/S064R06.edf',\n",
       " 'files4/S065R06.edf',\n",
       " 'files4/S066R06.edf',\n",
       " 'files4/S067R06.edf',\n",
       " 'files4/S068R06.edf',\n",
       " 'files4/S069R06.edf',\n",
       " 'files4/S070R06.edf',\n",
       " 'files4/S071R06.edf',\n",
       " 'files4/S072R06.edf',\n",
       " 'files4/S073R06.edf',\n",
       " 'files4/S074R06.edf',\n",
       " 'files4/S075R06.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files4/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ca7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 112425, 900000, 112425)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.8e-05 -3.7e-05 -1.0e-06 ... -8.8e-05 -3.7e-05  5.6e-05]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000053   0.000060   0.000035   0.000044   0.000049   0.000032   \n",
       "899996   0.000035   0.000046   0.000006   0.000023   0.000027  -0.000001   \n",
       "899997   0.000031   0.000011  -0.000019  -0.000008   0.000003  -0.000027   \n",
       "899998   0.000011   0.000017   0.000021  -0.000007   0.000022   0.000058   \n",
       "899999  -0.000008   0.000050   0.000059   0.000020   0.000048   0.000076   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000064   0.000115   0.000076   0.000070  ...    0.000020   \n",
       "899996   0.000035   0.000067   0.000067   0.000043  ...   -0.000052   \n",
       "899997   0.000007   0.000019   0.000022   0.000012  ...   -0.000039   \n",
       "899998   0.000063   0.000008   0.000050   0.000035  ...   -0.000041   \n",
       "899999   0.000087   0.000007   0.000088   0.000068  ...   -0.000042   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995    0.000121    0.000117    0.000060    0.000022    0.000036   \n",
       "899996    0.000027    0.000048    0.000016   -0.000032   -0.000023   \n",
       "899997    0.000022    0.000042    0.000023   -0.000008    0.000002   \n",
       "899998    0.000021    0.000048    0.000049    0.000023    0.000002   \n",
       "899999    0.000014    0.000031    0.000039    0.000003   -0.000016   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000135    0.000116    0.000082    0.000140  \n",
       "899996    0.000050    0.000045    0.000020    0.000031  \n",
       "899997    0.000035    0.000048    0.000055    0.000033  \n",
       "899998    0.000028    0.000028    0.000037    0.000026  \n",
       "899999    0.000010    0.000000    0.000014   -0.000003  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41757/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_41757/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_41757/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000053   0.000060   0.000035   0.000044   0.000049   0.000032   \n",
       "899996   0.000035   0.000046   0.000006   0.000023   0.000027  -0.000001   \n",
       "899997   0.000031   0.000011  -0.000019  -0.000008   0.000003  -0.000027   \n",
       "899998   0.000011   0.000017   0.000021  -0.000007   0.000022   0.000058   \n",
       "899999  -0.000008   0.000050   0.000059   0.000020   0.000048   0.000076   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000064   0.000115   0.000076   0.000070  ...    0.000020   \n",
       "899996   0.000035   0.000067   0.000067   0.000043  ...   -0.000052   \n",
       "899997   0.000007   0.000019   0.000022   0.000012  ...   -0.000039   \n",
       "899998   0.000063   0.000008   0.000050   0.000035  ...   -0.000041   \n",
       "899999   0.000087   0.000007   0.000088   0.000068  ...   -0.000042   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995    0.000121    0.000117    0.000060    0.000022    0.000036   \n",
       "899996    0.000027    0.000048    0.000016   -0.000032   -0.000023   \n",
       "899997    0.000022    0.000042    0.000023   -0.000008    0.000002   \n",
       "899998    0.000021    0.000048    0.000049    0.000023    0.000002   \n",
       "899999    0.000014    0.000031    0.000039    0.000003   -0.000016   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000135    0.000116    0.000082    0.000140  \n",
       "899996    0.000050    0.000045    0.000020    0.000031  \n",
       "899997    0.000035    0.000048    0.000055    0.000033  \n",
       "899998    0.000028    0.000028    0.000037    0.000026  \n",
       "899999    0.000010    0.000000    0.000014   -0.000003  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:4.07270\n",
      "[1]\tvalidation_0-mlogloss:3.93581\n",
      "[2]\tvalidation_0-mlogloss:3.85327\n",
      "[3]\tvalidation_0-mlogloss:3.79386\n",
      "[4]\tvalidation_0-mlogloss:3.74048\n",
      "[5]\tvalidation_0-mlogloss:3.69728\n",
      "[6]\tvalidation_0-mlogloss:3.66384\n",
      "[7]\tvalidation_0-mlogloss:3.63004\n",
      "[8]\tvalidation_0-mlogloss:3.60399\n",
      "[9]\tvalidation_0-mlogloss:3.58090\n",
      "[10]\tvalidation_0-mlogloss:3.55989\n",
      "[11]\tvalidation_0-mlogloss:3.53880\n",
      "[12]\tvalidation_0-mlogloss:3.52509\n",
      "[13]\tvalidation_0-mlogloss:3.51159\n",
      "[14]\tvalidation_0-mlogloss:3.49451\n",
      "[15]\tvalidation_0-mlogloss:3.48406\n",
      "[16]\tvalidation_0-mlogloss:3.47314\n",
      "[17]\tvalidation_0-mlogloss:3.45942\n",
      "[18]\tvalidation_0-mlogloss:3.44737\n",
      "[19]\tvalidation_0-mlogloss:3.43632\n",
      "[20]\tvalidation_0-mlogloss:3.42647\n",
      "[21]\tvalidation_0-mlogloss:3.41868\n",
      "[22]\tvalidation_0-mlogloss:3.40933\n",
      "[23]\tvalidation_0-mlogloss:3.40177\n",
      "[24]\tvalidation_0-mlogloss:3.39491\n",
      "[25]\tvalidation_0-mlogloss:3.38508\n",
      "[26]\tvalidation_0-mlogloss:3.37627\n",
      "[27]\tvalidation_0-mlogloss:3.36858\n",
      "[28]\tvalidation_0-mlogloss:3.36252\n",
      "[29]\tvalidation_0-mlogloss:3.35473\n",
      "[30]\tvalidation_0-mlogloss:3.34642\n",
      "[31]\tvalidation_0-mlogloss:3.33890\n",
      "[32]\tvalidation_0-mlogloss:3.33325\n",
      "[33]\tvalidation_0-mlogloss:3.32680\n",
      "[34]\tvalidation_0-mlogloss:3.32139\n",
      "[35]\tvalidation_0-mlogloss:3.31580\n",
      "[36]\tvalidation_0-mlogloss:3.31011\n",
      "[37]\tvalidation_0-mlogloss:3.30486\n",
      "[38]\tvalidation_0-mlogloss:3.29903\n",
      "[39]\tvalidation_0-mlogloss:3.29347\n",
      "[40]\tvalidation_0-mlogloss:3.28871\n",
      "[41]\tvalidation_0-mlogloss:3.28460\n",
      "[42]\tvalidation_0-mlogloss:3.27939\n",
      "[43]\tvalidation_0-mlogloss:3.27447\n",
      "[44]\tvalidation_0-mlogloss:3.26899\n",
      "[45]\tvalidation_0-mlogloss:3.26397\n",
      "[46]\tvalidation_0-mlogloss:3.25696\n",
      "[47]\tvalidation_0-mlogloss:3.25132\n",
      "[48]\tvalidation_0-mlogloss:3.24636\n",
      "[49]\tvalidation_0-mlogloss:3.24152\n",
      "[50]\tvalidation_0-mlogloss:3.23724\n",
      "[51]\tvalidation_0-mlogloss:3.23234\n",
      "[52]\tvalidation_0-mlogloss:3.22758\n",
      "[53]\tvalidation_0-mlogloss:3.22466\n",
      "[54]\tvalidation_0-mlogloss:3.22038\n",
      "[55]\tvalidation_0-mlogloss:3.21600\n",
      "[56]\tvalidation_0-mlogloss:3.21185\n",
      "[57]\tvalidation_0-mlogloss:3.20811\n",
      "[58]\tvalidation_0-mlogloss:3.20626\n",
      "[59]\tvalidation_0-mlogloss:3.20287\n",
      "[60]\tvalidation_0-mlogloss:3.20025\n",
      "[61]\tvalidation_0-mlogloss:3.19706\n",
      "[62]\tvalidation_0-mlogloss:3.19431\n",
      "[63]\tvalidation_0-mlogloss:3.19010\n",
      "[64]\tvalidation_0-mlogloss:3.18654\n",
      "[65]\tvalidation_0-mlogloss:3.18254\n",
      "[66]\tvalidation_0-mlogloss:3.17994\n",
      "[67]\tvalidation_0-mlogloss:3.17705\n",
      "[68]\tvalidation_0-mlogloss:3.17443\n",
      "[69]\tvalidation_0-mlogloss:3.17053\n",
      "[70]\tvalidation_0-mlogloss:3.16894\n",
      "[71]\tvalidation_0-mlogloss:3.16523\n",
      "[72]\tvalidation_0-mlogloss:3.16135\n",
      "[73]\tvalidation_0-mlogloss:3.15838\n",
      "[74]\tvalidation_0-mlogloss:3.15605\n",
      "[75]\tvalidation_0-mlogloss:3.15244\n",
      "[76]\tvalidation_0-mlogloss:3.15075\n",
      "[77]\tvalidation_0-mlogloss:3.14839\n",
      "[78]\tvalidation_0-mlogloss:3.14575\n",
      "[79]\tvalidation_0-mlogloss:3.14349\n",
      "[80]\tvalidation_0-mlogloss:3.14238\n",
      "[81]\tvalidation_0-mlogloss:3.14058\n",
      "[82]\tvalidation_0-mlogloss:3.13832\n",
      "[83]\tvalidation_0-mlogloss:3.13618\n",
      "[84]\tvalidation_0-mlogloss:3.13445\n",
      "[85]\tvalidation_0-mlogloss:3.13166\n",
      "[86]\tvalidation_0-mlogloss:3.12932\n",
      "[87]\tvalidation_0-mlogloss:3.12709\n",
      "[88]\tvalidation_0-mlogloss:3.12556\n",
      "[89]\tvalidation_0-mlogloss:3.12290\n",
      "[90]\tvalidation_0-mlogloss:3.12068\n",
      "[91]\tvalidation_0-mlogloss:3.11719\n",
      "[92]\tvalidation_0-mlogloss:3.11453\n",
      "[93]\tvalidation_0-mlogloss:3.11281\n",
      "[94]\tvalidation_0-mlogloss:3.11063\n",
      "[95]\tvalidation_0-mlogloss:3.10785\n",
      "[96]\tvalidation_0-mlogloss:3.10568\n",
      "[97]\tvalidation_0-mlogloss:3.10385\n",
      "[98]\tvalidation_0-mlogloss:3.10303\n",
      "[99]\tvalidation_0-mlogloss:3.10175\n",
      "[100]\tvalidation_0-mlogloss:3.10036\n",
      "[101]\tvalidation_0-mlogloss:3.09739\n",
      "[102]\tvalidation_0-mlogloss:3.09625\n",
      "[103]\tvalidation_0-mlogloss:3.09478\n",
      "[104]\tvalidation_0-mlogloss:3.09322\n",
      "[105]\tvalidation_0-mlogloss:3.09025\n",
      "[106]\tvalidation_0-mlogloss:3.08954\n",
      "[107]\tvalidation_0-mlogloss:3.08838\n",
      "[108]\tvalidation_0-mlogloss:3.08631\n",
      "[109]\tvalidation_0-mlogloss:3.08512\n",
      "[110]\tvalidation_0-mlogloss:3.08451\n",
      "[111]\tvalidation_0-mlogloss:3.08469\n",
      "[112]\tvalidation_0-mlogloss:3.08288\n",
      "[113]\tvalidation_0-mlogloss:3.08156\n",
      "[114]\tvalidation_0-mlogloss:3.07957\n",
      "[115]\tvalidation_0-mlogloss:3.07800\n",
      "[116]\tvalidation_0-mlogloss:3.07649\n",
      "[117]\tvalidation_0-mlogloss:3.07561\n",
      "[118]\tvalidation_0-mlogloss:3.07425\n",
      "[119]\tvalidation_0-mlogloss:3.07247\n",
      "[120]\tvalidation_0-mlogloss:3.07132\n",
      "[121]\tvalidation_0-mlogloss:3.07109\n",
      "[122]\tvalidation_0-mlogloss:3.06971\n",
      "[123]\tvalidation_0-mlogloss:3.06846\n",
      "[124]\tvalidation_0-mlogloss:3.06748\n",
      "[125]\tvalidation_0-mlogloss:3.06660\n",
      "[126]\tvalidation_0-mlogloss:3.06502\n",
      "[127]\tvalidation_0-mlogloss:3.06405\n",
      "[128]\tvalidation_0-mlogloss:3.06331\n",
      "[129]\tvalidation_0-mlogloss:3.06302\n",
      "[130]\tvalidation_0-mlogloss:3.06194\n",
      "[131]\tvalidation_0-mlogloss:3.06076\n",
      "[132]\tvalidation_0-mlogloss:3.05986\n",
      "[133]\tvalidation_0-mlogloss:3.05815\n",
      "[134]\tvalidation_0-mlogloss:3.05633\n",
      "[135]\tvalidation_0-mlogloss:3.05473\n",
      "[136]\tvalidation_0-mlogloss:3.05288\n",
      "[137]\tvalidation_0-mlogloss:3.05124\n",
      "[138]\tvalidation_0-mlogloss:3.05013\n",
      "[139]\tvalidation_0-mlogloss:3.04867\n",
      "[140]\tvalidation_0-mlogloss:3.04747\n",
      "[141]\tvalidation_0-mlogloss:3.04778\n",
      "[142]\tvalidation_0-mlogloss:3.04719\n",
      "[143]\tvalidation_0-mlogloss:3.04518\n",
      "[144]\tvalidation_0-mlogloss:3.04383\n",
      "[145]\tvalidation_0-mlogloss:3.04312\n",
      "[146]\tvalidation_0-mlogloss:3.04222\n",
      "[147]\tvalidation_0-mlogloss:3.04087\n",
      "[148]\tvalidation_0-mlogloss:3.03997\n",
      "[149]\tvalidation_0-mlogloss:3.03840\n",
      "[150]\tvalidation_0-mlogloss:3.03722\n",
      "[151]\tvalidation_0-mlogloss:3.03634\n",
      "[152]\tvalidation_0-mlogloss:3.03576\n",
      "[153]\tvalidation_0-mlogloss:3.03542\n",
      "[154]\tvalidation_0-mlogloss:3.03593\n",
      "[155]\tvalidation_0-mlogloss:3.03593\n",
      "[156]\tvalidation_0-mlogloss:3.03489\n",
      "[157]\tvalidation_0-mlogloss:3.03504\n",
      "[158]\tvalidation_0-mlogloss:3.03499\n",
      "[159]\tvalidation_0-mlogloss:3.03455\n",
      "[160]\tvalidation_0-mlogloss:3.03424\n",
      "[161]\tvalidation_0-mlogloss:3.03417\n",
      "[162]\tvalidation_0-mlogloss:3.03388\n",
      "[163]\tvalidation_0-mlogloss:3.03270\n",
      "[164]\tvalidation_0-mlogloss:3.03214\n",
      "[165]\tvalidation_0-mlogloss:3.03158\n",
      "[166]\tvalidation_0-mlogloss:3.03155\n",
      "[167]\tvalidation_0-mlogloss:3.03079\n",
      "[168]\tvalidation_0-mlogloss:3.02994\n",
      "[169]\tvalidation_0-mlogloss:3.02984\n",
      "[170]\tvalidation_0-mlogloss:3.03001\n",
      "[171]\tvalidation_0-mlogloss:3.02925\n",
      "[172]\tvalidation_0-mlogloss:3.02903\n",
      "[173]\tvalidation_0-mlogloss:3.02881\n",
      "[174]\tvalidation_0-mlogloss:3.02824\n",
      "[175]\tvalidation_0-mlogloss:3.02749\n",
      "[176]\tvalidation_0-mlogloss:3.02807\n",
      "[177]\tvalidation_0-mlogloss:3.02737\n",
      "[178]\tvalidation_0-mlogloss:3.02662\n",
      "[179]\tvalidation_0-mlogloss:3.02610\n",
      "[180]\tvalidation_0-mlogloss:3.02455\n",
      "[181]\tvalidation_0-mlogloss:3.02404\n",
      "[182]\tvalidation_0-mlogloss:3.02455\n",
      "[183]\tvalidation_0-mlogloss:3.02486\n",
      "[184]\tvalidation_0-mlogloss:3.02398\n",
      "[185]\tvalidation_0-mlogloss:3.02353\n",
      "[186]\tvalidation_0-mlogloss:3.02307\n",
      "[187]\tvalidation_0-mlogloss:3.02374\n",
      "[188]\tvalidation_0-mlogloss:3.02323\n",
      "[189]\tvalidation_0-mlogloss:3.02272\n",
      "[190]\tvalidation_0-mlogloss:3.02289\n",
      "[191]\tvalidation_0-mlogloss:3.02210\n",
      "[192]\tvalidation_0-mlogloss:3.02188\n",
      "[193]\tvalidation_0-mlogloss:3.02287\n",
      "[194]\tvalidation_0-mlogloss:3.02265\n",
      "[195]\tvalidation_0-mlogloss:3.02189\n",
      "[196]\tvalidation_0-mlogloss:3.02156\n",
      "[197]\tvalidation_0-mlogloss:3.02106\n",
      "[198]\tvalidation_0-mlogloss:3.02090\n",
      "[199]\tvalidation_0-mlogloss:3.02039\n",
      "[200]\tvalidation_0-mlogloss:3.02039\n",
      "[201]\tvalidation_0-mlogloss:3.02074\n",
      "[202]\tvalidation_0-mlogloss:3.02065\n",
      "[203]\tvalidation_0-mlogloss:3.02090\n",
      "[204]\tvalidation_0-mlogloss:3.01989\n",
      "[205]\tvalidation_0-mlogloss:3.01953\n",
      "[206]\tvalidation_0-mlogloss:3.01866\n",
      "[207]\tvalidation_0-mlogloss:3.01776\n",
      "[208]\tvalidation_0-mlogloss:3.01692\n",
      "[209]\tvalidation_0-mlogloss:3.01644\n",
      "[210]\tvalidation_0-mlogloss:3.01670\n",
      "[211]\tvalidation_0-mlogloss:3.01637\n",
      "[212]\tvalidation_0-mlogloss:3.01630\n",
      "[213]\tvalidation_0-mlogloss:3.01627\n",
      "[214]\tvalidation_0-mlogloss:3.01626\n",
      "[215]\tvalidation_0-mlogloss:3.01613\n",
      "[216]\tvalidation_0-mlogloss:3.01599\n",
      "[217]\tvalidation_0-mlogloss:3.01535\n",
      "[218]\tvalidation_0-mlogloss:3.01551\n",
      "[219]\tvalidation_0-mlogloss:3.01577\n",
      "[220]\tvalidation_0-mlogloss:3.01528\n",
      "[221]\tvalidation_0-mlogloss:3.01494\n",
      "[222]\tvalidation_0-mlogloss:3.01441\n",
      "[223]\tvalidation_0-mlogloss:3.01516\n",
      "[224]\tvalidation_0-mlogloss:3.01505\n",
      "[225]\tvalidation_0-mlogloss:3.01520\n",
      "[226]\tvalidation_0-mlogloss:3.01463\n",
      "[227]\tvalidation_0-mlogloss:3.01470\n",
      "[228]\tvalidation_0-mlogloss:3.01444\n",
      "[229]\tvalidation_0-mlogloss:3.01435\n",
      "[230]\tvalidation_0-mlogloss:3.01438\n",
      "[231]\tvalidation_0-mlogloss:3.01424\n",
      "[232]\tvalidation_0-mlogloss:3.01441\n",
      "[233]\tvalidation_0-mlogloss:3.01429\n",
      "[234]\tvalidation_0-mlogloss:3.01417\n",
      "[235]\tvalidation_0-mlogloss:3.01380\n",
      "[236]\tvalidation_0-mlogloss:3.01348\n",
      "[237]\tvalidation_0-mlogloss:3.01380\n",
      "[238]\tvalidation_0-mlogloss:3.01373\n",
      "[239]\tvalidation_0-mlogloss:3.01380\n",
      "[240]\tvalidation_0-mlogloss:3.01325\n",
      "[241]\tvalidation_0-mlogloss:3.01369\n",
      "[242]\tvalidation_0-mlogloss:3.01394\n",
      "[243]\tvalidation_0-mlogloss:3.01461\n",
      "[244]\tvalidation_0-mlogloss:3.01388\n",
      "[245]\tvalidation_0-mlogloss:3.01486\n",
      "[246]\tvalidation_0-mlogloss:3.01504\n",
      "[247]\tvalidation_0-mlogloss:3.01525\n",
      "[248]\tvalidation_0-mlogloss:3.01539\n",
      "[249]\tvalidation_0-mlogloss:3.01626\n",
      "[250]\tvalidation_0-mlogloss:3.01692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.08      0.09      1499\n",
      "           1       0.06      0.06      0.06      1499\n",
      "           2       0.24      0.19      0.21      1499\n",
      "           3       0.56      0.56      0.56      1499\n",
      "           4       0.76      0.69      0.72      1499\n",
      "           5       0.35      0.34      0.34      1499\n",
      "           6       0.25      0.37      0.30      1499\n",
      "           7       0.39      0.42      0.40      1499\n",
      "           8       0.48      0.62      0.54      1499\n",
      "           9       0.53      0.27      0.36      1499\n",
      "          10       0.26      0.20      0.23      1499\n",
      "          11       0.22      0.18      0.20      1499\n",
      "          12       0.18      0.09      0.12      1499\n",
      "          13       0.09      0.05      0.06      1499\n",
      "          14       0.10      0.06      0.07      1499\n",
      "          15       0.30      0.28      0.29      1499\n",
      "          16       0.17      0.11      0.13      1499\n",
      "          17       0.31      0.76      0.44      1499\n",
      "          18       0.13      0.06      0.08      1499\n",
      "          19       0.13      0.10      0.12      1499\n",
      "          20       0.66      0.61      0.63      1499\n",
      "          21       0.34      0.26      0.29      1499\n",
      "          22       0.21      0.15      0.18      1499\n",
      "          23       0.60      0.67      0.63      1499\n",
      "          24       0.29      0.38      0.33      1499\n",
      "          25       0.11      0.09      0.10      1499\n",
      "          26       0.13      0.14      0.14      1499\n",
      "          27       0.24      0.22      0.23      1499\n",
      "          28       0.35      0.46      0.40      1499\n",
      "          29       0.25      0.36      0.30      1499\n",
      "          30       0.18      0.14      0.16      1499\n",
      "          31       0.18      0.17      0.17      1499\n",
      "          32       0.10      0.06      0.08      1499\n",
      "          33       0.11      0.09      0.10      1499\n",
      "          34       0.47      0.50      0.48      1499\n",
      "          35       0.33      0.65      0.44      1499\n",
      "          36       0.16      0.29      0.21      1499\n",
      "          37       0.28      0.18      0.22      1499\n",
      "          38       0.25      0.15      0.19      1499\n",
      "          39       0.28      0.28      0.28      1499\n",
      "          40       0.21      0.37      0.27      1499\n",
      "          41       0.25      0.41      0.31      1499\n",
      "          42       0.21      0.14      0.16      1499\n",
      "          43       0.26      0.21      0.23      1499\n",
      "          44       0.43      0.54      0.48      1499\n",
      "          45       0.06      0.04      0.05      1499\n",
      "          46       0.12      0.17      0.14      1499\n",
      "          47       0.25      0.20      0.22      1499\n",
      "          48       0.48      0.70      0.57      1499\n",
      "          49       0.07      0.06      0.07      1499\n",
      "          50       0.26      0.55      0.35      1499\n",
      "          51       0.26      0.16      0.20      1499\n",
      "          52       0.23      0.12      0.16      1499\n",
      "          53       0.12      0.08      0.10      1499\n",
      "          54       0.20      0.42      0.27      1499\n",
      "          55       0.36      0.45      0.40      1499\n",
      "          56       0.08      0.06      0.07      1499\n",
      "          57       0.42      0.58      0.49      1499\n",
      "          58       0.75      0.61      0.67      1499\n",
      "          59       0.08      0.07      0.07      1499\n",
      "          60       0.39      0.55      0.46      1499\n",
      "          61       0.13      0.12      0.12      1499\n",
      "          62       0.32      0.37      0.34      1499\n",
      "          63       0.03      0.03      0.03      1499\n",
      "          64       0.21      0.14      0.17      1499\n",
      "          65       0.11      0.05      0.07      1499\n",
      "          66       0.18      0.25      0.21      1499\n",
      "          67       0.41      0.26      0.32      1499\n",
      "          68       0.40      0.39      0.39      1499\n",
      "          69       0.25      0.17      0.20      1499\n",
      "          70       0.28      0.21      0.24      1499\n",
      "          71       0.06      0.05      0.06      1499\n",
      "          72       0.10      0.11      0.11      1499\n",
      "          73       0.18      0.22      0.20      1499\n",
      "          74       0.10      0.06      0.07      1499\n",
      "\n",
      "    accuracy                           0.27    112425\n",
      "   macro avg       0.26      0.27      0.26    112425\n",
      "weighted avg       0.26      0.27      0.26    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b181cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 24s 841us/step - loss: 2.7382 - val_loss: 3.1125\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 23s 821us/step - loss: 2.1957 - val_loss: 3.0850\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 24s 839us/step - loss: 2.0590 - val_loss: 3.0434\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 23s 800us/step - loss: 2.0088 - val_loss: 3.0299\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 23s 824us/step - loss: 1.9786 - val_loss: 3.0936\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 24s 836us/step - loss: 1.9555 - val_loss: 3.0588\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 24s 870us/step - loss: 1.9370 - val_loss: 3.0399\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 25s 882us/step - loss: 1.9241 - val_loss: 3.1220\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 24s 864us/step - loss: 1.9105 - val_loss: 3.2073\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 25s 871us/step - loss: 1.9014 - val_loss: 3.0763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28fddf880>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 1s 359us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.07      0.09      1499\n",
      "           1       0.06      0.05      0.06      1499\n",
      "           2       0.30      0.13      0.18      1499\n",
      "           3       0.72      0.57      0.63      1499\n",
      "           4       0.75      0.69      0.72      1499\n",
      "           5       0.33      0.26      0.29      1499\n",
      "           6       0.40      0.35      0.37      1499\n",
      "           7       0.56      0.45      0.50      1499\n",
      "           8       0.50      0.62      0.55      1499\n",
      "           9       0.76      0.27      0.40      1499\n",
      "          10       0.17      0.11      0.13      1499\n",
      "          11       0.15      0.34      0.20      1499\n",
      "          12       0.24      0.08      0.11      1499\n",
      "          13       0.09      0.04      0.06      1499\n",
      "          14       0.15      0.05      0.08      1499\n",
      "          15       0.29      0.19      0.23      1499\n",
      "          16       0.19      0.11      0.14      1499\n",
      "          17       0.31      0.65      0.42      1499\n",
      "          18       0.12      0.15      0.13      1499\n",
      "          19       0.14      0.06      0.08      1499\n",
      "          20       0.62      0.41      0.49      1499\n",
      "          21       0.36      0.21      0.26      1499\n",
      "          22       0.23      0.16      0.19      1499\n",
      "          23       0.41      0.72      0.52      1499\n",
      "          24       0.29      0.41      0.34      1499\n",
      "          25       0.25      0.19      0.22      1499\n",
      "          26       0.16      0.25      0.20      1499\n",
      "          27       0.26      0.31      0.28      1499\n",
      "          28       0.32      0.68      0.43      1499\n",
      "          29       0.33      0.47      0.39      1499\n",
      "          30       0.18      0.24      0.21      1499\n",
      "          31       0.20      0.18      0.19      1499\n",
      "          32       0.14      0.04      0.07      1499\n",
      "          33       0.13      0.13      0.13      1499\n",
      "          34       0.62      0.47      0.53      1499\n",
      "          35       0.37      0.63      0.47      1499\n",
      "          36       0.21      0.17      0.19      1499\n",
      "          37       0.24      0.20      0.22      1499\n",
      "          38       0.23      0.25      0.24      1499\n",
      "          39       0.21      0.32      0.26      1499\n",
      "          40       0.24      0.35      0.28      1499\n",
      "          41       0.10      0.08      0.09      1499\n",
      "          42       0.15      0.12      0.13      1499\n",
      "          43       0.21      0.19      0.20      1499\n",
      "          44       0.37      0.65      0.47      1499\n",
      "          45       0.07      0.05      0.06      1499\n",
      "          46       0.21      0.35      0.26      1499\n",
      "          47       0.27      0.14      0.19      1499\n",
      "          48       0.60      0.72      0.65      1499\n",
      "          49       0.06      0.05      0.05      1499\n",
      "          50       0.32      0.49      0.38      1499\n",
      "          51       0.25      0.09      0.13      1499\n",
      "          52       0.25      0.11      0.15      1499\n",
      "          53       0.13      0.12      0.12      1499\n",
      "          54       0.28      0.52      0.36      1499\n",
      "          55       0.32      0.54      0.40      1499\n",
      "          56       0.07      0.09      0.08      1499\n",
      "          57       0.43      0.66      0.52      1499\n",
      "          58       0.80      0.57      0.66      1499\n",
      "          59       0.04      0.03      0.03      1499\n",
      "          60       0.53      0.63      0.58      1499\n",
      "          61       0.12      0.05      0.07      1499\n",
      "          62       0.40      0.40      0.40      1499\n",
      "          63       0.08      0.07      0.07      1499\n",
      "          64       0.20      0.09      0.12      1499\n",
      "          65       0.15      0.05      0.07      1499\n",
      "          66       0.17      0.25      0.20      1499\n",
      "          67       0.36      0.35      0.35      1499\n",
      "          68       0.38      0.57      0.46      1499\n",
      "          69       0.25      0.24      0.25      1499\n",
      "          70       0.26      0.19      0.22      1499\n",
      "          71       0.08      0.05      0.06      1499\n",
      "          72       0.12      0.28      0.17      1499\n",
      "          73       0.19      0.20      0.19      1499\n",
      "          74       0.12      0.06      0.08      1499\n",
      "\n",
      "    accuracy                           0.28    112425\n",
      "   macro avg       0.28      0.28      0.26    112425\n",
      "weighted avg       0.28      0.28      0.26    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41757/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_41757/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_41757/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000053   0.000060   0.000035   0.000044   0.000049   0.000032   \n",
       "899996   0.000035   0.000046   0.000006   0.000023   0.000027  -0.000001   \n",
       "899997   0.000031   0.000011  -0.000019  -0.000008   0.000003  -0.000027   \n",
       "899998   0.000011   0.000017   0.000021  -0.000007   0.000022   0.000058   \n",
       "899999  -0.000008   0.000050   0.000059   0.000020   0.000048   0.000076   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000064   0.000115   0.000076   0.000070  ...    0.000020   \n",
       "899996   0.000035   0.000067   0.000067   0.000043  ...   -0.000052   \n",
       "899997   0.000007   0.000019   0.000022   0.000012  ...   -0.000039   \n",
       "899998   0.000063   0.000008   0.000050   0.000035  ...   -0.000041   \n",
       "899999   0.000087   0.000007   0.000088   0.000068  ...   -0.000042   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995    0.000121    0.000117    0.000060    0.000022    0.000036   \n",
       "899996    0.000027    0.000048    0.000016   -0.000032   -0.000023   \n",
       "899997    0.000022    0.000042    0.000023   -0.000008    0.000002   \n",
       "899998    0.000021    0.000048    0.000049    0.000023    0.000002   \n",
       "899999    0.000014    0.000031    0.000039    0.000003   -0.000016   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000135    0.000116    0.000082    0.000140  \n",
       "899996    0.000050    0.000045    0.000020    0.000031  \n",
       "899997    0.000035    0.000048    0.000055    0.000033  \n",
       "899998    0.000028    0.000028    0.000037    0.000026  \n",
       "899999    0.000010    0.000000    0.000014   -0.000003  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.97688\n",
      "[1]\tvalidation_0-mlogloss:3.82565\n",
      "[2]\tvalidation_0-mlogloss:3.71997\n",
      "[3]\tvalidation_0-mlogloss:3.63783\n",
      "[4]\tvalidation_0-mlogloss:3.56949\n",
      "[5]\tvalidation_0-mlogloss:3.51264\n",
      "[6]\tvalidation_0-mlogloss:3.46750\n",
      "[7]\tvalidation_0-mlogloss:3.42456\n",
      "[8]\tvalidation_0-mlogloss:3.38510\n",
      "[9]\tvalidation_0-mlogloss:3.35379\n",
      "[10]\tvalidation_0-mlogloss:3.32325\n",
      "[11]\tvalidation_0-mlogloss:3.29447\n",
      "[12]\tvalidation_0-mlogloss:3.26926\n",
      "[13]\tvalidation_0-mlogloss:3.24776\n",
      "[14]\tvalidation_0-mlogloss:3.22389\n",
      "[15]\tvalidation_0-mlogloss:3.20452\n",
      "[16]\tvalidation_0-mlogloss:3.18528\n",
      "[17]\tvalidation_0-mlogloss:3.16811\n",
      "[18]\tvalidation_0-mlogloss:3.15288\n",
      "[19]\tvalidation_0-mlogloss:3.13718\n",
      "[20]\tvalidation_0-mlogloss:3.12502\n",
      "[21]\tvalidation_0-mlogloss:3.11146\n",
      "[22]\tvalidation_0-mlogloss:3.09980\n",
      "[23]\tvalidation_0-mlogloss:3.08305\n",
      "[24]\tvalidation_0-mlogloss:3.07079\n",
      "[25]\tvalidation_0-mlogloss:3.05778\n",
      "[26]\tvalidation_0-mlogloss:3.04830\n",
      "[27]\tvalidation_0-mlogloss:3.03679\n",
      "[28]\tvalidation_0-mlogloss:3.02672\n",
      "[29]\tvalidation_0-mlogloss:3.01414\n",
      "[30]\tvalidation_0-mlogloss:3.00391\n",
      "[31]\tvalidation_0-mlogloss:2.99241\n",
      "[32]\tvalidation_0-mlogloss:2.98224\n",
      "[33]\tvalidation_0-mlogloss:2.97195\n",
      "[34]\tvalidation_0-mlogloss:2.96166\n",
      "[35]\tvalidation_0-mlogloss:2.95409\n",
      "[36]\tvalidation_0-mlogloss:2.94484\n",
      "[37]\tvalidation_0-mlogloss:2.93519\n",
      "[38]\tvalidation_0-mlogloss:2.92572\n",
      "[39]\tvalidation_0-mlogloss:2.91858\n",
      "[40]\tvalidation_0-mlogloss:2.90906\n",
      "[41]\tvalidation_0-mlogloss:2.90027\n",
      "[42]\tvalidation_0-mlogloss:2.89187\n",
      "[43]\tvalidation_0-mlogloss:2.88180\n",
      "[44]\tvalidation_0-mlogloss:2.87404\n",
      "[45]\tvalidation_0-mlogloss:2.86688\n",
      "[46]\tvalidation_0-mlogloss:2.86007\n",
      "[47]\tvalidation_0-mlogloss:2.85259\n",
      "[48]\tvalidation_0-mlogloss:2.84614\n",
      "[49]\tvalidation_0-mlogloss:2.83797\n",
      "[50]\tvalidation_0-mlogloss:2.83342\n",
      "[51]\tvalidation_0-mlogloss:2.82716\n",
      "[52]\tvalidation_0-mlogloss:2.82039\n",
      "[53]\tvalidation_0-mlogloss:2.81702\n",
      "[54]\tvalidation_0-mlogloss:2.81034\n",
      "[55]\tvalidation_0-mlogloss:2.80273\n",
      "[56]\tvalidation_0-mlogloss:2.79656\n",
      "[57]\tvalidation_0-mlogloss:2.78939\n",
      "[58]\tvalidation_0-mlogloss:2.78313\n",
      "[59]\tvalidation_0-mlogloss:2.77833\n",
      "[60]\tvalidation_0-mlogloss:2.77154\n",
      "[61]\tvalidation_0-mlogloss:2.76825\n",
      "[62]\tvalidation_0-mlogloss:2.76314\n",
      "[63]\tvalidation_0-mlogloss:2.75901\n",
      "[64]\tvalidation_0-mlogloss:2.75476\n",
      "[65]\tvalidation_0-mlogloss:2.74945\n",
      "[66]\tvalidation_0-mlogloss:2.74504\n",
      "[67]\tvalidation_0-mlogloss:2.74004\n",
      "[68]\tvalidation_0-mlogloss:2.73558\n",
      "[69]\tvalidation_0-mlogloss:2.72994\n",
      "[70]\tvalidation_0-mlogloss:2.72424\n",
      "[71]\tvalidation_0-mlogloss:2.72112\n",
      "[72]\tvalidation_0-mlogloss:2.71564\n",
      "[73]\tvalidation_0-mlogloss:2.71128\n",
      "[74]\tvalidation_0-mlogloss:2.70841\n",
      "[75]\tvalidation_0-mlogloss:2.70296\n",
      "[76]\tvalidation_0-mlogloss:2.69920\n",
      "[77]\tvalidation_0-mlogloss:2.69576\n",
      "[78]\tvalidation_0-mlogloss:2.69234\n",
      "[79]\tvalidation_0-mlogloss:2.68844\n",
      "[80]\tvalidation_0-mlogloss:2.68498\n",
      "[81]\tvalidation_0-mlogloss:2.68010\n",
      "[82]\tvalidation_0-mlogloss:2.67601\n",
      "[83]\tvalidation_0-mlogloss:2.67282\n",
      "[84]\tvalidation_0-mlogloss:2.66913\n",
      "[85]\tvalidation_0-mlogloss:2.66533\n",
      "[86]\tvalidation_0-mlogloss:2.66173\n",
      "[87]\tvalidation_0-mlogloss:2.65933\n",
      "[88]\tvalidation_0-mlogloss:2.65582\n",
      "[89]\tvalidation_0-mlogloss:2.65276\n",
      "[90]\tvalidation_0-mlogloss:2.65025\n",
      "[91]\tvalidation_0-mlogloss:2.64734\n",
      "[92]\tvalidation_0-mlogloss:2.64371\n",
      "[93]\tvalidation_0-mlogloss:2.64119\n",
      "[94]\tvalidation_0-mlogloss:2.63864\n",
      "[95]\tvalidation_0-mlogloss:2.63547\n",
      "[96]\tvalidation_0-mlogloss:2.63147\n",
      "[97]\tvalidation_0-mlogloss:2.62690\n",
      "[98]\tvalidation_0-mlogloss:2.62539\n",
      "[99]\tvalidation_0-mlogloss:2.62179\n",
      "[100]\tvalidation_0-mlogloss:2.61982\n",
      "[101]\tvalidation_0-mlogloss:2.61776\n",
      "[102]\tvalidation_0-mlogloss:2.61659\n",
      "[103]\tvalidation_0-mlogloss:2.61411\n",
      "[104]\tvalidation_0-mlogloss:2.61166\n",
      "[105]\tvalidation_0-mlogloss:2.60892\n",
      "[106]\tvalidation_0-mlogloss:2.60650\n",
      "[107]\tvalidation_0-mlogloss:2.60450\n",
      "[108]\tvalidation_0-mlogloss:2.60149\n",
      "[109]\tvalidation_0-mlogloss:2.59818\n",
      "[110]\tvalidation_0-mlogloss:2.59560\n",
      "[111]\tvalidation_0-mlogloss:2.59372\n",
      "[112]\tvalidation_0-mlogloss:2.59214\n",
      "[113]\tvalidation_0-mlogloss:2.58882\n",
      "[114]\tvalidation_0-mlogloss:2.58736\n",
      "[115]\tvalidation_0-mlogloss:2.58513\n",
      "[116]\tvalidation_0-mlogloss:2.58154\n",
      "[117]\tvalidation_0-mlogloss:2.58094\n",
      "[118]\tvalidation_0-mlogloss:2.57810\n",
      "[119]\tvalidation_0-mlogloss:2.57647\n",
      "[120]\tvalidation_0-mlogloss:2.57335\n",
      "[121]\tvalidation_0-mlogloss:2.57235\n",
      "[122]\tvalidation_0-mlogloss:2.57084\n",
      "[123]\tvalidation_0-mlogloss:2.56922\n",
      "[124]\tvalidation_0-mlogloss:2.56759\n",
      "[125]\tvalidation_0-mlogloss:2.56510\n",
      "[126]\tvalidation_0-mlogloss:2.56295\n",
      "[127]\tvalidation_0-mlogloss:2.56141\n",
      "[128]\tvalidation_0-mlogloss:2.55965\n",
      "[129]\tvalidation_0-mlogloss:2.55872\n",
      "[130]\tvalidation_0-mlogloss:2.55589\n",
      "[131]\tvalidation_0-mlogloss:2.55442\n",
      "[132]\tvalidation_0-mlogloss:2.55161\n",
      "[133]\tvalidation_0-mlogloss:2.55002\n",
      "[134]\tvalidation_0-mlogloss:2.54684\n",
      "[135]\tvalidation_0-mlogloss:2.54536\n",
      "[136]\tvalidation_0-mlogloss:2.54353\n",
      "[137]\tvalidation_0-mlogloss:2.54189\n",
      "[138]\tvalidation_0-mlogloss:2.54029\n",
      "[139]\tvalidation_0-mlogloss:2.53961\n",
      "[140]\tvalidation_0-mlogloss:2.53823\n",
      "[141]\tvalidation_0-mlogloss:2.53634\n",
      "[142]\tvalidation_0-mlogloss:2.53540\n",
      "[143]\tvalidation_0-mlogloss:2.53321\n",
      "[144]\tvalidation_0-mlogloss:2.53121\n",
      "[145]\tvalidation_0-mlogloss:2.52961\n",
      "[146]\tvalidation_0-mlogloss:2.52713\n",
      "[147]\tvalidation_0-mlogloss:2.52606\n",
      "[148]\tvalidation_0-mlogloss:2.52564\n",
      "[149]\tvalidation_0-mlogloss:2.52395\n",
      "[150]\tvalidation_0-mlogloss:2.52263\n",
      "[151]\tvalidation_0-mlogloss:2.52003\n",
      "[152]\tvalidation_0-mlogloss:2.51944\n",
      "[153]\tvalidation_0-mlogloss:2.51750\n",
      "[154]\tvalidation_0-mlogloss:2.51742\n",
      "[155]\tvalidation_0-mlogloss:2.51680\n",
      "[156]\tvalidation_0-mlogloss:2.51570\n",
      "[157]\tvalidation_0-mlogloss:2.51353\n",
      "[158]\tvalidation_0-mlogloss:2.51170\n",
      "[159]\tvalidation_0-mlogloss:2.51081\n",
      "[160]\tvalidation_0-mlogloss:2.50928\n",
      "[161]\tvalidation_0-mlogloss:2.50859\n",
      "[162]\tvalidation_0-mlogloss:2.50723\n",
      "[163]\tvalidation_0-mlogloss:2.50701\n",
      "[164]\tvalidation_0-mlogloss:2.50523\n",
      "[165]\tvalidation_0-mlogloss:2.50408\n",
      "[166]\tvalidation_0-mlogloss:2.50261\n",
      "[167]\tvalidation_0-mlogloss:2.50234\n",
      "[168]\tvalidation_0-mlogloss:2.50142\n",
      "[169]\tvalidation_0-mlogloss:2.49982\n",
      "[170]\tvalidation_0-mlogloss:2.49856\n",
      "[171]\tvalidation_0-mlogloss:2.49623\n",
      "[172]\tvalidation_0-mlogloss:2.49538\n",
      "[173]\tvalidation_0-mlogloss:2.49452\n",
      "[174]\tvalidation_0-mlogloss:2.49341\n",
      "[175]\tvalidation_0-mlogloss:2.49202\n",
      "[176]\tvalidation_0-mlogloss:2.49123\n",
      "[177]\tvalidation_0-mlogloss:2.49022\n",
      "[178]\tvalidation_0-mlogloss:2.48992\n",
      "[179]\tvalidation_0-mlogloss:2.48998\n",
      "[180]\tvalidation_0-mlogloss:2.48941\n",
      "[181]\tvalidation_0-mlogloss:2.48756\n",
      "[182]\tvalidation_0-mlogloss:2.48699\n",
      "[183]\tvalidation_0-mlogloss:2.48523\n",
      "[184]\tvalidation_0-mlogloss:2.48395\n",
      "[185]\tvalidation_0-mlogloss:2.48221\n",
      "[186]\tvalidation_0-mlogloss:2.48122\n",
      "[187]\tvalidation_0-mlogloss:2.48001\n",
      "[188]\tvalidation_0-mlogloss:2.47904\n",
      "[189]\tvalidation_0-mlogloss:2.47865\n",
      "[190]\tvalidation_0-mlogloss:2.47876\n",
      "[191]\tvalidation_0-mlogloss:2.47816\n",
      "[192]\tvalidation_0-mlogloss:2.47669\n",
      "[193]\tvalidation_0-mlogloss:2.47607\n",
      "[194]\tvalidation_0-mlogloss:2.47427\n",
      "[195]\tvalidation_0-mlogloss:2.47364\n",
      "[196]\tvalidation_0-mlogloss:2.47266\n",
      "[197]\tvalidation_0-mlogloss:2.47146\n",
      "[198]\tvalidation_0-mlogloss:2.47083\n",
      "[199]\tvalidation_0-mlogloss:2.47015\n",
      "[200]\tvalidation_0-mlogloss:2.47022\n",
      "[201]\tvalidation_0-mlogloss:2.46941\n",
      "[202]\tvalidation_0-mlogloss:2.46802\n",
      "[203]\tvalidation_0-mlogloss:2.46731\n",
      "[204]\tvalidation_0-mlogloss:2.46717\n",
      "[205]\tvalidation_0-mlogloss:2.46648\n",
      "[206]\tvalidation_0-mlogloss:2.46602\n",
      "[207]\tvalidation_0-mlogloss:2.46435\n",
      "[208]\tvalidation_0-mlogloss:2.46382\n",
      "[209]\tvalidation_0-mlogloss:2.46374\n",
      "[210]\tvalidation_0-mlogloss:2.46459\n",
      "[211]\tvalidation_0-mlogloss:2.46397\n",
      "[212]\tvalidation_0-mlogloss:2.46297\n",
      "[213]\tvalidation_0-mlogloss:2.46256\n",
      "[214]\tvalidation_0-mlogloss:2.46241\n",
      "[215]\tvalidation_0-mlogloss:2.46166\n",
      "[216]\tvalidation_0-mlogloss:2.46094\n",
      "[217]\tvalidation_0-mlogloss:2.46118\n",
      "[218]\tvalidation_0-mlogloss:2.45991\n",
      "[219]\tvalidation_0-mlogloss:2.45942\n",
      "[220]\tvalidation_0-mlogloss:2.45854\n",
      "[221]\tvalidation_0-mlogloss:2.45699\n",
      "[222]\tvalidation_0-mlogloss:2.45561\n",
      "[223]\tvalidation_0-mlogloss:2.45497\n",
      "[224]\tvalidation_0-mlogloss:2.45503\n",
      "[225]\tvalidation_0-mlogloss:2.45438\n",
      "[226]\tvalidation_0-mlogloss:2.45393\n",
      "[227]\tvalidation_0-mlogloss:2.45370\n",
      "[228]\tvalidation_0-mlogloss:2.45257\n",
      "[229]\tvalidation_0-mlogloss:2.45217\n",
      "[230]\tvalidation_0-mlogloss:2.45173\n",
      "[231]\tvalidation_0-mlogloss:2.45237\n",
      "[232]\tvalidation_0-mlogloss:2.45255\n",
      "[233]\tvalidation_0-mlogloss:2.45173\n",
      "[234]\tvalidation_0-mlogloss:2.45102\n",
      "[235]\tvalidation_0-mlogloss:2.45188\n",
      "[236]\tvalidation_0-mlogloss:2.45199\n",
      "[237]\tvalidation_0-mlogloss:2.45199\n",
      "[238]\tvalidation_0-mlogloss:2.45186\n",
      "[239]\tvalidation_0-mlogloss:2.45139\n",
      "[240]\tvalidation_0-mlogloss:2.45093\n",
      "[241]\tvalidation_0-mlogloss:2.45102\n",
      "[242]\tvalidation_0-mlogloss:2.44986\n",
      "[243]\tvalidation_0-mlogloss:2.45003\n",
      "[244]\tvalidation_0-mlogloss:2.45030\n",
      "[245]\tvalidation_0-mlogloss:2.45002\n",
      "[246]\tvalidation_0-mlogloss:2.45016\n",
      "[247]\tvalidation_0-mlogloss:2.45013\n",
      "[248]\tvalidation_0-mlogloss:2.44922\n",
      "[249]\tvalidation_0-mlogloss:2.44972\n",
      "[250]\tvalidation_0-mlogloss:2.44962\n",
      "[251]\tvalidation_0-mlogloss:2.45007\n",
      "[252]\tvalidation_0-mlogloss:2.44962\n",
      "[253]\tvalidation_0-mlogloss:2.44851\n",
      "[254]\tvalidation_0-mlogloss:2.44835\n",
      "[255]\tvalidation_0-mlogloss:2.44810\n",
      "[256]\tvalidation_0-mlogloss:2.44768\n",
      "[257]\tvalidation_0-mlogloss:2.44766\n",
      "[258]\tvalidation_0-mlogloss:2.44815\n",
      "[259]\tvalidation_0-mlogloss:2.44753\n",
      "[260]\tvalidation_0-mlogloss:2.44699\n",
      "[261]\tvalidation_0-mlogloss:2.44692\n",
      "[262]\tvalidation_0-mlogloss:2.44695\n",
      "[263]\tvalidation_0-mlogloss:2.44708\n",
      "[264]\tvalidation_0-mlogloss:2.44753\n",
      "[265]\tvalidation_0-mlogloss:2.44812\n",
      "[266]\tvalidation_0-mlogloss:2.44791\n",
      "[267]\tvalidation_0-mlogloss:2.44803\n",
      "[268]\tvalidation_0-mlogloss:2.44714\n",
      "[269]\tvalidation_0-mlogloss:2.44727\n",
      "[270]\tvalidation_0-mlogloss:2.44602\n",
      "[271]\tvalidation_0-mlogloss:2.44626\n",
      "[272]\tvalidation_0-mlogloss:2.44586\n",
      "[273]\tvalidation_0-mlogloss:2.44545\n",
      "[274]\tvalidation_0-mlogloss:2.44508\n",
      "[275]\tvalidation_0-mlogloss:2.44516\n",
      "[276]\tvalidation_0-mlogloss:2.44541\n",
      "[277]\tvalidation_0-mlogloss:2.44543\n",
      "[278]\tvalidation_0-mlogloss:2.44590\n",
      "[279]\tvalidation_0-mlogloss:2.44578\n",
      "[280]\tvalidation_0-mlogloss:2.44555\n",
      "[281]\tvalidation_0-mlogloss:2.44542\n",
      "[282]\tvalidation_0-mlogloss:2.44554\n",
      "[283]\tvalidation_0-mlogloss:2.44531\n",
      "[284]\tvalidation_0-mlogloss:2.44438\n",
      "[285]\tvalidation_0-mlogloss:2.44425\n",
      "[286]\tvalidation_0-mlogloss:2.44397\n",
      "[287]\tvalidation_0-mlogloss:2.44335\n",
      "[288]\tvalidation_0-mlogloss:2.44357\n",
      "[289]\tvalidation_0-mlogloss:2.44287\n",
      "[290]\tvalidation_0-mlogloss:2.44277\n",
      "[291]\tvalidation_0-mlogloss:2.44182\n",
      "[292]\tvalidation_0-mlogloss:2.44177\n",
      "[293]\tvalidation_0-mlogloss:2.44142\n",
      "[294]\tvalidation_0-mlogloss:2.44087\n",
      "[295]\tvalidation_0-mlogloss:2.44061\n",
      "[296]\tvalidation_0-mlogloss:2.44074\n",
      "[297]\tvalidation_0-mlogloss:2.44049\n",
      "[298]\tvalidation_0-mlogloss:2.44017\n",
      "[299]\tvalidation_0-mlogloss:2.43979\n",
      "[300]\tvalidation_0-mlogloss:2.43987\n",
      "[301]\tvalidation_0-mlogloss:2.43996\n",
      "[302]\tvalidation_0-mlogloss:2.44041\n",
      "[303]\tvalidation_0-mlogloss:2.44149\n",
      "[304]\tvalidation_0-mlogloss:2.44230\n",
      "[305]\tvalidation_0-mlogloss:2.44290\n",
      "[306]\tvalidation_0-mlogloss:2.44239\n",
      "[307]\tvalidation_0-mlogloss:2.44236\n",
      "[308]\tvalidation_0-mlogloss:2.44258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.22      0.23      1499\n",
      "           1       0.10      0.12      0.11      1499\n",
      "           2       0.35      0.24      0.28      1499\n",
      "           3       0.59      0.41      0.48      1499\n",
      "           4       0.87      0.83      0.85      1499\n",
      "           5       0.49      0.45      0.47      1499\n",
      "           6       0.53      0.68      0.60      1499\n",
      "           7       0.55      0.43      0.48      1499\n",
      "           8       0.57      0.68      0.62      1499\n",
      "           9       0.87      0.63      0.73      1499\n",
      "          10       0.29      0.17      0.21      1499\n",
      "          11       0.42      0.45      0.43      1499\n",
      "          12       0.30      0.46      0.36      1499\n",
      "          13       0.18      0.18      0.18      1499\n",
      "          14       0.16      0.13      0.14      1499\n",
      "          15       0.55      0.38      0.45      1499\n",
      "          16       0.27      0.10      0.15      1499\n",
      "          17       0.44      0.62      0.52      1499\n",
      "          18       0.29      0.46      0.35      1499\n",
      "          19       0.28      0.27      0.27      1499\n",
      "          20       0.76      0.89      0.82      1499\n",
      "          21       0.33      0.25      0.29      1499\n",
      "          22       0.28      0.28      0.28      1499\n",
      "          23       0.79      0.83      0.81      1499\n",
      "          24       0.54      0.44      0.48      1499\n",
      "          25       0.52      0.33      0.40      1499\n",
      "          26       0.34      0.32      0.33      1499\n",
      "          27       0.21      0.24      0.22      1499\n",
      "          28       0.45      0.50      0.47      1499\n",
      "          29       0.36      0.55      0.43      1499\n",
      "          30       0.27      0.24      0.25      1499\n",
      "          31       0.26      0.39      0.31      1499\n",
      "          32       0.09      0.07      0.08      1499\n",
      "          33       0.32      0.32      0.32      1499\n",
      "          34       0.66      0.65      0.65      1499\n",
      "          35       0.48      0.73      0.58      1499\n",
      "          36       0.44      0.50      0.47      1499\n",
      "          37       0.37      0.54      0.44      1499\n",
      "          38       0.64      0.51      0.56      1499\n",
      "          39       0.53      0.51      0.52      1499\n",
      "          40       0.59      0.36      0.45      1499\n",
      "          41       0.58      0.61      0.59      1499\n",
      "          42       0.25      0.15      0.19      1499\n",
      "          43       0.35      0.32      0.33      1499\n",
      "          44       0.61      0.75      0.67      1499\n",
      "          45       0.23      0.22      0.23      1499\n",
      "          46       0.20      0.34      0.25      1499\n",
      "          47       0.65      0.53      0.58      1499\n",
      "          48       0.54      0.76      0.63      1499\n",
      "          49       0.30      0.25      0.27      1499\n",
      "          50       0.44      0.66      0.53      1499\n",
      "          51       0.37      0.29      0.33      1499\n",
      "          52       0.31      0.26      0.29      1499\n",
      "          53       0.19      0.14      0.16      1499\n",
      "          54       0.33      0.42      0.37      1499\n",
      "          55       0.50      0.47      0.48      1499\n",
      "          56       0.45      0.35      0.39      1499\n",
      "          57       0.48      0.62      0.54      1499\n",
      "          58       0.96      0.78      0.86      1499\n",
      "          59       0.26      0.16      0.20      1499\n",
      "          60       0.44      0.44      0.44      1499\n",
      "          61       0.30      0.28      0.29      1499\n",
      "          62       0.54      0.52      0.53      1499\n",
      "          63       0.07      0.07      0.07      1499\n",
      "          64       0.35      0.27      0.31      1499\n",
      "          65       0.25      0.21      0.23      1499\n",
      "          66       0.39      0.53      0.45      1499\n",
      "          67       0.66      0.12      0.20      1499\n",
      "          68       0.52      0.55      0.53      1499\n",
      "          69       0.26      0.25      0.26      1499\n",
      "          70       0.69      0.60      0.65      1499\n",
      "          71       0.19      0.21      0.20      1499\n",
      "          72       0.31      0.29      0.30      1499\n",
      "          73       0.29      0.36      0.32      1499\n",
      "          74       0.19      0.29      0.23      1499\n",
      "\n",
      "    accuracy                           0.41    112425\n",
      "   macro avg       0.41      0.41      0.40    112425\n",
      "weighted avg       0.41      0.41      0.40    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7980b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46f7c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 25s 878us/step - loss: 2.0846 - val_loss: 2.9505\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 24s 862us/step - loss: 1.4164 - val_loss: 3.0817\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 24s 865us/step - loss: 1.2646 - val_loss: 3.2629\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 25s 879us/step - loss: 1.1796 - val_loss: 3.1828\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 24s 863us/step - loss: 1.1058 - val_loss: 3.2632\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 25s 892us/step - loss: 1.0445 - val_loss: 3.3203\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 24s 870us/step - loss: 1.0056 - val_loss: 3.2761\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 25s 879us/step - loss: 0.9806 - val_loss: 3.2759\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 25s 885us/step - loss: 0.9608 - val_loss: 3.5193\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 25s 882us/step - loss: 0.9457 - val_loss: 3.3423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x310a87070>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 1s 360us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.34      0.32      1499\n",
      "           1       0.21      0.10      0.13      1499\n",
      "           2       0.38      0.14      0.21      1499\n",
      "           3       0.62      0.60      0.61      1499\n",
      "           4       0.80      0.84      0.82      1499\n",
      "           5       0.52      0.68      0.59      1499\n",
      "           6       0.72      0.78      0.75      1499\n",
      "           7       0.67      0.58      0.62      1499\n",
      "           8       0.52      0.69      0.59      1499\n",
      "           9       0.58      0.59      0.58      1499\n",
      "          10       0.49      0.35      0.41      1499\n",
      "          11       0.50      0.33      0.40      1499\n",
      "          12       0.44      0.44      0.44      1499\n",
      "          13       0.20      0.23      0.21      1499\n",
      "          14       0.26      0.08      0.12      1499\n",
      "          15       0.74      0.25      0.38      1499\n",
      "          16       0.12      0.14      0.13      1499\n",
      "          17       0.45      0.71      0.55      1499\n",
      "          18       0.38      0.48      0.42      1499\n",
      "          19       0.29      0.26      0.27      1499\n",
      "          20       0.64      0.75      0.69      1499\n",
      "          21       0.34      0.49      0.40      1499\n",
      "          22       0.36      0.31      0.34      1499\n",
      "          23       0.78      0.86      0.82      1499\n",
      "          24       0.49      0.32      0.39      1499\n",
      "          25       0.52      0.54      0.53      1499\n",
      "          26       0.34      0.23      0.28      1499\n",
      "          27       0.15      0.17      0.16      1499\n",
      "          28       0.40      0.77      0.53      1499\n",
      "          29       0.43      0.74      0.54      1499\n",
      "          30       0.42      0.42      0.42      1499\n",
      "          31       0.31      0.39      0.35      1499\n",
      "          32       0.10      0.09      0.09      1499\n",
      "          33       0.32      0.21      0.26      1499\n",
      "          34       0.88      0.56      0.68      1499\n",
      "          35       0.71      0.67      0.69      1499\n",
      "          36       0.67      0.25      0.37      1499\n",
      "          37       0.35      0.60      0.44      1499\n",
      "          38       0.30      0.24      0.26      1499\n",
      "          39       0.28      0.54      0.37      1499\n",
      "          40       0.84      0.19      0.31      1499\n",
      "          41       0.54      0.53      0.54      1499\n",
      "          42       0.24      0.15      0.18      1499\n",
      "          43       0.38      0.43      0.41      1499\n",
      "          44       0.68      0.75      0.71      1499\n",
      "          45       0.22      0.25      0.24      1499\n",
      "          46       0.24      0.26      0.25      1499\n",
      "          47       0.63      0.24      0.34      1499\n",
      "          48       0.58      0.80      0.67      1499\n",
      "          49       0.43      0.49      0.46      1499\n",
      "          50       0.45      0.49      0.47      1499\n",
      "          51       0.50      0.27      0.35      1499\n",
      "          52       0.47      0.19      0.27      1499\n",
      "          53       0.25      0.33      0.29      1499\n",
      "          54       0.46      0.50      0.48      1499\n",
      "          55       0.54      0.54      0.54      1499\n",
      "          56       0.41      0.44      0.43      1499\n",
      "          57       0.57      0.77      0.65      1499\n",
      "          58       0.91      0.81      0.86      1499\n",
      "          59       0.20      0.14      0.17      1499\n",
      "          60       0.44      0.64      0.52      1499\n",
      "          61       0.26      0.20      0.23      1499\n",
      "          62       0.52      0.77      0.62      1499\n",
      "          63       0.14      0.06      0.08      1499\n",
      "          64       0.43      0.33      0.37      1499\n",
      "          65       0.44      0.33      0.38      1499\n",
      "          66       0.42      0.44      0.43      1499\n",
      "          67       0.49      0.16      0.24      1499\n",
      "          68       0.56      0.62      0.59      1499\n",
      "          69       0.25      0.43      0.32      1499\n",
      "          70       0.53      0.44      0.48      1499\n",
      "          71       0.23      0.16      0.19      1499\n",
      "          72       0.38      0.48      0.43      1499\n",
      "          73       0.27      0.43      0.33      1499\n",
      "          74       0.18      0.25      0.21      1499\n",
      "\n",
      "    accuracy                           0.43    112425\n",
      "   macro avg       0.44      0.43      0.42    112425\n",
      "weighted avg       0.44      0.43      0.42    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41757/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_41757/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_41757/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.78055\n",
      "[1]\tvalidation_0-mlogloss:3.57016\n",
      "[2]\tvalidation_0-mlogloss:3.42238\n",
      "[3]\tvalidation_0-mlogloss:3.30021\n",
      "[4]\tvalidation_0-mlogloss:3.20967\n",
      "[5]\tvalidation_0-mlogloss:3.13224\n",
      "[6]\tvalidation_0-mlogloss:3.06683\n",
      "[7]\tvalidation_0-mlogloss:3.01234\n",
      "[8]\tvalidation_0-mlogloss:2.96362\n",
      "[9]\tvalidation_0-mlogloss:2.91947\n",
      "[10]\tvalidation_0-mlogloss:2.87832\n",
      "[11]\tvalidation_0-mlogloss:2.83500\n",
      "[12]\tvalidation_0-mlogloss:2.80239\n",
      "[13]\tvalidation_0-mlogloss:2.77259\n",
      "[14]\tvalidation_0-mlogloss:2.74162\n",
      "[15]\tvalidation_0-mlogloss:2.71366\n",
      "[16]\tvalidation_0-mlogloss:2.68637\n",
      "[17]\tvalidation_0-mlogloss:2.66215\n",
      "[18]\tvalidation_0-mlogloss:2.63690\n",
      "[19]\tvalidation_0-mlogloss:2.61959\n",
      "[20]\tvalidation_0-mlogloss:2.59564\n",
      "[21]\tvalidation_0-mlogloss:2.57510\n",
      "[22]\tvalidation_0-mlogloss:2.55419\n",
      "[23]\tvalidation_0-mlogloss:2.53688\n",
      "[24]\tvalidation_0-mlogloss:2.51913\n",
      "[25]\tvalidation_0-mlogloss:2.50247\n",
      "[26]\tvalidation_0-mlogloss:2.48338\n",
      "[27]\tvalidation_0-mlogloss:2.46903\n",
      "[28]\tvalidation_0-mlogloss:2.45404\n",
      "[29]\tvalidation_0-mlogloss:2.44071\n",
      "[30]\tvalidation_0-mlogloss:2.42361\n",
      "[31]\tvalidation_0-mlogloss:2.41156\n",
      "[32]\tvalidation_0-mlogloss:2.39641\n",
      "[33]\tvalidation_0-mlogloss:2.38447\n",
      "[34]\tvalidation_0-mlogloss:2.37256\n",
      "[35]\tvalidation_0-mlogloss:2.35894\n",
      "[36]\tvalidation_0-mlogloss:2.34528\n",
      "[37]\tvalidation_0-mlogloss:2.33401\n",
      "[38]\tvalidation_0-mlogloss:2.32198\n",
      "[39]\tvalidation_0-mlogloss:2.30949\n",
      "[40]\tvalidation_0-mlogloss:2.29845\n",
      "[41]\tvalidation_0-mlogloss:2.28556\n",
      "[42]\tvalidation_0-mlogloss:2.27405\n",
      "[43]\tvalidation_0-mlogloss:2.26398\n",
      "[44]\tvalidation_0-mlogloss:2.25592\n",
      "[45]\tvalidation_0-mlogloss:2.24363\n",
      "[46]\tvalidation_0-mlogloss:2.23521\n",
      "[47]\tvalidation_0-mlogloss:2.22315\n",
      "[48]\tvalidation_0-mlogloss:2.21476\n",
      "[49]\tvalidation_0-mlogloss:2.20717\n",
      "[50]\tvalidation_0-mlogloss:2.19725\n",
      "[51]\tvalidation_0-mlogloss:2.18879\n",
      "[52]\tvalidation_0-mlogloss:2.17935\n",
      "[53]\tvalidation_0-mlogloss:2.16818\n",
      "[54]\tvalidation_0-mlogloss:2.15767\n",
      "[55]\tvalidation_0-mlogloss:2.14869\n",
      "[56]\tvalidation_0-mlogloss:2.14089\n",
      "[57]\tvalidation_0-mlogloss:2.13232\n",
      "[58]\tvalidation_0-mlogloss:2.12390\n",
      "[59]\tvalidation_0-mlogloss:2.11543\n",
      "[60]\tvalidation_0-mlogloss:2.10631\n",
      "[61]\tvalidation_0-mlogloss:2.09958\n",
      "[62]\tvalidation_0-mlogloss:2.09382\n",
      "[63]\tvalidation_0-mlogloss:2.08561\n",
      "[64]\tvalidation_0-mlogloss:2.07971\n",
      "[65]\tvalidation_0-mlogloss:2.07284\n",
      "[66]\tvalidation_0-mlogloss:2.06544\n",
      "[67]\tvalidation_0-mlogloss:2.05823\n",
      "[68]\tvalidation_0-mlogloss:2.05027\n",
      "[69]\tvalidation_0-mlogloss:2.04323\n",
      "[70]\tvalidation_0-mlogloss:2.03750\n",
      "[71]\tvalidation_0-mlogloss:2.02990\n",
      "[72]\tvalidation_0-mlogloss:2.02338\n",
      "[73]\tvalidation_0-mlogloss:2.01782\n",
      "[74]\tvalidation_0-mlogloss:2.01318\n",
      "[75]\tvalidation_0-mlogloss:2.00789\n",
      "[76]\tvalidation_0-mlogloss:2.00230\n",
      "[77]\tvalidation_0-mlogloss:1.99782\n",
      "[78]\tvalidation_0-mlogloss:1.99244\n",
      "[79]\tvalidation_0-mlogloss:1.98598\n",
      "[80]\tvalidation_0-mlogloss:1.98026\n",
      "[81]\tvalidation_0-mlogloss:1.97392\n",
      "[82]\tvalidation_0-mlogloss:1.96990\n",
      "[83]\tvalidation_0-mlogloss:1.96306\n",
      "[84]\tvalidation_0-mlogloss:1.95808\n",
      "[85]\tvalidation_0-mlogloss:1.95207\n",
      "[86]\tvalidation_0-mlogloss:1.94693\n",
      "[87]\tvalidation_0-mlogloss:1.94218\n",
      "[88]\tvalidation_0-mlogloss:1.93739\n",
      "[89]\tvalidation_0-mlogloss:1.93258\n",
      "[90]\tvalidation_0-mlogloss:1.92793\n",
      "[91]\tvalidation_0-mlogloss:1.92484\n",
      "[92]\tvalidation_0-mlogloss:1.91993\n",
      "[93]\tvalidation_0-mlogloss:1.91446\n",
      "[94]\tvalidation_0-mlogloss:1.90829\n",
      "[95]\tvalidation_0-mlogloss:1.90260\n",
      "[96]\tvalidation_0-mlogloss:1.89877\n",
      "[97]\tvalidation_0-mlogloss:1.89584\n",
      "[98]\tvalidation_0-mlogloss:1.89177\n",
      "[99]\tvalidation_0-mlogloss:1.88800\n",
      "[100]\tvalidation_0-mlogloss:1.88324\n",
      "[101]\tvalidation_0-mlogloss:1.87902\n",
      "[102]\tvalidation_0-mlogloss:1.87579\n",
      "[103]\tvalidation_0-mlogloss:1.87125\n",
      "[104]\tvalidation_0-mlogloss:1.86819\n",
      "[105]\tvalidation_0-mlogloss:1.86459\n",
      "[106]\tvalidation_0-mlogloss:1.86028\n",
      "[107]\tvalidation_0-mlogloss:1.85708\n",
      "[108]\tvalidation_0-mlogloss:1.85247\n",
      "[109]\tvalidation_0-mlogloss:1.84752\n",
      "[110]\tvalidation_0-mlogloss:1.84440\n",
      "[111]\tvalidation_0-mlogloss:1.84027\n",
      "[112]\tvalidation_0-mlogloss:1.83739\n",
      "[113]\tvalidation_0-mlogloss:1.83408\n",
      "[114]\tvalidation_0-mlogloss:1.83066\n",
      "[115]\tvalidation_0-mlogloss:1.82817\n",
      "[116]\tvalidation_0-mlogloss:1.82563\n",
      "[117]\tvalidation_0-mlogloss:1.82242\n",
      "[118]\tvalidation_0-mlogloss:1.81902\n",
      "[119]\tvalidation_0-mlogloss:1.81705\n",
      "[120]\tvalidation_0-mlogloss:1.81375\n",
      "[121]\tvalidation_0-mlogloss:1.81025\n",
      "[122]\tvalidation_0-mlogloss:1.80772\n",
      "[123]\tvalidation_0-mlogloss:1.80452\n",
      "[124]\tvalidation_0-mlogloss:1.80099\n",
      "[125]\tvalidation_0-mlogloss:1.79741\n",
      "[126]\tvalidation_0-mlogloss:1.79425\n",
      "[127]\tvalidation_0-mlogloss:1.79172\n",
      "[128]\tvalidation_0-mlogloss:1.79090\n",
      "[129]\tvalidation_0-mlogloss:1.78801\n",
      "[130]\tvalidation_0-mlogloss:1.78510\n",
      "[131]\tvalidation_0-mlogloss:1.78227\n",
      "[132]\tvalidation_0-mlogloss:1.77801\n",
      "[133]\tvalidation_0-mlogloss:1.77396\n",
      "[134]\tvalidation_0-mlogloss:1.77106\n",
      "[135]\tvalidation_0-mlogloss:1.76789\n",
      "[136]\tvalidation_0-mlogloss:1.76500\n",
      "[137]\tvalidation_0-mlogloss:1.76094\n",
      "[138]\tvalidation_0-mlogloss:1.75843\n",
      "[139]\tvalidation_0-mlogloss:1.75551\n",
      "[140]\tvalidation_0-mlogloss:1.75355\n",
      "[141]\tvalidation_0-mlogloss:1.75127\n",
      "[142]\tvalidation_0-mlogloss:1.75058\n",
      "[143]\tvalidation_0-mlogloss:1.74865\n",
      "[144]\tvalidation_0-mlogloss:1.74690\n",
      "[145]\tvalidation_0-mlogloss:1.74350\n",
      "[146]\tvalidation_0-mlogloss:1.74099\n",
      "[147]\tvalidation_0-mlogloss:1.73800\n",
      "[148]\tvalidation_0-mlogloss:1.73593\n",
      "[149]\tvalidation_0-mlogloss:1.73307\n",
      "[150]\tvalidation_0-mlogloss:1.73096\n",
      "[151]\tvalidation_0-mlogloss:1.72912\n",
      "[152]\tvalidation_0-mlogloss:1.72907\n",
      "[153]\tvalidation_0-mlogloss:1.72774\n",
      "[154]\tvalidation_0-mlogloss:1.72536\n",
      "[155]\tvalidation_0-mlogloss:1.72473\n",
      "[156]\tvalidation_0-mlogloss:1.72440\n",
      "[157]\tvalidation_0-mlogloss:1.72305\n",
      "[158]\tvalidation_0-mlogloss:1.72065\n",
      "[159]\tvalidation_0-mlogloss:1.71987\n",
      "[160]\tvalidation_0-mlogloss:1.71848\n",
      "[161]\tvalidation_0-mlogloss:1.71656\n",
      "[162]\tvalidation_0-mlogloss:1.71474\n",
      "[163]\tvalidation_0-mlogloss:1.71252\n",
      "[164]\tvalidation_0-mlogloss:1.71086\n",
      "[165]\tvalidation_0-mlogloss:1.70871\n",
      "[166]\tvalidation_0-mlogloss:1.70670\n",
      "[167]\tvalidation_0-mlogloss:1.70476\n",
      "[168]\tvalidation_0-mlogloss:1.70232\n",
      "[169]\tvalidation_0-mlogloss:1.70023\n",
      "[170]\tvalidation_0-mlogloss:1.69799\n",
      "[171]\tvalidation_0-mlogloss:1.69592\n",
      "[172]\tvalidation_0-mlogloss:1.69521\n",
      "[173]\tvalidation_0-mlogloss:1.69422\n",
      "[174]\tvalidation_0-mlogloss:1.69262\n",
      "[175]\tvalidation_0-mlogloss:1.69117\n",
      "[176]\tvalidation_0-mlogloss:1.69025\n",
      "[177]\tvalidation_0-mlogloss:1.68809\n",
      "[178]\tvalidation_0-mlogloss:1.68589\n",
      "[179]\tvalidation_0-mlogloss:1.68416\n",
      "[180]\tvalidation_0-mlogloss:1.68315\n",
      "[181]\tvalidation_0-mlogloss:1.68385\n",
      "[182]\tvalidation_0-mlogloss:1.68191\n",
      "[183]\tvalidation_0-mlogloss:1.68056\n",
      "[184]\tvalidation_0-mlogloss:1.67843\n",
      "[185]\tvalidation_0-mlogloss:1.67659\n",
      "[186]\tvalidation_0-mlogloss:1.67490\n",
      "[187]\tvalidation_0-mlogloss:1.67299\n",
      "[188]\tvalidation_0-mlogloss:1.67286\n",
      "[189]\tvalidation_0-mlogloss:1.67181\n",
      "[190]\tvalidation_0-mlogloss:1.67005\n",
      "[191]\tvalidation_0-mlogloss:1.66813\n",
      "[192]\tvalidation_0-mlogloss:1.66668\n",
      "[193]\tvalidation_0-mlogloss:1.66530\n",
      "[194]\tvalidation_0-mlogloss:1.66440\n",
      "[195]\tvalidation_0-mlogloss:1.66370\n",
      "[196]\tvalidation_0-mlogloss:1.66225\n",
      "[197]\tvalidation_0-mlogloss:1.65990\n",
      "[198]\tvalidation_0-mlogloss:1.65823\n",
      "[199]\tvalidation_0-mlogloss:1.65642\n",
      "[200]\tvalidation_0-mlogloss:1.65531\n",
      "[201]\tvalidation_0-mlogloss:1.65474\n",
      "[202]\tvalidation_0-mlogloss:1.65323\n",
      "[203]\tvalidation_0-mlogloss:1.65196\n",
      "[204]\tvalidation_0-mlogloss:1.65018\n",
      "[205]\tvalidation_0-mlogloss:1.64943\n",
      "[206]\tvalidation_0-mlogloss:1.64811\n",
      "[207]\tvalidation_0-mlogloss:1.64717\n",
      "[208]\tvalidation_0-mlogloss:1.64675\n",
      "[209]\tvalidation_0-mlogloss:1.64589\n",
      "[210]\tvalidation_0-mlogloss:1.64449\n",
      "[211]\tvalidation_0-mlogloss:1.64333\n",
      "[212]\tvalidation_0-mlogloss:1.64231\n",
      "[213]\tvalidation_0-mlogloss:1.64272\n",
      "[214]\tvalidation_0-mlogloss:1.64122\n",
      "[215]\tvalidation_0-mlogloss:1.64024\n",
      "[216]\tvalidation_0-mlogloss:1.63895\n",
      "[217]\tvalidation_0-mlogloss:1.63832\n",
      "[218]\tvalidation_0-mlogloss:1.63707\n",
      "[219]\tvalidation_0-mlogloss:1.63570\n",
      "[220]\tvalidation_0-mlogloss:1.63420\n",
      "[221]\tvalidation_0-mlogloss:1.63327\n",
      "[222]\tvalidation_0-mlogloss:1.63303\n",
      "[223]\tvalidation_0-mlogloss:1.63157\n",
      "[224]\tvalidation_0-mlogloss:1.63059\n",
      "[225]\tvalidation_0-mlogloss:1.62945\n",
      "[226]\tvalidation_0-mlogloss:1.62839\n",
      "[227]\tvalidation_0-mlogloss:1.62835\n",
      "[228]\tvalidation_0-mlogloss:1.62736\n",
      "[229]\tvalidation_0-mlogloss:1.62610\n",
      "[230]\tvalidation_0-mlogloss:1.62521\n",
      "[231]\tvalidation_0-mlogloss:1.62408\n",
      "[232]\tvalidation_0-mlogloss:1.62379\n",
      "[233]\tvalidation_0-mlogloss:1.62214\n",
      "[234]\tvalidation_0-mlogloss:1.62171\n",
      "[235]\tvalidation_0-mlogloss:1.62033\n",
      "[236]\tvalidation_0-mlogloss:1.61969\n",
      "[237]\tvalidation_0-mlogloss:1.61894\n",
      "[238]\tvalidation_0-mlogloss:1.61901\n",
      "[239]\tvalidation_0-mlogloss:1.61831\n",
      "[240]\tvalidation_0-mlogloss:1.61769\n",
      "[241]\tvalidation_0-mlogloss:1.61716\n",
      "[242]\tvalidation_0-mlogloss:1.61639\n",
      "[243]\tvalidation_0-mlogloss:1.61564\n",
      "[244]\tvalidation_0-mlogloss:1.61480\n",
      "[245]\tvalidation_0-mlogloss:1.61372\n",
      "[246]\tvalidation_0-mlogloss:1.61349\n",
      "[247]\tvalidation_0-mlogloss:1.61254\n",
      "[248]\tvalidation_0-mlogloss:1.61228\n",
      "[249]\tvalidation_0-mlogloss:1.61076\n",
      "[250]\tvalidation_0-mlogloss:1.60987\n",
      "[251]\tvalidation_0-mlogloss:1.60867\n",
      "[252]\tvalidation_0-mlogloss:1.60769\n",
      "[253]\tvalidation_0-mlogloss:1.60678\n",
      "[254]\tvalidation_0-mlogloss:1.60586\n",
      "[255]\tvalidation_0-mlogloss:1.60468\n",
      "[256]\tvalidation_0-mlogloss:1.60411\n",
      "[257]\tvalidation_0-mlogloss:1.60404\n",
      "[258]\tvalidation_0-mlogloss:1.60298\n",
      "[259]\tvalidation_0-mlogloss:1.60251\n",
      "[260]\tvalidation_0-mlogloss:1.60230\n",
      "[261]\tvalidation_0-mlogloss:1.60248\n",
      "[262]\tvalidation_0-mlogloss:1.60186\n",
      "[263]\tvalidation_0-mlogloss:1.60068\n",
      "[264]\tvalidation_0-mlogloss:1.59989\n",
      "[265]\tvalidation_0-mlogloss:1.59993\n",
      "[266]\tvalidation_0-mlogloss:1.59970\n",
      "[267]\tvalidation_0-mlogloss:1.59898\n",
      "[268]\tvalidation_0-mlogloss:1.59785\n",
      "[269]\tvalidation_0-mlogloss:1.59716\n",
      "[270]\tvalidation_0-mlogloss:1.59625\n",
      "[271]\tvalidation_0-mlogloss:1.59644\n",
      "[272]\tvalidation_0-mlogloss:1.59612\n",
      "[273]\tvalidation_0-mlogloss:1.59543\n",
      "[274]\tvalidation_0-mlogloss:1.59508\n",
      "[275]\tvalidation_0-mlogloss:1.59426\n",
      "[276]\tvalidation_0-mlogloss:1.59423\n",
      "[277]\tvalidation_0-mlogloss:1.59317\n",
      "[278]\tvalidation_0-mlogloss:1.59362\n",
      "[279]\tvalidation_0-mlogloss:1.59306\n",
      "[280]\tvalidation_0-mlogloss:1.59278\n",
      "[281]\tvalidation_0-mlogloss:1.59247\n",
      "[282]\tvalidation_0-mlogloss:1.59178\n",
      "[283]\tvalidation_0-mlogloss:1.59139\n",
      "[284]\tvalidation_0-mlogloss:1.59128\n",
      "[285]\tvalidation_0-mlogloss:1.59034\n",
      "[286]\tvalidation_0-mlogloss:1.58980\n",
      "[287]\tvalidation_0-mlogloss:1.59019\n",
      "[288]\tvalidation_0-mlogloss:1.59020\n",
      "[289]\tvalidation_0-mlogloss:1.58932\n",
      "[290]\tvalidation_0-mlogloss:1.58942\n",
      "[291]\tvalidation_0-mlogloss:1.58900\n",
      "[292]\tvalidation_0-mlogloss:1.58789\n",
      "[293]\tvalidation_0-mlogloss:1.58672\n",
      "[294]\tvalidation_0-mlogloss:1.58576\n",
      "[295]\tvalidation_0-mlogloss:1.58623\n",
      "[296]\tvalidation_0-mlogloss:1.58578\n",
      "[297]\tvalidation_0-mlogloss:1.58519\n",
      "[298]\tvalidation_0-mlogloss:1.58449\n",
      "[299]\tvalidation_0-mlogloss:1.58413\n",
      "[300]\tvalidation_0-mlogloss:1.58296\n",
      "[301]\tvalidation_0-mlogloss:1.58322\n",
      "[302]\tvalidation_0-mlogloss:1.58264\n",
      "[303]\tvalidation_0-mlogloss:1.58143\n",
      "[304]\tvalidation_0-mlogloss:1.58085\n",
      "[305]\tvalidation_0-mlogloss:1.58033\n",
      "[306]\tvalidation_0-mlogloss:1.57959\n",
      "[307]\tvalidation_0-mlogloss:1.57937\n",
      "[308]\tvalidation_0-mlogloss:1.57976\n",
      "[309]\tvalidation_0-mlogloss:1.58023\n",
      "[310]\tvalidation_0-mlogloss:1.57982\n",
      "[311]\tvalidation_0-mlogloss:1.57971\n",
      "[312]\tvalidation_0-mlogloss:1.57927\n",
      "[313]\tvalidation_0-mlogloss:1.57917\n",
      "[314]\tvalidation_0-mlogloss:1.57875\n",
      "[315]\tvalidation_0-mlogloss:1.57973\n",
      "[316]\tvalidation_0-mlogloss:1.57955\n",
      "[317]\tvalidation_0-mlogloss:1.57973\n",
      "[318]\tvalidation_0-mlogloss:1.57940\n",
      "[319]\tvalidation_0-mlogloss:1.57872\n",
      "[320]\tvalidation_0-mlogloss:1.57857\n",
      "[321]\tvalidation_0-mlogloss:1.57865\n",
      "[322]\tvalidation_0-mlogloss:1.57919\n",
      "[323]\tvalidation_0-mlogloss:1.57896\n",
      "[324]\tvalidation_0-mlogloss:1.57896\n",
      "[325]\tvalidation_0-mlogloss:1.57858\n",
      "[326]\tvalidation_0-mlogloss:1.57896\n",
      "[327]\tvalidation_0-mlogloss:1.57937\n",
      "[328]\tvalidation_0-mlogloss:1.57977\n",
      "[329]\tvalidation_0-mlogloss:1.57892\n",
      "[330]\tvalidation_0-mlogloss:1.57849\n",
      "[331]\tvalidation_0-mlogloss:1.57841\n",
      "[332]\tvalidation_0-mlogloss:1.57906\n",
      "[333]\tvalidation_0-mlogloss:1.57845\n",
      "[334]\tvalidation_0-mlogloss:1.57842\n",
      "[335]\tvalidation_0-mlogloss:1.57952\n",
      "[336]\tvalidation_0-mlogloss:1.57802\n",
      "[337]\tvalidation_0-mlogloss:1.57745\n",
      "[338]\tvalidation_0-mlogloss:1.57655\n",
      "[339]\tvalidation_0-mlogloss:1.57640\n",
      "[340]\tvalidation_0-mlogloss:1.57590\n",
      "[341]\tvalidation_0-mlogloss:1.57536\n",
      "[342]\tvalidation_0-mlogloss:1.57500\n",
      "[343]\tvalidation_0-mlogloss:1.57476\n",
      "[344]\tvalidation_0-mlogloss:1.57461\n",
      "[345]\tvalidation_0-mlogloss:1.57441\n",
      "[346]\tvalidation_0-mlogloss:1.57490\n",
      "[347]\tvalidation_0-mlogloss:1.57468\n",
      "[348]\tvalidation_0-mlogloss:1.57470\n",
      "[349]\tvalidation_0-mlogloss:1.57486\n",
      "[350]\tvalidation_0-mlogloss:1.57447\n",
      "[351]\tvalidation_0-mlogloss:1.57413\n",
      "[352]\tvalidation_0-mlogloss:1.57311\n",
      "[353]\tvalidation_0-mlogloss:1.57310\n",
      "[354]\tvalidation_0-mlogloss:1.57311\n",
      "[355]\tvalidation_0-mlogloss:1.57324\n",
      "[356]\tvalidation_0-mlogloss:1.57362\n",
      "[357]\tvalidation_0-mlogloss:1.57263\n",
      "[358]\tvalidation_0-mlogloss:1.57245\n",
      "[359]\tvalidation_0-mlogloss:1.57240\n",
      "[360]\tvalidation_0-mlogloss:1.57246\n",
      "[361]\tvalidation_0-mlogloss:1.57146\n",
      "[362]\tvalidation_0-mlogloss:1.57119\n",
      "[363]\tvalidation_0-mlogloss:1.57110\n",
      "[364]\tvalidation_0-mlogloss:1.57153\n",
      "[365]\tvalidation_0-mlogloss:1.57102\n",
      "[366]\tvalidation_0-mlogloss:1.57165\n",
      "[367]\tvalidation_0-mlogloss:1.57118\n",
      "[368]\tvalidation_0-mlogloss:1.57066\n",
      "[369]\tvalidation_0-mlogloss:1.57058\n",
      "[370]\tvalidation_0-mlogloss:1.57110\n",
      "[371]\tvalidation_0-mlogloss:1.57157\n",
      "[372]\tvalidation_0-mlogloss:1.57103\n",
      "[373]\tvalidation_0-mlogloss:1.57087\n",
      "[374]\tvalidation_0-mlogloss:1.57067\n",
      "[375]\tvalidation_0-mlogloss:1.57028\n",
      "[376]\tvalidation_0-mlogloss:1.57075\n",
      "[377]\tvalidation_0-mlogloss:1.57018\n",
      "[378]\tvalidation_0-mlogloss:1.56954\n",
      "[379]\tvalidation_0-mlogloss:1.56922\n",
      "[380]\tvalidation_0-mlogloss:1.56901\n",
      "[381]\tvalidation_0-mlogloss:1.56874\n",
      "[382]\tvalidation_0-mlogloss:1.56804\n",
      "[383]\tvalidation_0-mlogloss:1.56868\n",
      "[384]\tvalidation_0-mlogloss:1.56915\n",
      "[385]\tvalidation_0-mlogloss:1.56902\n",
      "[386]\tvalidation_0-mlogloss:1.56950\n",
      "[387]\tvalidation_0-mlogloss:1.57012\n",
      "[388]\tvalidation_0-mlogloss:1.57068\n",
      "[389]\tvalidation_0-mlogloss:1.57104\n",
      "[390]\tvalidation_0-mlogloss:1.57125\n",
      "[391]\tvalidation_0-mlogloss:1.57085\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.62      0.57      1499\n",
      "           1       0.28      0.22      0.24      1499\n",
      "           2       0.74      0.79      0.77      1499\n",
      "           3       0.77      0.38      0.51      1499\n",
      "           4       0.92      0.94      0.93      1499\n",
      "           5       0.91      0.89      0.90      1499\n",
      "           6       0.79      0.73      0.76      1499\n",
      "           7       0.62      0.36      0.45      1499\n",
      "           8       0.72      0.83      0.77      1499\n",
      "           9       0.91      0.69      0.78      1499\n",
      "          10       0.82      0.68      0.74      1499\n",
      "          11       0.95      0.91      0.93      1499\n",
      "          12       0.70      0.79      0.74      1499\n",
      "          13       0.58      0.57      0.57      1499\n",
      "          14       0.50      0.63      0.56      1499\n",
      "          15       0.70      0.68      0.69      1499\n",
      "          16       0.55      0.36      0.43      1499\n",
      "          17       0.65      0.84      0.74      1499\n",
      "          18       0.69      0.83      0.75      1499\n",
      "          19       0.37      0.41      0.39      1499\n",
      "          20       0.95      0.97      0.96      1499\n",
      "          21       0.67      0.67      0.67      1499\n",
      "          22       0.58      0.71      0.64      1499\n",
      "          23       0.95      0.94      0.94      1499\n",
      "          24       0.82      0.61      0.70      1499\n",
      "          25       0.74      0.48      0.58      1499\n",
      "          26       0.62      0.31      0.41      1499\n",
      "          27       0.36      0.24      0.29      1499\n",
      "          28       0.58      0.75      0.66      1499\n",
      "          29       0.77      0.83      0.80      1499\n",
      "          30       0.50      0.78      0.61      1499\n",
      "          31       0.38      0.65      0.48      1499\n",
      "          32       0.19      0.12      0.15      1499\n",
      "          33       0.65      0.74      0.69      1499\n",
      "          34       0.95      0.79      0.86      1499\n",
      "          35       0.65      0.88      0.75      1499\n",
      "          36       0.77      0.82      0.79      1499\n",
      "          37       0.56      0.85      0.67      1499\n",
      "          38       0.72      0.53      0.61      1499\n",
      "          39       0.72      0.51      0.60      1499\n",
      "          40       0.86      0.54      0.66      1499\n",
      "          41       0.71      0.67      0.69      1499\n",
      "          42       0.49      0.31      0.38      1499\n",
      "          43       0.72      0.69      0.71      1499\n",
      "          44       0.81      0.87      0.84      1499\n",
      "          45       0.54      0.45      0.50      1499\n",
      "          46       0.26      0.31      0.28      1499\n",
      "          47       0.63      0.31      0.41      1499\n",
      "          48       0.61      0.82      0.70      1499\n",
      "          49       0.62      0.61      0.61      1499\n",
      "          50       0.75      0.66      0.70      1499\n",
      "          51       0.64      0.46      0.54      1499\n",
      "          52       0.64      0.74      0.69      1499\n",
      "          53       0.58      0.52      0.55      1499\n",
      "          54       0.73      0.85      0.78      1499\n",
      "          55       0.59      0.73      0.65      1499\n",
      "          56       0.66      0.60      0.63      1499\n",
      "          57       0.81      0.88      0.85      1499\n",
      "          58       0.96      0.80      0.88      1499\n",
      "          59       0.67      0.52      0.59      1499\n",
      "          60       0.69      0.78      0.73      1499\n",
      "          61       0.90      0.88      0.89      1499\n",
      "          62       0.75      0.84      0.79      1499\n",
      "          63       0.41      0.41      0.41      1499\n",
      "          64       0.55      0.53      0.54      1499\n",
      "          65       0.68      0.67      0.68      1499\n",
      "          66       0.81      0.83      0.82      1499\n",
      "          67       0.99      0.71      0.83      1499\n",
      "          68       0.67      0.72      0.70      1499\n",
      "          69       0.38      0.54      0.45      1499\n",
      "          70       0.71      0.66      0.68      1499\n",
      "          71       0.56      0.51      0.53      1499\n",
      "          72       0.71      0.83      0.77      1499\n",
      "          73       0.67      0.56      0.61      1499\n",
      "          74       0.28      0.65      0.39      1499\n",
      "\n",
      "    accuracy                           0.65    112425\n",
      "   macro avg       0.67      0.65      0.65    112425\n",
      "weighted avg       0.67      0.65      0.65    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300aa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baab7fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 27s 955us/step - loss: 1.4301 - val_loss: 2.7131\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 26s 929us/step - loss: 0.6283 - val_loss: 2.5845\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 27s 944us/step - loss: 0.4776 - val_loss: 2.5206\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 29s 1ms/step - loss: 0.4093 - val_loss: 2.6662\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 27s 960us/step - loss: 0.3673 - val_loss: 2.7442\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 27s 950us/step - loss: 0.3391 - val_loss: 2.6514\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 26s 938us/step - loss: 0.3166 - val_loss: 2.7831\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 27s 947us/step - loss: 0.2994 - val_loss: 2.6584\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 26s 937us/step - loss: 0.2848 - val_loss: 2.7271\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 27s 953us/step - loss: 0.2732 - val_loss: 2.6304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x317efe7a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 1s 383us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.57      0.64      1499\n",
      "           1       0.31      0.14      0.19      1499\n",
      "           2       0.62      0.74      0.67      1499\n",
      "           3       0.72      0.46      0.56      1499\n",
      "           4       0.90      0.93      0.92      1499\n",
      "           5       0.85      0.88      0.86      1499\n",
      "           6       0.96      0.72      0.82      1499\n",
      "           7       0.82      0.53      0.64      1499\n",
      "           8       0.68      0.85      0.75      1499\n",
      "           9       0.71      0.73      0.72      1499\n",
      "          10       0.61      0.94      0.74      1499\n",
      "          11       0.81      0.95      0.87      1499\n",
      "          12       0.80      0.76      0.78      1499\n",
      "          13       0.57      0.53      0.55      1499\n",
      "          14       0.52      0.66      0.58      1499\n",
      "          15       0.76      0.77      0.77      1499\n",
      "          16       0.63      0.38      0.48      1499\n",
      "          17       0.73      0.58      0.65      1499\n",
      "          18       0.78      0.87      0.82      1499\n",
      "          19       0.30      0.57      0.40      1499\n",
      "          20       0.83      0.97      0.89      1499\n",
      "          21       0.65      0.61      0.63      1499\n",
      "          22       0.75      0.73      0.74      1499\n",
      "          23       0.96      0.94      0.95      1499\n",
      "          24       0.74      0.60      0.66      1499\n",
      "          25       0.95      0.21      0.35      1499\n",
      "          26       0.32      0.19      0.24      1499\n",
      "          27       0.40      0.42      0.41      1499\n",
      "          28       0.55      0.94      0.69      1499\n",
      "          29       0.78      0.76      0.77      1499\n",
      "          30       0.72      0.74      0.73      1499\n",
      "          31       0.56      0.66      0.61      1499\n",
      "          32       0.48      0.16      0.24      1499\n",
      "          33       0.79      0.57      0.66      1499\n",
      "          34       1.00      0.65      0.79      1499\n",
      "          35       0.86      0.76      0.81      1499\n",
      "          36       0.86      0.64      0.74      1499\n",
      "          37       0.63      0.87      0.73      1499\n",
      "          38       0.51      0.50      0.51      1499\n",
      "          39       0.58      0.51      0.55      1499\n",
      "          40       0.85      0.29      0.43      1499\n",
      "          41       0.70      0.65      0.67      1499\n",
      "          42       0.41      0.61      0.49      1499\n",
      "          43       0.84      0.68      0.75      1499\n",
      "          44       0.88      0.87      0.87      1499\n",
      "          45       0.40      0.51      0.45      1499\n",
      "          46       0.18      0.10      0.13      1499\n",
      "          47       0.90      0.44      0.59      1499\n",
      "          48       0.55      0.90      0.68      1499\n",
      "          49       0.57      0.69      0.63      1499\n",
      "          50       0.64      0.88      0.74      1499\n",
      "          51       0.70      0.31      0.43      1499\n",
      "          52       0.70      0.75      0.73      1499\n",
      "          53       0.63      0.77      0.69      1499\n",
      "          54       0.86      0.76      0.81      1499\n",
      "          55       0.68      0.76      0.72      1499\n",
      "          56       0.51      0.54      0.52      1499\n",
      "          57       0.89      0.84      0.87      1499\n",
      "          58       0.85      0.85      0.85      1499\n",
      "          59       0.54      0.28      0.37      1499\n",
      "          60       0.80      0.66      0.72      1499\n",
      "          61       0.89      0.71      0.79      1499\n",
      "          62       0.80      0.84      0.82      1499\n",
      "          63       0.43      0.30      0.35      1499\n",
      "          64       0.52      0.42      0.47      1499\n",
      "          65       0.75      0.73      0.74      1499\n",
      "          66       0.86      0.84      0.85      1499\n",
      "          67       0.86      0.75      0.80      1499\n",
      "          68       0.66      0.83      0.74      1499\n",
      "          69       0.33      0.63      0.44      1499\n",
      "          70       0.64      0.62      0.63      1499\n",
      "          71       0.37      0.36      0.37      1499\n",
      "          72       0.40      0.86      0.55      1499\n",
      "          73       0.79      0.54      0.64      1499\n",
      "          74       0.27      0.66      0.39      1499\n",
      "\n",
      "    accuracy                           0.64    112425\n",
      "   macro avg       0.67      0.64      0.64    112425\n",
      "weighted avg       0.67      0.64      0.64    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.54983\n",
      "[1]\tvalidation_0-mlogloss:3.25889\n",
      "[2]\tvalidation_0-mlogloss:3.06115\n",
      "[3]\tvalidation_0-mlogloss:2.92295\n",
      "[4]\tvalidation_0-mlogloss:2.80457\n",
      "[5]\tvalidation_0-mlogloss:2.70221\n",
      "[6]\tvalidation_0-mlogloss:2.62148\n",
      "[7]\tvalidation_0-mlogloss:2.54302\n",
      "[8]\tvalidation_0-mlogloss:2.47749\n",
      "[9]\tvalidation_0-mlogloss:2.41887\n",
      "[10]\tvalidation_0-mlogloss:2.36722\n",
      "[11]\tvalidation_0-mlogloss:2.32322\n",
      "[12]\tvalidation_0-mlogloss:2.27935\n",
      "[13]\tvalidation_0-mlogloss:2.23690\n",
      "[14]\tvalidation_0-mlogloss:2.19956\n",
      "[15]\tvalidation_0-mlogloss:2.16809\n",
      "[16]\tvalidation_0-mlogloss:2.13115\n",
      "[17]\tvalidation_0-mlogloss:2.10099\n",
      "[18]\tvalidation_0-mlogloss:2.06963\n",
      "[19]\tvalidation_0-mlogloss:2.04097\n",
      "[20]\tvalidation_0-mlogloss:2.01682\n",
      "[21]\tvalidation_0-mlogloss:1.99170\n",
      "[22]\tvalidation_0-mlogloss:1.96983\n",
      "[23]\tvalidation_0-mlogloss:1.94550\n",
      "[24]\tvalidation_0-mlogloss:1.92654\n",
      "[25]\tvalidation_0-mlogloss:1.90653\n",
      "[26]\tvalidation_0-mlogloss:1.88677\n",
      "[27]\tvalidation_0-mlogloss:1.87071\n",
      "[28]\tvalidation_0-mlogloss:1.85053\n",
      "[29]\tvalidation_0-mlogloss:1.83259\n",
      "[30]\tvalidation_0-mlogloss:1.81577\n",
      "[31]\tvalidation_0-mlogloss:1.79615\n",
      "[32]\tvalidation_0-mlogloss:1.78075\n",
      "[33]\tvalidation_0-mlogloss:1.76479\n",
      "[34]\tvalidation_0-mlogloss:1.74835\n",
      "[35]\tvalidation_0-mlogloss:1.73717\n",
      "[36]\tvalidation_0-mlogloss:1.72072\n",
      "[37]\tvalidation_0-mlogloss:1.70699\n",
      "[38]\tvalidation_0-mlogloss:1.69363\n",
      "[39]\tvalidation_0-mlogloss:1.68223\n",
      "[40]\tvalidation_0-mlogloss:1.66735\n",
      "[41]\tvalidation_0-mlogloss:1.65485\n",
      "[42]\tvalidation_0-mlogloss:1.64425\n",
      "[43]\tvalidation_0-mlogloss:1.63218\n",
      "[44]\tvalidation_0-mlogloss:1.62155\n",
      "[45]\tvalidation_0-mlogloss:1.61074\n",
      "[46]\tvalidation_0-mlogloss:1.60264\n",
      "[47]\tvalidation_0-mlogloss:1.59302\n",
      "[48]\tvalidation_0-mlogloss:1.58231\n",
      "[49]\tvalidation_0-mlogloss:1.57192\n",
      "[50]\tvalidation_0-mlogloss:1.56080\n",
      "[51]\tvalidation_0-mlogloss:1.55076\n",
      "[52]\tvalidation_0-mlogloss:1.54089\n",
      "[53]\tvalidation_0-mlogloss:1.53156\n",
      "[54]\tvalidation_0-mlogloss:1.52161\n",
      "[55]\tvalidation_0-mlogloss:1.51522\n",
      "[56]\tvalidation_0-mlogloss:1.50445\n",
      "[57]\tvalidation_0-mlogloss:1.49629\n",
      "[58]\tvalidation_0-mlogloss:1.48750\n",
      "[59]\tvalidation_0-mlogloss:1.47971\n",
      "[60]\tvalidation_0-mlogloss:1.47141\n",
      "[61]\tvalidation_0-mlogloss:1.46317\n",
      "[62]\tvalidation_0-mlogloss:1.45422\n",
      "[63]\tvalidation_0-mlogloss:1.44588\n",
      "[64]\tvalidation_0-mlogloss:1.43855\n",
      "[65]\tvalidation_0-mlogloss:1.42939\n",
      "[66]\tvalidation_0-mlogloss:1.42156\n",
      "[67]\tvalidation_0-mlogloss:1.41417\n",
      "[68]\tvalidation_0-mlogloss:1.40676\n",
      "[69]\tvalidation_0-mlogloss:1.40102\n",
      "[70]\tvalidation_0-mlogloss:1.39302\n",
      "[71]\tvalidation_0-mlogloss:1.38522\n",
      "[72]\tvalidation_0-mlogloss:1.37896\n",
      "[73]\tvalidation_0-mlogloss:1.37139\n",
      "[74]\tvalidation_0-mlogloss:1.36394\n",
      "[75]\tvalidation_0-mlogloss:1.35714\n",
      "[76]\tvalidation_0-mlogloss:1.35186\n",
      "[77]\tvalidation_0-mlogloss:1.34726\n",
      "[78]\tvalidation_0-mlogloss:1.34260\n",
      "[79]\tvalidation_0-mlogloss:1.33658\n",
      "[80]\tvalidation_0-mlogloss:1.33140\n",
      "[81]\tvalidation_0-mlogloss:1.32624\n",
      "[82]\tvalidation_0-mlogloss:1.32012\n",
      "[83]\tvalidation_0-mlogloss:1.31363\n",
      "[84]\tvalidation_0-mlogloss:1.30849\n",
      "[85]\tvalidation_0-mlogloss:1.30290\n",
      "[86]\tvalidation_0-mlogloss:1.29755\n",
      "[87]\tvalidation_0-mlogloss:1.29355\n",
      "[88]\tvalidation_0-mlogloss:1.28911\n",
      "[89]\tvalidation_0-mlogloss:1.28515\n",
      "[90]\tvalidation_0-mlogloss:1.27936\n",
      "[91]\tvalidation_0-mlogloss:1.27489\n",
      "[92]\tvalidation_0-mlogloss:1.26962\n",
      "[93]\tvalidation_0-mlogloss:1.26408\n",
      "[94]\tvalidation_0-mlogloss:1.25932\n",
      "[95]\tvalidation_0-mlogloss:1.25435\n",
      "[96]\tvalidation_0-mlogloss:1.25102\n",
      "[97]\tvalidation_0-mlogloss:1.24594\n",
      "[98]\tvalidation_0-mlogloss:1.24304\n",
      "[99]\tvalidation_0-mlogloss:1.23966\n",
      "[100]\tvalidation_0-mlogloss:1.23469\n",
      "[101]\tvalidation_0-mlogloss:1.23157\n",
      "[102]\tvalidation_0-mlogloss:1.22759\n",
      "[103]\tvalidation_0-mlogloss:1.22312\n",
      "[104]\tvalidation_0-mlogloss:1.21983\n",
      "[105]\tvalidation_0-mlogloss:1.21617\n",
      "[106]\tvalidation_0-mlogloss:1.21248\n",
      "[107]\tvalidation_0-mlogloss:1.20782\n",
      "[108]\tvalidation_0-mlogloss:1.20251\n",
      "[109]\tvalidation_0-mlogloss:1.19849\n",
      "[110]\tvalidation_0-mlogloss:1.19474\n",
      "[111]\tvalidation_0-mlogloss:1.18989\n",
      "[112]\tvalidation_0-mlogloss:1.18681\n",
      "[113]\tvalidation_0-mlogloss:1.18340\n",
      "[114]\tvalidation_0-mlogloss:1.18012\n",
      "[115]\tvalidation_0-mlogloss:1.17771\n",
      "[116]\tvalidation_0-mlogloss:1.17405\n",
      "[117]\tvalidation_0-mlogloss:1.17161\n",
      "[118]\tvalidation_0-mlogloss:1.16746\n",
      "[119]\tvalidation_0-mlogloss:1.16451\n",
      "[120]\tvalidation_0-mlogloss:1.16053\n",
      "[121]\tvalidation_0-mlogloss:1.15757\n",
      "[122]\tvalidation_0-mlogloss:1.15395\n",
      "[123]\tvalidation_0-mlogloss:1.15232\n",
      "[124]\tvalidation_0-mlogloss:1.14939\n",
      "[125]\tvalidation_0-mlogloss:1.14575\n",
      "[126]\tvalidation_0-mlogloss:1.14396\n",
      "[127]\tvalidation_0-mlogloss:1.14081\n",
      "[128]\tvalidation_0-mlogloss:1.13701\n",
      "[129]\tvalidation_0-mlogloss:1.13539\n",
      "[130]\tvalidation_0-mlogloss:1.13236\n",
      "[131]\tvalidation_0-mlogloss:1.13015\n",
      "[132]\tvalidation_0-mlogloss:1.12827\n",
      "[133]\tvalidation_0-mlogloss:1.12524\n",
      "[134]\tvalidation_0-mlogloss:1.12221\n",
      "[135]\tvalidation_0-mlogloss:1.11911\n",
      "[136]\tvalidation_0-mlogloss:1.11674\n",
      "[137]\tvalidation_0-mlogloss:1.11403\n",
      "[138]\tvalidation_0-mlogloss:1.11054\n",
      "[139]\tvalidation_0-mlogloss:1.10837\n",
      "[140]\tvalidation_0-mlogloss:1.10523\n",
      "[141]\tvalidation_0-mlogloss:1.10311\n",
      "[142]\tvalidation_0-mlogloss:1.10032\n",
      "[143]\tvalidation_0-mlogloss:1.09791\n",
      "[144]\tvalidation_0-mlogloss:1.09446\n",
      "[145]\tvalidation_0-mlogloss:1.09192\n",
      "[146]\tvalidation_0-mlogloss:1.09038\n",
      "[147]\tvalidation_0-mlogloss:1.08791\n",
      "[148]\tvalidation_0-mlogloss:1.08615\n",
      "[149]\tvalidation_0-mlogloss:1.08351\n",
      "[150]\tvalidation_0-mlogloss:1.08173\n",
      "[151]\tvalidation_0-mlogloss:1.07957\n",
      "[152]\tvalidation_0-mlogloss:1.07771\n",
      "[153]\tvalidation_0-mlogloss:1.07585\n",
      "[154]\tvalidation_0-mlogloss:1.07472\n",
      "[155]\tvalidation_0-mlogloss:1.07293\n",
      "[156]\tvalidation_0-mlogloss:1.07086\n",
      "[157]\tvalidation_0-mlogloss:1.06863\n",
      "[158]\tvalidation_0-mlogloss:1.06631\n",
      "[159]\tvalidation_0-mlogloss:1.06502\n",
      "[160]\tvalidation_0-mlogloss:1.06316\n",
      "[161]\tvalidation_0-mlogloss:1.06146\n",
      "[162]\tvalidation_0-mlogloss:1.05975\n",
      "[163]\tvalidation_0-mlogloss:1.05701\n",
      "[164]\tvalidation_0-mlogloss:1.05514\n",
      "[165]\tvalidation_0-mlogloss:1.05360\n",
      "[166]\tvalidation_0-mlogloss:1.05202\n",
      "[167]\tvalidation_0-mlogloss:1.05029\n",
      "[168]\tvalidation_0-mlogloss:1.04865\n",
      "[169]\tvalidation_0-mlogloss:1.04685\n",
      "[170]\tvalidation_0-mlogloss:1.04512\n",
      "[171]\tvalidation_0-mlogloss:1.04406\n",
      "[172]\tvalidation_0-mlogloss:1.04316\n",
      "[173]\tvalidation_0-mlogloss:1.04175\n",
      "[174]\tvalidation_0-mlogloss:1.03955\n",
      "[175]\tvalidation_0-mlogloss:1.03763\n",
      "[176]\tvalidation_0-mlogloss:1.03605\n",
      "[177]\tvalidation_0-mlogloss:1.03532\n",
      "[178]\tvalidation_0-mlogloss:1.03399\n",
      "[179]\tvalidation_0-mlogloss:1.03266\n",
      "[180]\tvalidation_0-mlogloss:1.03034\n",
      "[181]\tvalidation_0-mlogloss:1.02800\n",
      "[182]\tvalidation_0-mlogloss:1.02657\n",
      "[183]\tvalidation_0-mlogloss:1.02488\n",
      "[184]\tvalidation_0-mlogloss:1.02305\n",
      "[185]\tvalidation_0-mlogloss:1.02140\n",
      "[186]\tvalidation_0-mlogloss:1.01995\n",
      "[187]\tvalidation_0-mlogloss:1.01840\n",
      "[188]\tvalidation_0-mlogloss:1.01672\n",
      "[189]\tvalidation_0-mlogloss:1.01540\n",
      "[190]\tvalidation_0-mlogloss:1.01454\n",
      "[191]\tvalidation_0-mlogloss:1.01243\n",
      "[192]\tvalidation_0-mlogloss:1.01094\n",
      "[193]\tvalidation_0-mlogloss:1.00990\n",
      "[194]\tvalidation_0-mlogloss:1.00928\n",
      "[195]\tvalidation_0-mlogloss:1.00884\n",
      "[196]\tvalidation_0-mlogloss:1.00746\n",
      "[197]\tvalidation_0-mlogloss:1.00642\n",
      "[198]\tvalidation_0-mlogloss:1.00474\n",
      "[199]\tvalidation_0-mlogloss:1.00287\n",
      "[200]\tvalidation_0-mlogloss:1.00136\n",
      "[201]\tvalidation_0-mlogloss:1.00112\n",
      "[202]\tvalidation_0-mlogloss:0.99958\n",
      "[203]\tvalidation_0-mlogloss:0.99839\n",
      "[204]\tvalidation_0-mlogloss:0.99798\n",
      "[205]\tvalidation_0-mlogloss:0.99686\n",
      "[206]\tvalidation_0-mlogloss:0.99528\n",
      "[207]\tvalidation_0-mlogloss:0.99369\n",
      "[208]\tvalidation_0-mlogloss:0.99289\n",
      "[209]\tvalidation_0-mlogloss:0.99242\n",
      "[210]\tvalidation_0-mlogloss:0.99164\n",
      "[211]\tvalidation_0-mlogloss:0.98992\n",
      "[212]\tvalidation_0-mlogloss:0.98896\n",
      "[213]\tvalidation_0-mlogloss:0.98737\n",
      "[214]\tvalidation_0-mlogloss:0.98635\n",
      "[215]\tvalidation_0-mlogloss:0.98554\n",
      "[216]\tvalidation_0-mlogloss:0.98439\n",
      "[217]\tvalidation_0-mlogloss:0.98337\n",
      "[218]\tvalidation_0-mlogloss:0.98191\n",
      "[219]\tvalidation_0-mlogloss:0.98047\n",
      "[220]\tvalidation_0-mlogloss:0.97929\n",
      "[221]\tvalidation_0-mlogloss:0.97784\n",
      "[222]\tvalidation_0-mlogloss:0.97618\n",
      "[223]\tvalidation_0-mlogloss:0.97468\n",
      "[224]\tvalidation_0-mlogloss:0.97352\n",
      "[225]\tvalidation_0-mlogloss:0.97325\n",
      "[226]\tvalidation_0-mlogloss:0.97243\n",
      "[227]\tvalidation_0-mlogloss:0.97192\n",
      "[228]\tvalidation_0-mlogloss:0.97091\n",
      "[229]\tvalidation_0-mlogloss:0.96973\n",
      "[230]\tvalidation_0-mlogloss:0.96899\n",
      "[231]\tvalidation_0-mlogloss:0.96809\n",
      "[232]\tvalidation_0-mlogloss:0.96751\n",
      "[233]\tvalidation_0-mlogloss:0.96676\n",
      "[234]\tvalidation_0-mlogloss:0.96576\n",
      "[235]\tvalidation_0-mlogloss:0.96526\n",
      "[236]\tvalidation_0-mlogloss:0.96445\n",
      "[237]\tvalidation_0-mlogloss:0.96399\n",
      "[238]\tvalidation_0-mlogloss:0.96338\n",
      "[239]\tvalidation_0-mlogloss:0.96258\n",
      "[240]\tvalidation_0-mlogloss:0.96165\n",
      "[241]\tvalidation_0-mlogloss:0.96088\n",
      "[242]\tvalidation_0-mlogloss:0.96043\n",
      "[243]\tvalidation_0-mlogloss:0.95936\n",
      "[244]\tvalidation_0-mlogloss:0.95896\n",
      "[245]\tvalidation_0-mlogloss:0.95773\n",
      "[246]\tvalidation_0-mlogloss:0.95684\n",
      "[247]\tvalidation_0-mlogloss:0.95631\n",
      "[248]\tvalidation_0-mlogloss:0.95522\n",
      "[249]\tvalidation_0-mlogloss:0.95457\n",
      "[250]\tvalidation_0-mlogloss:0.95303\n",
      "[251]\tvalidation_0-mlogloss:0.95230\n",
      "[252]\tvalidation_0-mlogloss:0.95138\n",
      "[253]\tvalidation_0-mlogloss:0.95159\n",
      "[254]\tvalidation_0-mlogloss:0.95145\n",
      "[255]\tvalidation_0-mlogloss:0.95002\n",
      "[256]\tvalidation_0-mlogloss:0.94921\n",
      "[257]\tvalidation_0-mlogloss:0.94866\n",
      "[258]\tvalidation_0-mlogloss:0.94787\n",
      "[259]\tvalidation_0-mlogloss:0.94722\n",
      "[260]\tvalidation_0-mlogloss:0.94615\n",
      "[261]\tvalidation_0-mlogloss:0.94515\n",
      "[262]\tvalidation_0-mlogloss:0.94452\n",
      "[263]\tvalidation_0-mlogloss:0.94328\n",
      "[264]\tvalidation_0-mlogloss:0.94224\n",
      "[265]\tvalidation_0-mlogloss:0.94104\n",
      "[266]\tvalidation_0-mlogloss:0.94047\n",
      "[267]\tvalidation_0-mlogloss:0.93937\n",
      "[268]\tvalidation_0-mlogloss:0.93820\n",
      "[269]\tvalidation_0-mlogloss:0.93816\n",
      "[270]\tvalidation_0-mlogloss:0.93681\n",
      "[271]\tvalidation_0-mlogloss:0.93617\n",
      "[272]\tvalidation_0-mlogloss:0.93573\n",
      "[273]\tvalidation_0-mlogloss:0.93491\n",
      "[274]\tvalidation_0-mlogloss:0.93435\n",
      "[275]\tvalidation_0-mlogloss:0.93403\n",
      "[276]\tvalidation_0-mlogloss:0.93300\n",
      "[277]\tvalidation_0-mlogloss:0.93228\n",
      "[278]\tvalidation_0-mlogloss:0.93177\n",
      "[279]\tvalidation_0-mlogloss:0.93140\n",
      "[280]\tvalidation_0-mlogloss:0.93075\n",
      "[281]\tvalidation_0-mlogloss:0.93044\n",
      "[282]\tvalidation_0-mlogloss:0.92979\n",
      "[283]\tvalidation_0-mlogloss:0.92881\n",
      "[284]\tvalidation_0-mlogloss:0.92798\n",
      "[285]\tvalidation_0-mlogloss:0.92799\n",
      "[286]\tvalidation_0-mlogloss:0.92769\n",
      "[287]\tvalidation_0-mlogloss:0.92703\n",
      "[288]\tvalidation_0-mlogloss:0.92681\n",
      "[289]\tvalidation_0-mlogloss:0.92683\n",
      "[290]\tvalidation_0-mlogloss:0.92671\n",
      "[291]\tvalidation_0-mlogloss:0.92666\n",
      "[292]\tvalidation_0-mlogloss:0.92611\n",
      "[293]\tvalidation_0-mlogloss:0.92541\n",
      "[294]\tvalidation_0-mlogloss:0.92469\n",
      "[295]\tvalidation_0-mlogloss:0.92402\n",
      "[296]\tvalidation_0-mlogloss:0.92390\n",
      "[297]\tvalidation_0-mlogloss:0.92337\n",
      "[298]\tvalidation_0-mlogloss:0.92328\n",
      "[299]\tvalidation_0-mlogloss:0.92213\n",
      "[300]\tvalidation_0-mlogloss:0.92203\n",
      "[301]\tvalidation_0-mlogloss:0.92118\n",
      "[302]\tvalidation_0-mlogloss:0.92087\n",
      "[303]\tvalidation_0-mlogloss:0.92000\n",
      "[304]\tvalidation_0-mlogloss:0.91990\n",
      "[305]\tvalidation_0-mlogloss:0.91960\n",
      "[306]\tvalidation_0-mlogloss:0.91907\n",
      "[307]\tvalidation_0-mlogloss:0.91833\n",
      "[308]\tvalidation_0-mlogloss:0.91815\n",
      "[309]\tvalidation_0-mlogloss:0.91774\n",
      "[310]\tvalidation_0-mlogloss:0.91695\n",
      "[311]\tvalidation_0-mlogloss:0.91624\n",
      "[312]\tvalidation_0-mlogloss:0.91578\n",
      "[313]\tvalidation_0-mlogloss:0.91510\n",
      "[314]\tvalidation_0-mlogloss:0.91468\n",
      "[315]\tvalidation_0-mlogloss:0.91403\n",
      "[316]\tvalidation_0-mlogloss:0.91348\n",
      "[317]\tvalidation_0-mlogloss:0.91285\n",
      "[318]\tvalidation_0-mlogloss:0.91198\n",
      "[319]\tvalidation_0-mlogloss:0.91126\n",
      "[320]\tvalidation_0-mlogloss:0.91108\n",
      "[321]\tvalidation_0-mlogloss:0.91085\n",
      "[322]\tvalidation_0-mlogloss:0.91070\n",
      "[323]\tvalidation_0-mlogloss:0.91081\n",
      "[324]\tvalidation_0-mlogloss:0.91059\n",
      "[325]\tvalidation_0-mlogloss:0.90969\n",
      "[326]\tvalidation_0-mlogloss:0.90889\n",
      "[327]\tvalidation_0-mlogloss:0.90828\n",
      "[328]\tvalidation_0-mlogloss:0.90834\n",
      "[329]\tvalidation_0-mlogloss:0.90786\n",
      "[330]\tvalidation_0-mlogloss:0.90749\n",
      "[331]\tvalidation_0-mlogloss:0.90712\n",
      "[332]\tvalidation_0-mlogloss:0.90646\n",
      "[333]\tvalidation_0-mlogloss:0.90704\n",
      "[334]\tvalidation_0-mlogloss:0.90718\n",
      "[335]\tvalidation_0-mlogloss:0.90736\n",
      "[336]\tvalidation_0-mlogloss:0.90687\n",
      "[337]\tvalidation_0-mlogloss:0.90626\n",
      "[338]\tvalidation_0-mlogloss:0.90648\n",
      "[339]\tvalidation_0-mlogloss:0.90605\n",
      "[340]\tvalidation_0-mlogloss:0.90555\n",
      "[341]\tvalidation_0-mlogloss:0.90490\n",
      "[342]\tvalidation_0-mlogloss:0.90457\n",
      "[343]\tvalidation_0-mlogloss:0.90387\n",
      "[344]\tvalidation_0-mlogloss:0.90379\n",
      "[345]\tvalidation_0-mlogloss:0.90347\n",
      "[346]\tvalidation_0-mlogloss:0.90301\n",
      "[347]\tvalidation_0-mlogloss:0.90253\n",
      "[348]\tvalidation_0-mlogloss:0.90150\n",
      "[349]\tvalidation_0-mlogloss:0.90109\n",
      "[350]\tvalidation_0-mlogloss:0.90064\n",
      "[351]\tvalidation_0-mlogloss:0.90008\n",
      "[352]\tvalidation_0-mlogloss:0.89998\n",
      "[353]\tvalidation_0-mlogloss:0.89972\n",
      "[354]\tvalidation_0-mlogloss:0.89891\n",
      "[355]\tvalidation_0-mlogloss:0.89845\n",
      "[356]\tvalidation_0-mlogloss:0.89777\n",
      "[357]\tvalidation_0-mlogloss:0.89721\n",
      "[358]\tvalidation_0-mlogloss:0.89667\n",
      "[359]\tvalidation_0-mlogloss:0.89598\n",
      "[360]\tvalidation_0-mlogloss:0.89608\n",
      "[361]\tvalidation_0-mlogloss:0.89615\n",
      "[362]\tvalidation_0-mlogloss:0.89636\n",
      "[363]\tvalidation_0-mlogloss:0.89597\n",
      "[364]\tvalidation_0-mlogloss:0.89570\n",
      "[365]\tvalidation_0-mlogloss:0.89618\n",
      "[366]\tvalidation_0-mlogloss:0.89599\n",
      "[367]\tvalidation_0-mlogloss:0.89546\n",
      "[368]\tvalidation_0-mlogloss:0.89520\n",
      "[369]\tvalidation_0-mlogloss:0.89453\n",
      "[370]\tvalidation_0-mlogloss:0.89417\n",
      "[371]\tvalidation_0-mlogloss:0.89378\n",
      "[372]\tvalidation_0-mlogloss:0.89335\n",
      "[373]\tvalidation_0-mlogloss:0.89276\n",
      "[374]\tvalidation_0-mlogloss:0.89199\n",
      "[375]\tvalidation_0-mlogloss:0.89207\n",
      "[376]\tvalidation_0-mlogloss:0.89174\n",
      "[377]\tvalidation_0-mlogloss:0.89114\n",
      "[378]\tvalidation_0-mlogloss:0.89071\n",
      "[379]\tvalidation_0-mlogloss:0.89005\n",
      "[380]\tvalidation_0-mlogloss:0.88981\n",
      "[381]\tvalidation_0-mlogloss:0.88987\n",
      "[382]\tvalidation_0-mlogloss:0.88973\n",
      "[383]\tvalidation_0-mlogloss:0.88967\n",
      "[384]\tvalidation_0-mlogloss:0.88919\n",
      "[385]\tvalidation_0-mlogloss:0.88874\n",
      "[386]\tvalidation_0-mlogloss:0.88843\n",
      "[387]\tvalidation_0-mlogloss:0.88875\n",
      "[388]\tvalidation_0-mlogloss:0.88800\n",
      "[389]\tvalidation_0-mlogloss:0.88796\n",
      "[390]\tvalidation_0-mlogloss:0.88755\n",
      "[391]\tvalidation_0-mlogloss:0.88726\n",
      "[392]\tvalidation_0-mlogloss:0.88680\n",
      "[393]\tvalidation_0-mlogloss:0.88658\n",
      "[394]\tvalidation_0-mlogloss:0.88598\n",
      "[395]\tvalidation_0-mlogloss:0.88551\n",
      "[396]\tvalidation_0-mlogloss:0.88614\n",
      "[397]\tvalidation_0-mlogloss:0.88594\n",
      "[398]\tvalidation_0-mlogloss:0.88558\n",
      "[399]\tvalidation_0-mlogloss:0.88536\n",
      "[400]\tvalidation_0-mlogloss:0.88522\n",
      "[401]\tvalidation_0-mlogloss:0.88493\n",
      "[402]\tvalidation_0-mlogloss:0.88497\n",
      "[403]\tvalidation_0-mlogloss:0.88495\n",
      "[404]\tvalidation_0-mlogloss:0.88487\n",
      "[405]\tvalidation_0-mlogloss:0.88431\n",
      "[406]\tvalidation_0-mlogloss:0.88407\n",
      "[407]\tvalidation_0-mlogloss:0.88401\n",
      "[408]\tvalidation_0-mlogloss:0.88389\n",
      "[409]\tvalidation_0-mlogloss:0.88386\n",
      "[410]\tvalidation_0-mlogloss:0.88378\n",
      "[411]\tvalidation_0-mlogloss:0.88350\n",
      "[412]\tvalidation_0-mlogloss:0.88380\n",
      "[413]\tvalidation_0-mlogloss:0.88300\n",
      "[414]\tvalidation_0-mlogloss:0.88319\n",
      "[415]\tvalidation_0-mlogloss:0.88282\n",
      "[416]\tvalidation_0-mlogloss:0.88237\n",
      "[417]\tvalidation_0-mlogloss:0.88224\n",
      "[418]\tvalidation_0-mlogloss:0.88206\n",
      "[419]\tvalidation_0-mlogloss:0.88200\n",
      "[420]\tvalidation_0-mlogloss:0.88157\n",
      "[421]\tvalidation_0-mlogloss:0.88131\n",
      "[422]\tvalidation_0-mlogloss:0.88078\n",
      "[423]\tvalidation_0-mlogloss:0.88047\n",
      "[424]\tvalidation_0-mlogloss:0.88012\n",
      "[425]\tvalidation_0-mlogloss:0.87991\n",
      "[426]\tvalidation_0-mlogloss:0.87973\n",
      "[427]\tvalidation_0-mlogloss:0.87929\n",
      "[428]\tvalidation_0-mlogloss:0.87865\n",
      "[429]\tvalidation_0-mlogloss:0.87820\n",
      "[430]\tvalidation_0-mlogloss:0.87809\n",
      "[431]\tvalidation_0-mlogloss:0.87792\n",
      "[432]\tvalidation_0-mlogloss:0.87765\n",
      "[433]\tvalidation_0-mlogloss:0.87728\n",
      "[434]\tvalidation_0-mlogloss:0.87705\n",
      "[435]\tvalidation_0-mlogloss:0.87741\n",
      "[436]\tvalidation_0-mlogloss:0.87722\n",
      "[437]\tvalidation_0-mlogloss:0.87699\n",
      "[438]\tvalidation_0-mlogloss:0.87698\n",
      "[439]\tvalidation_0-mlogloss:0.87710\n",
      "[440]\tvalidation_0-mlogloss:0.87747\n",
      "[441]\tvalidation_0-mlogloss:0.87743\n",
      "[442]\tvalidation_0-mlogloss:0.87765\n",
      "[443]\tvalidation_0-mlogloss:0.87749\n",
      "[444]\tvalidation_0-mlogloss:0.87737\n",
      "[445]\tvalidation_0-mlogloss:0.87714\n",
      "[446]\tvalidation_0-mlogloss:0.87691\n",
      "[447]\tvalidation_0-mlogloss:0.87653\n",
      "[448]\tvalidation_0-mlogloss:0.87663\n",
      "[449]\tvalidation_0-mlogloss:0.87659\n",
      "[450]\tvalidation_0-mlogloss:0.87614\n",
      "[451]\tvalidation_0-mlogloss:0.87607\n",
      "[452]\tvalidation_0-mlogloss:0.87583\n",
      "[453]\tvalidation_0-mlogloss:0.87554\n",
      "[454]\tvalidation_0-mlogloss:0.87547\n",
      "[455]\tvalidation_0-mlogloss:0.87506\n",
      "[456]\tvalidation_0-mlogloss:0.87485\n",
      "[457]\tvalidation_0-mlogloss:0.87495\n",
      "[458]\tvalidation_0-mlogloss:0.87474\n",
      "[459]\tvalidation_0-mlogloss:0.87437\n",
      "[460]\tvalidation_0-mlogloss:0.87413\n",
      "[461]\tvalidation_0-mlogloss:0.87391\n",
      "[462]\tvalidation_0-mlogloss:0.87333\n",
      "[463]\tvalidation_0-mlogloss:0.87317\n",
      "[464]\tvalidation_0-mlogloss:0.87303\n",
      "[465]\tvalidation_0-mlogloss:0.87287\n",
      "[466]\tvalidation_0-mlogloss:0.87290\n",
      "[467]\tvalidation_0-mlogloss:0.87236\n",
      "[468]\tvalidation_0-mlogloss:0.87219\n",
      "[469]\tvalidation_0-mlogloss:0.87184\n",
      "[470]\tvalidation_0-mlogloss:0.87168\n",
      "[471]\tvalidation_0-mlogloss:0.87152\n",
      "[472]\tvalidation_0-mlogloss:0.87125\n",
      "[473]\tvalidation_0-mlogloss:0.87112\n",
      "[474]\tvalidation_0-mlogloss:0.87086\n",
      "[475]\tvalidation_0-mlogloss:0.87077\n",
      "[476]\tvalidation_0-mlogloss:0.87064\n",
      "[477]\tvalidation_0-mlogloss:0.87043\n",
      "[478]\tvalidation_0-mlogloss:0.87010\n",
      "[479]\tvalidation_0-mlogloss:0.87003\n",
      "[480]\tvalidation_0-mlogloss:0.86996\n",
      "[481]\tvalidation_0-mlogloss:0.86965\n",
      "[482]\tvalidation_0-mlogloss:0.86953\n",
      "[483]\tvalidation_0-mlogloss:0.86912\n",
      "[484]\tvalidation_0-mlogloss:0.86899\n",
      "[485]\tvalidation_0-mlogloss:0.86879\n",
      "[486]\tvalidation_0-mlogloss:0.86892\n",
      "[487]\tvalidation_0-mlogloss:0.86893\n",
      "[488]\tvalidation_0-mlogloss:0.86883\n",
      "[489]\tvalidation_0-mlogloss:0.86891\n",
      "[490]\tvalidation_0-mlogloss:0.86859\n",
      "[491]\tvalidation_0-mlogloss:0.86901\n",
      "[492]\tvalidation_0-mlogloss:0.86872\n",
      "[493]\tvalidation_0-mlogloss:0.86843\n",
      "[494]\tvalidation_0-mlogloss:0.86849\n",
      "[495]\tvalidation_0-mlogloss:0.86829\n",
      "[496]\tvalidation_0-mlogloss:0.86813\n",
      "[497]\tvalidation_0-mlogloss:0.86804\n",
      "[498]\tvalidation_0-mlogloss:0.86820\n",
      "[499]\tvalidation_0-mlogloss:0.86791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77      1499\n",
      "           1       0.80      0.50      0.61      1499\n",
      "           2       0.83      0.89      0.86      1499\n",
      "           3       0.88      0.86      0.87      1499\n",
      "           4       0.99      0.99      0.99      1499\n",
      "           5       0.99      0.99      0.99      1499\n",
      "           6       0.93      0.83      0.87      1499\n",
      "           7       0.92      0.55      0.69      1499\n",
      "           8       0.79      0.93      0.85      1499\n",
      "           9       0.96      0.88      0.92      1499\n",
      "          10       0.77      0.67      0.72      1499\n",
      "          11       0.96      0.99      0.97      1499\n",
      "          12       0.83      0.91      0.87      1499\n",
      "          13       0.85      0.70      0.77      1499\n",
      "          14       0.80      0.91      0.85      1499\n",
      "          15       0.88      0.90      0.89      1499\n",
      "          16       0.87      0.91      0.89      1499\n",
      "          17       0.75      0.95      0.84      1499\n",
      "          18       0.87      0.95      0.91      1499\n",
      "          19       0.64      0.75      0.69      1499\n",
      "          20       0.99      1.00      0.99      1499\n",
      "          21       0.84      0.80      0.82      1499\n",
      "          22       0.77      0.90      0.83      1499\n",
      "          23       0.98      0.99      0.99      1499\n",
      "          24       0.91      0.85      0.88      1499\n",
      "          25       0.90      0.61      0.73      1499\n",
      "          26       0.72      0.28      0.40      1499\n",
      "          27       0.52      0.46      0.49      1499\n",
      "          28       0.77      0.95      0.85      1499\n",
      "          29       0.94      0.93      0.93      1499\n",
      "          30       0.76      0.97      0.86      1499\n",
      "          31       0.73      0.91      0.81      1499\n",
      "          32       0.63      0.56      0.59      1499\n",
      "          33       0.73      0.82      0.77      1499\n",
      "          34       0.98      0.76      0.85      1499\n",
      "          35       0.75      0.97      0.85      1499\n",
      "          36       0.86      0.93      0.90      1499\n",
      "          37       0.74      0.97      0.84      1499\n",
      "          38       0.83      0.33      0.48      1499\n",
      "          39       0.90      0.76      0.82      1499\n",
      "          40       0.92      0.81      0.87      1499\n",
      "          41       0.88      0.79      0.83      1499\n",
      "          42       0.91      0.78      0.84      1499\n",
      "          43       0.85      0.89      0.87      1499\n",
      "          44       0.91      0.97      0.94      1499\n",
      "          45       0.76      0.72      0.74      1499\n",
      "          46       0.45      0.50      0.47      1499\n",
      "          47       0.89      0.41      0.56      1499\n",
      "          48       0.69      0.91      0.78      1499\n",
      "          49       0.76      0.79      0.77      1499\n",
      "          50       0.87      0.83      0.85      1499\n",
      "          51       0.80      0.67      0.73      1499\n",
      "          52       0.81      0.91      0.86      1499\n",
      "          53       0.79      0.79      0.79      1499\n",
      "          54       0.81      0.96      0.88      1499\n",
      "          55       0.83      0.90      0.87      1499\n",
      "          56       0.72      0.81      0.76      1499\n",
      "          57       0.95      0.99      0.97      1499\n",
      "          58       0.98      0.75      0.85      1499\n",
      "          59       0.72      0.54      0.62      1499\n",
      "          60       0.73      0.91      0.81      1499\n",
      "          61       0.93      0.88      0.91      1499\n",
      "          62       0.78      0.97      0.87      1499\n",
      "          63       0.64      0.41      0.50      1499\n",
      "          64       0.85      0.85      0.85      1499\n",
      "          65       0.90      0.90      0.90      1499\n",
      "          66       0.92      0.94      0.93      1499\n",
      "          67       0.98      0.68      0.80      1499\n",
      "          68       0.84      0.90      0.87      1499\n",
      "          69       0.59      0.77      0.67      1499\n",
      "          70       0.71      0.80      0.75      1499\n",
      "          71       0.77      0.76      0.76      1499\n",
      "          72       0.89      0.93      0.91      1499\n",
      "          73       0.92      0.92      0.92      1499\n",
      "          74       0.41      0.72      0.52      1499\n",
      "\n",
      "    accuracy                           0.81    112425\n",
      "   macro avg       0.82      0.81      0.80    112425\n",
      "weighted avg       0.82      0.81      0.80    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3361defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a896a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 29s 1ms/step - loss: 1.0420 - val_loss: 2.6500\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 28s 999us/step - loss: 0.4017 - val_loss: 2.5875\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 28s 996us/step - loss: 0.2887 - val_loss: 2.5339\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 28s 1ms/step - loss: 0.2335 - val_loss: 2.4446\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 28s 1ms/step - loss: 0.1976 - val_loss: 2.5517\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 28s 1ms/step - loss: 0.1750 - val_loss: 2.6834\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 28s 1ms/step - loss: 0.1549 - val_loss: 2.3723\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 23s 806us/step - loss: 0.1412 - val_loss: 2.5446\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 22s 766us/step - loss: 0.1295 - val_loss: 2.5495\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 22s 769us/step - loss: 0.1211 - val_loss: 2.5667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x33579b700>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 1s 317us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.66      0.77      1499\n",
      "           1       0.70      0.40      0.51      1499\n",
      "           2       0.83      0.73      0.78      1499\n",
      "           3       0.96      0.80      0.87      1499\n",
      "           4       0.95      0.99      0.97      1499\n",
      "           5       0.96      0.98      0.97      1499\n",
      "           6       0.95      0.83      0.89      1499\n",
      "           7       0.81      0.67      0.73      1499\n",
      "           8       0.78      0.91      0.84      1499\n",
      "           9       0.96      0.89      0.92      1499\n",
      "          10       0.68      0.77      0.72      1499\n",
      "          11       0.97      0.99      0.98      1499\n",
      "          12       0.89      0.85      0.87      1499\n",
      "          13       0.74      0.82      0.78      1499\n",
      "          14       0.63      0.88      0.73      1499\n",
      "          15       0.96      0.96      0.96      1499\n",
      "          16       0.91      0.76      0.82      1499\n",
      "          17       0.82      0.82      0.82      1499\n",
      "          18       0.93      0.83      0.88      1499\n",
      "          19       0.41      0.76      0.53      1499\n",
      "          20       0.97      0.98      0.98      1499\n",
      "          21       0.80      0.71      0.75      1499\n",
      "          22       0.84      0.79      0.81      1499\n",
      "          23       0.95      0.99      0.97      1499\n",
      "          24       0.84      0.78      0.81      1499\n",
      "          25       0.90      0.38      0.53      1499\n",
      "          26       0.62      0.48      0.54      1499\n",
      "          27       0.63      0.67      0.65      1499\n",
      "          28       0.77      0.93      0.84      1499\n",
      "          29       0.99      0.79      0.88      1499\n",
      "          30       0.86      0.97      0.91      1499\n",
      "          31       0.83      0.90      0.86      1499\n",
      "          32       0.60      0.36      0.45      1499\n",
      "          33       0.67      0.85      0.75      1499\n",
      "          34       0.99      0.92      0.95      1499\n",
      "          35       0.88      0.91      0.90      1499\n",
      "          36       0.77      0.90      0.83      1499\n",
      "          37       0.84      0.94      0.88      1499\n",
      "          38       0.61      0.44      0.51      1499\n",
      "          39       0.69      0.57      0.62      1499\n",
      "          40       0.93      0.59      0.72      1499\n",
      "          41       0.81      0.47      0.60      1499\n",
      "          42       0.51      0.67      0.58      1499\n",
      "          43       0.78      0.93      0.85      1499\n",
      "          44       0.94      0.95      0.94      1499\n",
      "          45       0.79      0.68      0.73      1499\n",
      "          46       0.34      0.46      0.39      1499\n",
      "          47       0.93      0.37      0.53      1499\n",
      "          48       0.68      0.83      0.75      1499\n",
      "          49       0.74      0.74      0.74      1499\n",
      "          50       0.77      0.62      0.69      1499\n",
      "          51       0.49      0.60      0.54      1499\n",
      "          52       0.83      0.88      0.86      1499\n",
      "          53       0.68      0.81      0.74      1499\n",
      "          54       0.79      0.95      0.86      1499\n",
      "          55       0.86      0.88      0.87      1499\n",
      "          56       0.60      0.82      0.69      1499\n",
      "          57       0.94      0.91      0.92      1499\n",
      "          58       0.84      0.72      0.77      1499\n",
      "          59       0.56      0.45      0.50      1499\n",
      "          60       0.73      0.79      0.76      1499\n",
      "          61       0.93      0.69      0.79      1499\n",
      "          62       0.91      0.96      0.93      1499\n",
      "          63       0.42      0.32      0.36      1499\n",
      "          64       0.55      0.76      0.64      1499\n",
      "          65       0.86      0.91      0.89      1499\n",
      "          66       0.95      0.86      0.90      1499\n",
      "          67       0.89      0.80      0.84      1499\n",
      "          68       0.95      0.91      0.93      1499\n",
      "          69       0.58      0.66      0.62      1499\n",
      "          70       0.72      0.75      0.74      1499\n",
      "          71       0.61      0.58      0.60      1499\n",
      "          72       0.95      0.76      0.85      1499\n",
      "          73       0.77      0.90      0.83      1499\n",
      "          74       0.37      0.76      0.50      1499\n",
      "\n",
      "    accuracy                           0.76    112425\n",
      "   macro avg       0.78      0.76      0.76    112425\n",
      "weighted avg       0.78      0.76      0.76    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca6c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431f02d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
