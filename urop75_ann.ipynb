{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 75 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 03:21:08.563975: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 03:21:11.308648: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 03:21:11.308734: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 03:21:11.682561: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 03:21:12.519495: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 03:21:12.520106: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 03:21:16.350718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/S001R03.edf',\n",
       " 'files/S002R03.edf',\n",
       " 'files/S003R03.edf',\n",
       " 'files/S004R03.edf',\n",
       " 'files/S005R03.edf',\n",
       " 'files/S006R03.edf',\n",
       " 'files/S007R03.edf',\n",
       " 'files/S008R03.edf',\n",
       " 'files/S009R03.edf',\n",
       " 'files/S010R03.edf',\n",
       " 'files/S011R03.edf',\n",
       " 'files/S012R03.edf',\n",
       " 'files/S013R03.edf',\n",
       " 'files/S014R03.edf',\n",
       " 'files/S015R03.edf',\n",
       " 'files/S016R03.edf',\n",
       " 'files/S017R03.edf',\n",
       " 'files/S018R03.edf',\n",
       " 'files/S019R03.edf',\n",
       " 'files/S020R03.edf',\n",
       " 'files/S021R03.edf',\n",
       " 'files/S022R03.edf',\n",
       " 'files/S023R03.edf',\n",
       " 'files/S024R03.edf',\n",
       " 'files/S025R03.edf',\n",
       " 'files/S026R03.edf',\n",
       " 'files/S027R03.edf',\n",
       " 'files/S028R03.edf',\n",
       " 'files/S029R03.edf',\n",
       " 'files/S030R03.edf',\n",
       " 'files/S031R03.edf',\n",
       " 'files/S032R03.edf',\n",
       " 'files/S033R03.edf',\n",
       " 'files/S034R03.edf',\n",
       " 'files/S035R03.edf',\n",
       " 'files/S036R03.edf',\n",
       " 'files/S037R03.edf',\n",
       " 'files/S038R03.edf',\n",
       " 'files/S039R03.edf',\n",
       " 'files/S040R03.edf',\n",
       " 'files/S041R03.edf',\n",
       " 'files/S042R03.edf',\n",
       " 'files/S043R03.edf',\n",
       " 'files/S044R03.edf',\n",
       " 'files/S045R03.edf',\n",
       " 'files/S046R03.edf',\n",
       " 'files/S047R03.edf',\n",
       " 'files/S048R03.edf',\n",
       " 'files/S049R03.edf',\n",
       " 'files/S050R03.edf',\n",
       " 'files/S051R03.edf',\n",
       " 'files/S052R03.edf',\n",
       " 'files/S053R03.edf',\n",
       " 'files/S054R03.edf',\n",
       " 'files/S055R03.edf',\n",
       " 'files/S056R03.edf',\n",
       " 'files/S057R03.edf',\n",
       " 'files/S058R03.edf',\n",
       " 'files/S059R03.edf',\n",
       " 'files/S060R03.edf',\n",
       " 'files/S061R03.edf',\n",
       " 'files/S062R03.edf',\n",
       " 'files/S063R03.edf',\n",
       " 'files/S064R03.edf',\n",
       " 'files/S065R03.edf',\n",
       " 'files/S066R03.edf',\n",
       " 'files/S067R03.edf',\n",
       " 'files/S068R03.edf',\n",
       " 'files/S069R03.edf',\n",
       " 'files/S070R03.edf',\n",
       " 'files/S071R03.edf',\n",
       " 'files/S072R03.edf',\n",
       " 'files/S073R03.edf',\n",
       " 'files/S074R03.edf',\n",
       " 'files/S075R03.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ca7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 112425, 900000, 112425)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.7e-05 -4.5e-05 -2.9e-05 ...  5.9e-05 -8.0e-06  1.4e-05]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000057  -0.000013  -0.000015  -0.000012  -0.000013  -0.000008   \n",
       "1       -0.000049  -0.000011  -0.000010  -0.000012  -0.000019  -0.000024   \n",
       "2       -0.000055  -0.000017  -0.000016  -0.000019  -0.000024  -0.000029   \n",
       "3       -0.000073  -0.000042  -0.000040  -0.000037  -0.000037  -0.000040   \n",
       "4       -0.000087  -0.000053  -0.000052  -0.000051  -0.000045  -0.000043   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995  -0.000069  -0.000027  -0.000012   0.000014   0.000003  -0.000085   \n",
       "899996  -0.000083  -0.000031  -0.000012   0.000024   0.000006  -0.000078   \n",
       "899997  -0.000001  -0.000066  -0.000030   0.000001  -0.000022  -0.000072   \n",
       "899998  -0.000065  -0.000056  -0.000013   0.000015  -0.000005  -0.000077   \n",
       "899999  -0.000070  -0.000065  -0.000040  -0.000011  -0.000029  -0.000081   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000040  -0.000054  -0.000012  -0.000014  ...   -0.000048   \n",
       "1       -0.000058  -0.000051  -0.000019  -0.000023  ...   -0.000055   \n",
       "2       -0.000066  -0.000061  -0.000030  -0.000036  ...   -0.000054   \n",
       "3       -0.000071  -0.000078  -0.000053  -0.000053  ...   -0.000065   \n",
       "4       -0.000071  -0.000087  -0.000065  -0.000064  ...   -0.000075   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000053  -0.000061  -0.000040  -0.000025  ...    0.000008   \n",
       "899996   0.000030  -0.000059  -0.000030  -0.000021  ...   -0.000024   \n",
       "899997  -0.000006  -0.000043  -0.000043  -0.000039  ...    0.000013   \n",
       "899998   0.000008  -0.000069  -0.000028  -0.000022  ...   -0.000033   \n",
       "899999   0.000006  -0.000100  -0.000059  -0.000052  ...   -0.000072   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000038   -0.000042   -0.000068   -0.000076   -0.000103   \n",
       "1        -0.000055   -0.000063   -0.000082   -0.000087   -0.000099   \n",
       "2        -0.000063   -0.000072   -0.000091   -0.000092   -0.000091   \n",
       "3        -0.000052   -0.000066   -0.000100   -0.000105   -0.000105   \n",
       "4        -0.000082   -0.000090   -0.000117   -0.000119   -0.000118   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995   -0.000014    0.000031    0.000005   -0.000013    0.000002   \n",
       "899996   -0.000029    0.000003   -0.000020   -0.000035   -0.000036   \n",
       "899997    0.000001    0.000023   -0.000005   -0.000025   -0.000016   \n",
       "899998   -0.000025   -0.000007   -0.000031   -0.000048   -0.000048   \n",
       "899999   -0.000095   -0.000090   -0.000100   -0.000092   -0.000077   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000051   -0.000056   -0.000124   -0.000028  \n",
       "1        -0.000059   -0.000070   -0.000149   -0.000040  \n",
       "2        -0.000067   -0.000077   -0.000153   -0.000037  \n",
       "3        -0.000067   -0.000072   -0.000148   -0.000026  \n",
       "4        -0.000075   -0.000082   -0.000161   -0.000035  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000021    0.000050   -0.000034   -0.000002  \n",
       "899996    0.000001    0.000025   -0.000039    0.000013  \n",
       "899997    0.000022    0.000055   -0.000040    0.000014  \n",
       "899998   -0.000012    0.000034   -0.000044   -0.000002  \n",
       "899999   -0.000078   -0.000010   -0.000065   -0.000025  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2632/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_2632/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_2632/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000057  -0.000013  -0.000015  -0.000012  -0.000013  -0.000008   \n",
       "1       -0.000049  -0.000011  -0.000010  -0.000012  -0.000019  -0.000024   \n",
       "2       -0.000055  -0.000017  -0.000016  -0.000019  -0.000024  -0.000029   \n",
       "3       -0.000073  -0.000042  -0.000040  -0.000037  -0.000037  -0.000040   \n",
       "4       -0.000087  -0.000053  -0.000052  -0.000051  -0.000045  -0.000043   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995  -0.000069  -0.000027  -0.000012   0.000014   0.000003  -0.000085   \n",
       "899996  -0.000083  -0.000031  -0.000012   0.000024   0.000006  -0.000078   \n",
       "899997  -0.000001  -0.000066  -0.000030   0.000001  -0.000022  -0.000072   \n",
       "899998  -0.000065  -0.000056  -0.000013   0.000015  -0.000005  -0.000077   \n",
       "899999  -0.000070  -0.000065  -0.000040  -0.000011  -0.000029  -0.000081   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000040  -0.000054  -0.000012  -0.000014  ...   -0.000048   \n",
       "1       -0.000058  -0.000051  -0.000019  -0.000023  ...   -0.000055   \n",
       "2       -0.000066  -0.000061  -0.000030  -0.000036  ...   -0.000054   \n",
       "3       -0.000071  -0.000078  -0.000053  -0.000053  ...   -0.000065   \n",
       "4       -0.000071  -0.000087  -0.000065  -0.000064  ...   -0.000075   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000053  -0.000061  -0.000040  -0.000025  ...    0.000008   \n",
       "899996   0.000030  -0.000059  -0.000030  -0.000021  ...   -0.000024   \n",
       "899997  -0.000006  -0.000043  -0.000043  -0.000039  ...    0.000013   \n",
       "899998   0.000008  -0.000069  -0.000028  -0.000022  ...   -0.000033   \n",
       "899999   0.000006  -0.000100  -0.000059  -0.000052  ...   -0.000072   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000038   -0.000042   -0.000068   -0.000076   -0.000103   \n",
       "1        -0.000055   -0.000063   -0.000082   -0.000087   -0.000099   \n",
       "2        -0.000063   -0.000072   -0.000091   -0.000092   -0.000091   \n",
       "3        -0.000052   -0.000066   -0.000100   -0.000105   -0.000105   \n",
       "4        -0.000082   -0.000090   -0.000117   -0.000119   -0.000118   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995   -0.000014    0.000031    0.000005   -0.000013    0.000002   \n",
       "899996   -0.000029    0.000003   -0.000020   -0.000035   -0.000036   \n",
       "899997    0.000001    0.000023   -0.000005   -0.000025   -0.000016   \n",
       "899998   -0.000025   -0.000007   -0.000031   -0.000048   -0.000048   \n",
       "899999   -0.000095   -0.000090   -0.000100   -0.000092   -0.000077   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000051   -0.000056   -0.000124   -0.000028  \n",
       "1        -0.000059   -0.000070   -0.000149   -0.000040  \n",
       "2        -0.000067   -0.000077   -0.000153   -0.000037  \n",
       "3        -0.000067   -0.000072   -0.000148   -0.000026  \n",
       "4        -0.000075   -0.000082   -0.000161   -0.000035  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000021    0.000050   -0.000034   -0.000002  \n",
       "899996    0.000001    0.000025   -0.000039    0.000013  \n",
       "899997    0.000022    0.000055   -0.000040    0.000014  \n",
       "899998   -0.000012    0.000034   -0.000044   -0.000002  \n",
       "899999   -0.000078   -0.000010   -0.000065   -0.000025  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:4.06621\n",
      "[1]\tvalidation_0-mlogloss:3.95270\n",
      "[2]\tvalidation_0-mlogloss:3.86804\n",
      "[3]\tvalidation_0-mlogloss:3.80472\n",
      "[4]\tvalidation_0-mlogloss:3.75237\n",
      "[5]\tvalidation_0-mlogloss:3.70919\n",
      "[6]\tvalidation_0-mlogloss:3.67118\n",
      "[7]\tvalidation_0-mlogloss:3.64003\n",
      "[8]\tvalidation_0-mlogloss:3.61112\n",
      "[9]\tvalidation_0-mlogloss:3.58615\n",
      "[10]\tvalidation_0-mlogloss:3.56485\n",
      "[11]\tvalidation_0-mlogloss:3.54325\n",
      "[12]\tvalidation_0-mlogloss:3.52554\n",
      "[13]\tvalidation_0-mlogloss:3.50459\n",
      "[14]\tvalidation_0-mlogloss:3.48736\n",
      "[15]\tvalidation_0-mlogloss:3.47361\n",
      "[16]\tvalidation_0-mlogloss:3.46029\n",
      "[17]\tvalidation_0-mlogloss:3.44851\n",
      "[18]\tvalidation_0-mlogloss:3.43506\n",
      "[19]\tvalidation_0-mlogloss:3.42360\n",
      "[20]\tvalidation_0-mlogloss:3.41246\n",
      "[21]\tvalidation_0-mlogloss:3.40137\n",
      "[22]\tvalidation_0-mlogloss:3.39074\n",
      "[23]\tvalidation_0-mlogloss:3.38149\n",
      "[24]\tvalidation_0-mlogloss:3.37101\n",
      "[25]\tvalidation_0-mlogloss:3.36180\n",
      "[26]\tvalidation_0-mlogloss:3.35150\n",
      "[27]\tvalidation_0-mlogloss:3.34194\n",
      "[28]\tvalidation_0-mlogloss:3.33135\n",
      "[29]\tvalidation_0-mlogloss:3.32188\n",
      "[30]\tvalidation_0-mlogloss:3.31282\n",
      "[31]\tvalidation_0-mlogloss:3.30539\n",
      "[32]\tvalidation_0-mlogloss:3.29621\n",
      "[33]\tvalidation_0-mlogloss:3.28929\n",
      "[34]\tvalidation_0-mlogloss:3.28165\n",
      "[35]\tvalidation_0-mlogloss:3.27260\n",
      "[36]\tvalidation_0-mlogloss:3.26463\n",
      "[37]\tvalidation_0-mlogloss:3.25837\n",
      "[38]\tvalidation_0-mlogloss:3.25135\n",
      "[39]\tvalidation_0-mlogloss:3.24465\n",
      "[40]\tvalidation_0-mlogloss:3.23744\n",
      "[41]\tvalidation_0-mlogloss:3.23003\n",
      "[42]\tvalidation_0-mlogloss:3.22254\n",
      "[43]\tvalidation_0-mlogloss:3.21610\n",
      "[44]\tvalidation_0-mlogloss:3.21043\n",
      "[45]\tvalidation_0-mlogloss:3.20199\n",
      "[46]\tvalidation_0-mlogloss:3.19574\n",
      "[47]\tvalidation_0-mlogloss:3.19010\n",
      "[48]\tvalidation_0-mlogloss:3.18381\n",
      "[49]\tvalidation_0-mlogloss:3.17845\n",
      "[50]\tvalidation_0-mlogloss:3.17277\n",
      "[51]\tvalidation_0-mlogloss:3.16708\n",
      "[52]\tvalidation_0-mlogloss:3.16138\n",
      "[53]\tvalidation_0-mlogloss:3.15631\n",
      "[54]\tvalidation_0-mlogloss:3.15177\n",
      "[55]\tvalidation_0-mlogloss:3.14569\n",
      "[56]\tvalidation_0-mlogloss:3.13978\n",
      "[57]\tvalidation_0-mlogloss:3.13425\n",
      "[58]\tvalidation_0-mlogloss:3.12926\n",
      "[59]\tvalidation_0-mlogloss:3.12391\n",
      "[60]\tvalidation_0-mlogloss:3.11881\n",
      "[61]\tvalidation_0-mlogloss:3.11389\n",
      "[62]\tvalidation_0-mlogloss:3.10922\n",
      "[63]\tvalidation_0-mlogloss:3.10407\n",
      "[64]\tvalidation_0-mlogloss:3.09790\n",
      "[65]\tvalidation_0-mlogloss:3.09322\n",
      "[66]\tvalidation_0-mlogloss:3.08915\n",
      "[67]\tvalidation_0-mlogloss:3.08539\n",
      "[68]\tvalidation_0-mlogloss:3.08169\n",
      "[69]\tvalidation_0-mlogloss:3.07766\n",
      "[70]\tvalidation_0-mlogloss:3.07458\n",
      "[71]\tvalidation_0-mlogloss:3.07098\n",
      "[72]\tvalidation_0-mlogloss:3.06745\n",
      "[73]\tvalidation_0-mlogloss:3.06382\n",
      "[74]\tvalidation_0-mlogloss:3.05991\n",
      "[75]\tvalidation_0-mlogloss:3.05597\n",
      "[76]\tvalidation_0-mlogloss:3.05205\n",
      "[77]\tvalidation_0-mlogloss:3.04823\n",
      "[78]\tvalidation_0-mlogloss:3.04391\n",
      "[79]\tvalidation_0-mlogloss:3.03978\n",
      "[80]\tvalidation_0-mlogloss:3.03562\n",
      "[81]\tvalidation_0-mlogloss:3.03119\n",
      "[82]\tvalidation_0-mlogloss:3.02823\n",
      "[83]\tvalidation_0-mlogloss:3.02456\n",
      "[84]\tvalidation_0-mlogloss:3.02097\n",
      "[85]\tvalidation_0-mlogloss:3.01800\n",
      "[86]\tvalidation_0-mlogloss:3.01490\n",
      "[87]\tvalidation_0-mlogloss:3.01188\n",
      "[88]\tvalidation_0-mlogloss:3.00884\n",
      "[89]\tvalidation_0-mlogloss:3.00676\n",
      "[90]\tvalidation_0-mlogloss:3.00390\n",
      "[91]\tvalidation_0-mlogloss:3.00074\n",
      "[92]\tvalidation_0-mlogloss:2.99735\n",
      "[93]\tvalidation_0-mlogloss:2.99442\n",
      "[94]\tvalidation_0-mlogloss:2.99156\n",
      "[95]\tvalidation_0-mlogloss:2.98918\n",
      "[96]\tvalidation_0-mlogloss:2.98612\n",
      "[97]\tvalidation_0-mlogloss:2.98266\n",
      "[98]\tvalidation_0-mlogloss:2.98003\n",
      "[99]\tvalidation_0-mlogloss:2.97750\n",
      "[100]\tvalidation_0-mlogloss:2.97503\n",
      "[101]\tvalidation_0-mlogloss:2.97267\n",
      "[102]\tvalidation_0-mlogloss:2.97006\n",
      "[103]\tvalidation_0-mlogloss:2.96717\n",
      "[104]\tvalidation_0-mlogloss:2.96393\n",
      "[105]\tvalidation_0-mlogloss:2.96070\n",
      "[106]\tvalidation_0-mlogloss:2.95790\n",
      "[107]\tvalidation_0-mlogloss:2.95499\n",
      "[108]\tvalidation_0-mlogloss:2.95180\n",
      "[109]\tvalidation_0-mlogloss:2.94985\n",
      "[110]\tvalidation_0-mlogloss:2.94739\n",
      "[111]\tvalidation_0-mlogloss:2.94488\n",
      "[112]\tvalidation_0-mlogloss:2.94216\n",
      "[113]\tvalidation_0-mlogloss:2.93984\n",
      "[114]\tvalidation_0-mlogloss:2.93634\n",
      "[115]\tvalidation_0-mlogloss:2.93361\n",
      "[116]\tvalidation_0-mlogloss:2.93122\n",
      "[117]\tvalidation_0-mlogloss:2.92875\n",
      "[118]\tvalidation_0-mlogloss:2.92583\n",
      "[119]\tvalidation_0-mlogloss:2.92252\n",
      "[120]\tvalidation_0-mlogloss:2.91975\n",
      "[121]\tvalidation_0-mlogloss:2.91688\n",
      "[122]\tvalidation_0-mlogloss:2.91482\n",
      "[123]\tvalidation_0-mlogloss:2.91315\n",
      "[124]\tvalidation_0-mlogloss:2.91111\n",
      "[125]\tvalidation_0-mlogloss:2.90863\n",
      "[126]\tvalidation_0-mlogloss:2.90657\n",
      "[127]\tvalidation_0-mlogloss:2.90509\n",
      "[128]\tvalidation_0-mlogloss:2.90233\n",
      "[129]\tvalidation_0-mlogloss:2.90026\n",
      "[130]\tvalidation_0-mlogloss:2.89815\n",
      "[131]\tvalidation_0-mlogloss:2.89555\n",
      "[132]\tvalidation_0-mlogloss:2.89343\n",
      "[133]\tvalidation_0-mlogloss:2.89107\n",
      "[134]\tvalidation_0-mlogloss:2.88924\n",
      "[135]\tvalidation_0-mlogloss:2.88679\n",
      "[136]\tvalidation_0-mlogloss:2.88490\n",
      "[137]\tvalidation_0-mlogloss:2.88342\n",
      "[138]\tvalidation_0-mlogloss:2.88167\n",
      "[139]\tvalidation_0-mlogloss:2.88007\n",
      "[140]\tvalidation_0-mlogloss:2.87809\n",
      "[141]\tvalidation_0-mlogloss:2.87648\n",
      "[142]\tvalidation_0-mlogloss:2.87488\n",
      "[143]\tvalidation_0-mlogloss:2.87365\n",
      "[144]\tvalidation_0-mlogloss:2.87224\n",
      "[145]\tvalidation_0-mlogloss:2.87035\n",
      "[146]\tvalidation_0-mlogloss:2.86912\n",
      "[147]\tvalidation_0-mlogloss:2.86787\n",
      "[148]\tvalidation_0-mlogloss:2.86674\n",
      "[149]\tvalidation_0-mlogloss:2.86519\n",
      "[150]\tvalidation_0-mlogloss:2.86328\n",
      "[151]\tvalidation_0-mlogloss:2.86190\n",
      "[152]\tvalidation_0-mlogloss:2.86047\n",
      "[153]\tvalidation_0-mlogloss:2.85823\n",
      "[154]\tvalidation_0-mlogloss:2.85663\n",
      "[155]\tvalidation_0-mlogloss:2.85523\n",
      "[156]\tvalidation_0-mlogloss:2.85379\n",
      "[157]\tvalidation_0-mlogloss:2.85139\n",
      "[158]\tvalidation_0-mlogloss:2.85003\n",
      "[159]\tvalidation_0-mlogloss:2.84901\n",
      "[160]\tvalidation_0-mlogloss:2.84779\n",
      "[161]\tvalidation_0-mlogloss:2.84670\n",
      "[162]\tvalidation_0-mlogloss:2.84485\n",
      "[163]\tvalidation_0-mlogloss:2.84397\n",
      "[164]\tvalidation_0-mlogloss:2.84258\n",
      "[165]\tvalidation_0-mlogloss:2.84143\n",
      "[166]\tvalidation_0-mlogloss:2.84041\n",
      "[167]\tvalidation_0-mlogloss:2.83989\n",
      "[168]\tvalidation_0-mlogloss:2.83801\n",
      "[169]\tvalidation_0-mlogloss:2.83688\n",
      "[170]\tvalidation_0-mlogloss:2.83554\n",
      "[171]\tvalidation_0-mlogloss:2.83425\n",
      "[172]\tvalidation_0-mlogloss:2.83313\n",
      "[173]\tvalidation_0-mlogloss:2.83138\n",
      "[174]\tvalidation_0-mlogloss:2.82989\n",
      "[175]\tvalidation_0-mlogloss:2.82807\n",
      "[176]\tvalidation_0-mlogloss:2.82685\n",
      "[177]\tvalidation_0-mlogloss:2.82567\n",
      "[178]\tvalidation_0-mlogloss:2.82444\n",
      "[179]\tvalidation_0-mlogloss:2.82327\n",
      "[180]\tvalidation_0-mlogloss:2.82242\n",
      "[181]\tvalidation_0-mlogloss:2.82154\n",
      "[182]\tvalidation_0-mlogloss:2.81988\n",
      "[183]\tvalidation_0-mlogloss:2.81871\n",
      "[184]\tvalidation_0-mlogloss:2.81780\n",
      "[185]\tvalidation_0-mlogloss:2.81656\n",
      "[186]\tvalidation_0-mlogloss:2.81554\n",
      "[187]\tvalidation_0-mlogloss:2.81423\n",
      "[188]\tvalidation_0-mlogloss:2.81345\n",
      "[189]\tvalidation_0-mlogloss:2.81231\n",
      "[190]\tvalidation_0-mlogloss:2.81151\n",
      "[191]\tvalidation_0-mlogloss:2.81055\n",
      "[192]\tvalidation_0-mlogloss:2.80956\n",
      "[193]\tvalidation_0-mlogloss:2.80805\n",
      "[194]\tvalidation_0-mlogloss:2.80715\n",
      "[195]\tvalidation_0-mlogloss:2.80643\n",
      "[196]\tvalidation_0-mlogloss:2.80574\n",
      "[197]\tvalidation_0-mlogloss:2.80481\n",
      "[198]\tvalidation_0-mlogloss:2.80350\n",
      "[199]\tvalidation_0-mlogloss:2.80247\n",
      "[200]\tvalidation_0-mlogloss:2.80168\n",
      "[201]\tvalidation_0-mlogloss:2.80037\n",
      "[202]\tvalidation_0-mlogloss:2.79889\n",
      "[203]\tvalidation_0-mlogloss:2.79661\n",
      "[204]\tvalidation_0-mlogloss:2.79531\n",
      "[205]\tvalidation_0-mlogloss:2.79428\n",
      "[206]\tvalidation_0-mlogloss:2.79345\n",
      "[207]\tvalidation_0-mlogloss:2.79208\n",
      "[208]\tvalidation_0-mlogloss:2.79121\n",
      "[209]\tvalidation_0-mlogloss:2.79019\n",
      "[210]\tvalidation_0-mlogloss:2.78905\n",
      "[211]\tvalidation_0-mlogloss:2.78776\n",
      "[212]\tvalidation_0-mlogloss:2.78695\n",
      "[213]\tvalidation_0-mlogloss:2.78575\n",
      "[214]\tvalidation_0-mlogloss:2.78434\n",
      "[215]\tvalidation_0-mlogloss:2.78351\n",
      "[216]\tvalidation_0-mlogloss:2.78228\n",
      "[217]\tvalidation_0-mlogloss:2.78129\n",
      "[218]\tvalidation_0-mlogloss:2.78038\n",
      "[219]\tvalidation_0-mlogloss:2.77984\n",
      "[220]\tvalidation_0-mlogloss:2.77913\n",
      "[221]\tvalidation_0-mlogloss:2.77836\n",
      "[222]\tvalidation_0-mlogloss:2.77774\n",
      "[223]\tvalidation_0-mlogloss:2.77688\n",
      "[224]\tvalidation_0-mlogloss:2.77626\n",
      "[225]\tvalidation_0-mlogloss:2.77575\n",
      "[226]\tvalidation_0-mlogloss:2.77515\n",
      "[227]\tvalidation_0-mlogloss:2.77472\n",
      "[228]\tvalidation_0-mlogloss:2.77435\n",
      "[229]\tvalidation_0-mlogloss:2.77386\n",
      "[230]\tvalidation_0-mlogloss:2.77331\n",
      "[231]\tvalidation_0-mlogloss:2.77311\n",
      "[232]\tvalidation_0-mlogloss:2.77249\n",
      "[233]\tvalidation_0-mlogloss:2.77178\n",
      "[234]\tvalidation_0-mlogloss:2.77132\n",
      "[235]\tvalidation_0-mlogloss:2.77105\n",
      "[236]\tvalidation_0-mlogloss:2.76999\n",
      "[237]\tvalidation_0-mlogloss:2.76961\n",
      "[238]\tvalidation_0-mlogloss:2.76887\n",
      "[239]\tvalidation_0-mlogloss:2.76834\n",
      "[240]\tvalidation_0-mlogloss:2.76737\n",
      "[241]\tvalidation_0-mlogloss:2.76683\n",
      "[242]\tvalidation_0-mlogloss:2.76604\n",
      "[243]\tvalidation_0-mlogloss:2.76513\n",
      "[244]\tvalidation_0-mlogloss:2.76450\n",
      "[245]\tvalidation_0-mlogloss:2.76388\n",
      "[246]\tvalidation_0-mlogloss:2.76324\n",
      "[247]\tvalidation_0-mlogloss:2.76267\n",
      "[248]\tvalidation_0-mlogloss:2.76191\n",
      "[249]\tvalidation_0-mlogloss:2.76098\n",
      "[250]\tvalidation_0-mlogloss:2.76016\n",
      "[251]\tvalidation_0-mlogloss:2.75979\n",
      "[252]\tvalidation_0-mlogloss:2.75934\n",
      "[253]\tvalidation_0-mlogloss:2.75876\n",
      "[254]\tvalidation_0-mlogloss:2.75801\n",
      "[255]\tvalidation_0-mlogloss:2.75773\n",
      "[256]\tvalidation_0-mlogloss:2.75679\n",
      "[257]\tvalidation_0-mlogloss:2.75649\n",
      "[258]\tvalidation_0-mlogloss:2.75579\n",
      "[259]\tvalidation_0-mlogloss:2.75531\n",
      "[260]\tvalidation_0-mlogloss:2.75459\n",
      "[261]\tvalidation_0-mlogloss:2.75340\n",
      "[262]\tvalidation_0-mlogloss:2.75290\n",
      "[263]\tvalidation_0-mlogloss:2.75236\n",
      "[264]\tvalidation_0-mlogloss:2.75181\n",
      "[265]\tvalidation_0-mlogloss:2.75147\n",
      "[266]\tvalidation_0-mlogloss:2.75117\n",
      "[267]\tvalidation_0-mlogloss:2.75027\n",
      "[268]\tvalidation_0-mlogloss:2.74936\n",
      "[269]\tvalidation_0-mlogloss:2.74869\n",
      "[270]\tvalidation_0-mlogloss:2.74836\n",
      "[271]\tvalidation_0-mlogloss:2.74790\n",
      "[272]\tvalidation_0-mlogloss:2.74709\n",
      "[273]\tvalidation_0-mlogloss:2.74654\n",
      "[274]\tvalidation_0-mlogloss:2.74565\n",
      "[275]\tvalidation_0-mlogloss:2.74523\n",
      "[276]\tvalidation_0-mlogloss:2.74439\n",
      "[277]\tvalidation_0-mlogloss:2.74401\n",
      "[278]\tvalidation_0-mlogloss:2.74348\n",
      "[279]\tvalidation_0-mlogloss:2.74302\n",
      "[280]\tvalidation_0-mlogloss:2.74275\n",
      "[281]\tvalidation_0-mlogloss:2.74196\n",
      "[282]\tvalidation_0-mlogloss:2.74131\n",
      "[283]\tvalidation_0-mlogloss:2.74122\n",
      "[284]\tvalidation_0-mlogloss:2.74083\n",
      "[285]\tvalidation_0-mlogloss:2.74011\n",
      "[286]\tvalidation_0-mlogloss:2.73961\n",
      "[287]\tvalidation_0-mlogloss:2.73915\n",
      "[288]\tvalidation_0-mlogloss:2.73925\n",
      "[289]\tvalidation_0-mlogloss:2.73873\n",
      "[290]\tvalidation_0-mlogloss:2.73814\n",
      "[291]\tvalidation_0-mlogloss:2.73809\n",
      "[292]\tvalidation_0-mlogloss:2.73802\n",
      "[293]\tvalidation_0-mlogloss:2.73739\n",
      "[294]\tvalidation_0-mlogloss:2.73701\n",
      "[295]\tvalidation_0-mlogloss:2.73632\n",
      "[296]\tvalidation_0-mlogloss:2.73562\n",
      "[297]\tvalidation_0-mlogloss:2.73487\n",
      "[298]\tvalidation_0-mlogloss:2.73450\n",
      "[299]\tvalidation_0-mlogloss:2.73375\n",
      "[300]\tvalidation_0-mlogloss:2.73291\n",
      "[301]\tvalidation_0-mlogloss:2.73240\n",
      "[302]\tvalidation_0-mlogloss:2.73230\n",
      "[303]\tvalidation_0-mlogloss:2.73174\n",
      "[304]\tvalidation_0-mlogloss:2.73156\n",
      "[305]\tvalidation_0-mlogloss:2.73082\n",
      "[306]\tvalidation_0-mlogloss:2.73090\n",
      "[307]\tvalidation_0-mlogloss:2.73002\n",
      "[308]\tvalidation_0-mlogloss:2.72926\n",
      "[309]\tvalidation_0-mlogloss:2.72879\n",
      "[310]\tvalidation_0-mlogloss:2.72853\n",
      "[311]\tvalidation_0-mlogloss:2.72795\n",
      "[312]\tvalidation_0-mlogloss:2.72785\n",
      "[313]\tvalidation_0-mlogloss:2.72750\n",
      "[314]\tvalidation_0-mlogloss:2.72741\n",
      "[315]\tvalidation_0-mlogloss:2.72704\n",
      "[316]\tvalidation_0-mlogloss:2.72645\n",
      "[317]\tvalidation_0-mlogloss:2.72632\n",
      "[318]\tvalidation_0-mlogloss:2.72610\n",
      "[319]\tvalidation_0-mlogloss:2.72583\n",
      "[320]\tvalidation_0-mlogloss:2.72533\n",
      "[321]\tvalidation_0-mlogloss:2.72537\n",
      "[322]\tvalidation_0-mlogloss:2.72498\n",
      "[323]\tvalidation_0-mlogloss:2.72429\n",
      "[324]\tvalidation_0-mlogloss:2.72401\n",
      "[325]\tvalidation_0-mlogloss:2.72338\n",
      "[326]\tvalidation_0-mlogloss:2.72301\n",
      "[327]\tvalidation_0-mlogloss:2.72235\n",
      "[328]\tvalidation_0-mlogloss:2.72204\n",
      "[329]\tvalidation_0-mlogloss:2.72176\n",
      "[330]\tvalidation_0-mlogloss:2.72116\n",
      "[331]\tvalidation_0-mlogloss:2.72069\n",
      "[332]\tvalidation_0-mlogloss:2.72042\n",
      "[333]\tvalidation_0-mlogloss:2.72030\n",
      "[334]\tvalidation_0-mlogloss:2.71962\n",
      "[335]\tvalidation_0-mlogloss:2.71956\n",
      "[336]\tvalidation_0-mlogloss:2.71913\n",
      "[337]\tvalidation_0-mlogloss:2.71873\n",
      "[338]\tvalidation_0-mlogloss:2.71825\n",
      "[339]\tvalidation_0-mlogloss:2.71808\n",
      "[340]\tvalidation_0-mlogloss:2.71777\n",
      "[341]\tvalidation_0-mlogloss:2.71739\n",
      "[342]\tvalidation_0-mlogloss:2.71713\n",
      "[343]\tvalidation_0-mlogloss:2.71665\n",
      "[344]\tvalidation_0-mlogloss:2.71621\n",
      "[345]\tvalidation_0-mlogloss:2.71600\n",
      "[346]\tvalidation_0-mlogloss:2.71572\n",
      "[347]\tvalidation_0-mlogloss:2.71554\n",
      "[348]\tvalidation_0-mlogloss:2.71543\n",
      "[349]\tvalidation_0-mlogloss:2.71514\n",
      "[350]\tvalidation_0-mlogloss:2.71484\n",
      "[351]\tvalidation_0-mlogloss:2.71458\n",
      "[352]\tvalidation_0-mlogloss:2.71433\n",
      "[353]\tvalidation_0-mlogloss:2.71411\n",
      "[354]\tvalidation_0-mlogloss:2.71332\n",
      "[355]\tvalidation_0-mlogloss:2.71304\n",
      "[356]\tvalidation_0-mlogloss:2.71275\n",
      "[357]\tvalidation_0-mlogloss:2.71221\n",
      "[358]\tvalidation_0-mlogloss:2.71186\n",
      "[359]\tvalidation_0-mlogloss:2.71171\n",
      "[360]\tvalidation_0-mlogloss:2.71157\n",
      "[361]\tvalidation_0-mlogloss:2.71133\n",
      "[362]\tvalidation_0-mlogloss:2.71130\n",
      "[363]\tvalidation_0-mlogloss:2.71121\n",
      "[364]\tvalidation_0-mlogloss:2.71124\n",
      "[365]\tvalidation_0-mlogloss:2.71086\n",
      "[366]\tvalidation_0-mlogloss:2.71055\n",
      "[367]\tvalidation_0-mlogloss:2.71033\n",
      "[368]\tvalidation_0-mlogloss:2.71032\n",
      "[369]\tvalidation_0-mlogloss:2.71015\n",
      "[370]\tvalidation_0-mlogloss:2.70969\n",
      "[371]\tvalidation_0-mlogloss:2.70957\n",
      "[372]\tvalidation_0-mlogloss:2.70912\n",
      "[373]\tvalidation_0-mlogloss:2.70915\n",
      "[374]\tvalidation_0-mlogloss:2.70902\n",
      "[375]\tvalidation_0-mlogloss:2.70899\n",
      "[376]\tvalidation_0-mlogloss:2.70860\n",
      "[377]\tvalidation_0-mlogloss:2.70829\n",
      "[378]\tvalidation_0-mlogloss:2.70801\n",
      "[379]\tvalidation_0-mlogloss:2.70760\n",
      "[380]\tvalidation_0-mlogloss:2.70726\n",
      "[381]\tvalidation_0-mlogloss:2.70705\n",
      "[382]\tvalidation_0-mlogloss:2.70719\n",
      "[383]\tvalidation_0-mlogloss:2.70689\n",
      "[384]\tvalidation_0-mlogloss:2.70667\n",
      "[385]\tvalidation_0-mlogloss:2.70643\n",
      "[386]\tvalidation_0-mlogloss:2.70625\n",
      "[387]\tvalidation_0-mlogloss:2.70621\n",
      "[388]\tvalidation_0-mlogloss:2.70605\n",
      "[389]\tvalidation_0-mlogloss:2.70598\n",
      "[390]\tvalidation_0-mlogloss:2.70562\n",
      "[391]\tvalidation_0-mlogloss:2.70554\n",
      "[392]\tvalidation_0-mlogloss:2.70564\n",
      "[393]\tvalidation_0-mlogloss:2.70570\n",
      "[394]\tvalidation_0-mlogloss:2.70536\n",
      "[395]\tvalidation_0-mlogloss:2.70535\n",
      "[396]\tvalidation_0-mlogloss:2.70513\n",
      "[397]\tvalidation_0-mlogloss:2.70538\n",
      "[398]\tvalidation_0-mlogloss:2.70562\n",
      "[399]\tvalidation_0-mlogloss:2.70569\n",
      "[400]\tvalidation_0-mlogloss:2.70589\n",
      "[401]\tvalidation_0-mlogloss:2.70593\n",
      "[402]\tvalidation_0-mlogloss:2.70587\n",
      "[403]\tvalidation_0-mlogloss:2.70547\n",
      "[404]\tvalidation_0-mlogloss:2.70556\n",
      "[405]\tvalidation_0-mlogloss:2.70529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.09      0.12      1499\n",
      "           1       0.15      0.13      0.14      1499\n",
      "           2       0.25      0.22      0.23      1499\n",
      "           3       0.45      0.49      0.47      1499\n",
      "           4       0.64      0.68      0.66      1499\n",
      "           5       0.22      0.31      0.26      1499\n",
      "           6       0.31      0.50      0.38      1499\n",
      "           7       0.49      0.41      0.45      1499\n",
      "           8       0.45      0.61      0.52      1499\n",
      "           9       0.44      0.33      0.38      1499\n",
      "          10       0.26      0.31      0.29      1499\n",
      "          11       0.16      0.13      0.14      1499\n",
      "          12       0.27      0.24      0.25      1499\n",
      "          13       0.13      0.09      0.11      1499\n",
      "          14       0.17      0.09      0.12      1499\n",
      "          15       0.40      0.31      0.35      1499\n",
      "          16       0.03      0.02      0.02      1499\n",
      "          17       0.37      0.67      0.48      1499\n",
      "          18       0.16      0.11      0.13      1499\n",
      "          19       0.12      0.09      0.10      1499\n",
      "          20       0.51      0.63      0.56      1499\n",
      "          21       0.38      0.35      0.37      1499\n",
      "          22       0.14      0.10      0.11      1499\n",
      "          23       0.41      0.53      0.46      1499\n",
      "          24       0.59      0.73      0.65      1499\n",
      "          25       0.14      0.16      0.15      1499\n",
      "          26       0.45      0.46      0.45      1499\n",
      "          27       0.15      0.12      0.13      1499\n",
      "          28       0.18      0.17      0.18      1499\n",
      "          29       0.29      0.39      0.33      1499\n",
      "          30       0.16      0.15      0.15      1499\n",
      "          31       0.11      0.16      0.13      1499\n",
      "          32       0.21      0.12      0.15      1499\n",
      "          33       0.10      0.07      0.08      1499\n",
      "          34       0.45      0.56      0.50      1499\n",
      "          35       0.43      0.61      0.51      1499\n",
      "          36       0.28      0.22      0.24      1499\n",
      "          37       0.17      0.15      0.16      1499\n",
      "          38       0.10      0.07      0.08      1499\n",
      "          39       0.39      0.35      0.36      1499\n",
      "          40       0.25      0.22      0.23      1499\n",
      "          41       0.33      0.52      0.40      1499\n",
      "          42       0.22      0.21      0.21      1499\n",
      "          43       0.31      0.34      0.32      1499\n",
      "          44       0.47      0.67      0.56      1499\n",
      "          45       0.06      0.04      0.04      1499\n",
      "          46       0.30      0.48      0.37      1499\n",
      "          47       0.29      0.24      0.26      1499\n",
      "          48       0.47      0.69      0.56      1499\n",
      "          49       0.22      0.16      0.18      1499\n",
      "          50       0.32      0.45      0.37      1499\n",
      "          51       0.20      0.17      0.18      1499\n",
      "          52       0.14      0.07      0.09      1499\n",
      "          53       0.28      0.28      0.28      1499\n",
      "          54       0.25      0.40      0.31      1499\n",
      "          55       0.30      0.35      0.32      1499\n",
      "          56       0.32      0.32      0.32      1499\n",
      "          57       0.33      0.41      0.36      1499\n",
      "          58       0.82      0.87      0.85      1499\n",
      "          59       0.14      0.11      0.12      1499\n",
      "          60       0.32      0.33      0.33      1499\n",
      "          61       0.01      0.00      0.01      1499\n",
      "          62       0.39      0.53      0.45      1499\n",
      "          63       0.15      0.15      0.15      1499\n",
      "          64       0.30      0.29      0.29      1499\n",
      "          65       0.17      0.15      0.16      1499\n",
      "          66       0.47      0.56      0.51      1499\n",
      "          67       0.36      0.16      0.22      1499\n",
      "          68       0.40      0.50      0.45      1499\n",
      "          69       0.18      0.16      0.17      1499\n",
      "          70       0.14      0.07      0.10      1499\n",
      "          71       0.10      0.09      0.10      1499\n",
      "          72       0.16      0.13      0.14      1499\n",
      "          73       0.22      0.23      0.23      1499\n",
      "          74       0.13      0.11      0.12      1499\n",
      "\n",
      "    accuracy                           0.30    112425\n",
      "   macro avg       0.28      0.30      0.28    112425\n",
      "weighted avg       0.28      0.30      0.28    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b181cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 13:16:49.421721: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 28800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28125/28125 [==============================] - 55s 2ms/step - loss: 2.8225 - val_loss: 2.8317\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 48s 2ms/step - loss: 2.3176 - val_loss: 2.6093\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 2.0972 - val_loss: 2.6304\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 55s 2ms/step - loss: 2.0274 - val_loss: 2.6406\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 62s 2ms/step - loss: 1.9869 - val_loss: 2.6239\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 60s 2ms/step - loss: 1.9615 - val_loss: 2.6328\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 1.9420 - val_loss: 2.6870\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 1.9262 - val_loss: 2.6899\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 1.9133 - val_loss: 2.7013\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 1.9034 - val_loss: 2.7297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5077c9ac80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  52/3514 [..............................] - ETA: 3s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 4s 997us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 13:25:44.320311: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 33727500 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.13      0.15      1499\n",
      "           1       0.19      0.10      0.13      1499\n",
      "           2       0.34      0.11      0.17      1499\n",
      "           3       0.66      0.48      0.56      1499\n",
      "           4       0.67      0.64      0.65      1499\n",
      "           5       0.40      0.37      0.38      1499\n",
      "           6       0.42      0.49      0.45      1499\n",
      "           7       0.48      0.46      0.47      1499\n",
      "           8       0.55      0.56      0.56      1499\n",
      "           9       0.54      0.25      0.34      1499\n",
      "          10       0.28      0.42      0.34      1499\n",
      "          11       0.18      0.26      0.22      1499\n",
      "          12       0.28      0.23      0.25      1499\n",
      "          13       0.16      0.11      0.13      1499\n",
      "          14       0.21      0.03      0.05      1499\n",
      "          15       0.41      0.35      0.38      1499\n",
      "          16       0.04      0.03      0.04      1499\n",
      "          17       0.32      0.80      0.46      1499\n",
      "          18       0.15      0.11      0.13      1499\n",
      "          19       0.20      0.07      0.10      1499\n",
      "          20       0.62      0.53      0.57      1499\n",
      "          21       0.38      0.38      0.38      1499\n",
      "          22       0.14      0.07      0.10      1499\n",
      "          23       0.38      0.60      0.46      1499\n",
      "          24       0.67      0.52      0.59      1499\n",
      "          25       0.18      0.17      0.18      1499\n",
      "          26       0.44      0.48      0.46      1499\n",
      "          27       0.16      0.22      0.18      1499\n",
      "          28       0.15      0.18      0.16      1499\n",
      "          29       0.33      0.40      0.36      1499\n",
      "          30       0.23      0.11      0.15      1499\n",
      "          31       0.10      0.15      0.12      1499\n",
      "          32       0.26      0.12      0.17      1499\n",
      "          33       0.10      0.05      0.06      1499\n",
      "          34       0.60      0.47      0.53      1499\n",
      "          35       0.58      0.54      0.56      1499\n",
      "          36       0.38      0.10      0.15      1499\n",
      "          37       0.19      0.22      0.20      1499\n",
      "          38       0.14      0.13      0.13      1499\n",
      "          39       0.30      0.39      0.34      1499\n",
      "          40       0.26      0.20      0.23      1499\n",
      "          41       0.50      0.62      0.56      1499\n",
      "          42       0.25      0.20      0.23      1499\n",
      "          43       0.27      0.43      0.33      1499\n",
      "          44       0.56      0.57      0.56      1499\n",
      "          45       0.08      0.08      0.08      1499\n",
      "          46       0.30      0.65      0.41      1499\n",
      "          47       0.32      0.21      0.25      1499\n",
      "          48       0.38      0.79      0.51      1499\n",
      "          49       0.25      0.21      0.23      1499\n",
      "          50       0.33      0.40      0.36      1499\n",
      "          51       0.27      0.06      0.09      1499\n",
      "          52       0.11      0.08      0.09      1499\n",
      "          53       0.34      0.29      0.31      1499\n",
      "          54       0.30      0.42      0.35      1499\n",
      "          55       0.27      0.49      0.35      1499\n",
      "          56       0.27      0.32      0.29      1499\n",
      "          57       0.40      0.54      0.46      1499\n",
      "          58       0.83      0.94      0.88      1499\n",
      "          59       0.12      0.08      0.10      1499\n",
      "          60       0.30      0.20      0.24      1499\n",
      "          61       0.02      0.01      0.02      1499\n",
      "          62       0.50      0.69      0.58      1499\n",
      "          63       0.16      0.20      0.18      1499\n",
      "          64       0.23      0.34      0.28      1499\n",
      "          65       0.16      0.15      0.16      1499\n",
      "          66       0.54      0.49      0.51      1499\n",
      "          67       0.50      0.34      0.40      1499\n",
      "          68       0.50      0.58      0.54      1499\n",
      "          69       0.17      0.22      0.19      1499\n",
      "          70       0.09      0.03      0.05      1499\n",
      "          71       0.09      0.10      0.10      1499\n",
      "          72       0.11      0.13      0.12      1499\n",
      "          73       0.23      0.36      0.28      1499\n",
      "          74       0.17      0.16      0.16      1499\n",
      "\n",
      "    accuracy                           0.31    112425\n",
      "   macro avg       0.31      0.31      0.30    112425\n",
      "weighted avg       0.31      0.31      0.30    112425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 13:25:44.760401: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 33727500 exceeds 10% of free system memory.\n",
      "2023-10-01 13:25:44.766960: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 33727500 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2632/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_2632/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_2632/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000057  -0.000013  -0.000015  -0.000012  -0.000013  -0.000008   \n",
       "1       -0.000049  -0.000011  -0.000010  -0.000012  -0.000019  -0.000024   \n",
       "2       -0.000055  -0.000017  -0.000016  -0.000019  -0.000024  -0.000029   \n",
       "3       -0.000073  -0.000042  -0.000040  -0.000037  -0.000037  -0.000040   \n",
       "4       -0.000087  -0.000053  -0.000052  -0.000051  -0.000045  -0.000043   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995  -0.000069  -0.000027  -0.000012   0.000014   0.000003  -0.000085   \n",
       "899996  -0.000083  -0.000031  -0.000012   0.000024   0.000006  -0.000078   \n",
       "899997  -0.000001  -0.000066  -0.000030   0.000001  -0.000022  -0.000072   \n",
       "899998  -0.000065  -0.000056  -0.000013   0.000015  -0.000005  -0.000077   \n",
       "899999  -0.000070  -0.000065  -0.000040  -0.000011  -0.000029  -0.000081   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000040  -0.000054  -0.000012  -0.000014  ...   -0.000048   \n",
       "1       -0.000058  -0.000051  -0.000019  -0.000023  ...   -0.000055   \n",
       "2       -0.000066  -0.000061  -0.000030  -0.000036  ...   -0.000054   \n",
       "3       -0.000071  -0.000078  -0.000053  -0.000053  ...   -0.000065   \n",
       "4       -0.000071  -0.000087  -0.000065  -0.000064  ...   -0.000075   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995   0.000053  -0.000061  -0.000040  -0.000025  ...    0.000008   \n",
       "899996   0.000030  -0.000059  -0.000030  -0.000021  ...   -0.000024   \n",
       "899997  -0.000006  -0.000043  -0.000043  -0.000039  ...    0.000013   \n",
       "899998   0.000008  -0.000069  -0.000028  -0.000022  ...   -0.000033   \n",
       "899999   0.000006  -0.000100  -0.000059  -0.000052  ...   -0.000072   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000038   -0.000042   -0.000068   -0.000076   -0.000103   \n",
       "1        -0.000055   -0.000063   -0.000082   -0.000087   -0.000099   \n",
       "2        -0.000063   -0.000072   -0.000091   -0.000092   -0.000091   \n",
       "3        -0.000052   -0.000066   -0.000100   -0.000105   -0.000105   \n",
       "4        -0.000082   -0.000090   -0.000117   -0.000119   -0.000118   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995   -0.000014    0.000031    0.000005   -0.000013    0.000002   \n",
       "899996   -0.000029    0.000003   -0.000020   -0.000035   -0.000036   \n",
       "899997    0.000001    0.000023   -0.000005   -0.000025   -0.000016   \n",
       "899998   -0.000025   -0.000007   -0.000031   -0.000048   -0.000048   \n",
       "899999   -0.000095   -0.000090   -0.000100   -0.000092   -0.000077   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000051   -0.000056   -0.000124   -0.000028  \n",
       "1        -0.000059   -0.000070   -0.000149   -0.000040  \n",
       "2        -0.000067   -0.000077   -0.000153   -0.000037  \n",
       "3        -0.000067   -0.000072   -0.000148   -0.000026  \n",
       "4        -0.000075   -0.000082   -0.000161   -0.000035  \n",
       "...            ...         ...         ...         ...  \n",
       "899995    0.000021    0.000050   -0.000034   -0.000002  \n",
       "899996    0.000001    0.000025   -0.000039    0.000013  \n",
       "899997    0.000022    0.000055   -0.000040    0.000014  \n",
       "899998   -0.000012    0.000034   -0.000044   -0.000002  \n",
       "899999   -0.000078   -0.000010   -0.000065   -0.000025  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:4.01357\n",
      "[1]\tvalidation_0-mlogloss:3.88020\n",
      "[2]\tvalidation_0-mlogloss:3.77775\n",
      "[3]\tvalidation_0-mlogloss:3.69507\n",
      "[4]\tvalidation_0-mlogloss:3.62790\n",
      "[5]\tvalidation_0-mlogloss:3.57138\n",
      "[6]\tvalidation_0-mlogloss:3.52469\n",
      "[7]\tvalidation_0-mlogloss:3.48014\n",
      "[8]\tvalidation_0-mlogloss:3.44108\n",
      "[9]\tvalidation_0-mlogloss:3.40157\n",
      "[10]\tvalidation_0-mlogloss:3.36797\n",
      "[11]\tvalidation_0-mlogloss:3.34074\n",
      "[12]\tvalidation_0-mlogloss:3.31621\n",
      "[13]\tvalidation_0-mlogloss:3.28974\n",
      "[14]\tvalidation_0-mlogloss:3.26823\n",
      "[15]\tvalidation_0-mlogloss:3.24714\n",
      "[16]\tvalidation_0-mlogloss:3.22512\n",
      "[17]\tvalidation_0-mlogloss:3.20676\n",
      "[18]\tvalidation_0-mlogloss:3.18883\n",
      "[19]\tvalidation_0-mlogloss:3.17265\n",
      "[20]\tvalidation_0-mlogloss:3.15428\n",
      "[21]\tvalidation_0-mlogloss:3.14169\n",
      "[22]\tvalidation_0-mlogloss:3.12612\n",
      "[23]\tvalidation_0-mlogloss:3.10928\n",
      "[24]\tvalidation_0-mlogloss:3.09533\n",
      "[25]\tvalidation_0-mlogloss:3.08151\n",
      "[26]\tvalidation_0-mlogloss:3.06733\n",
      "[27]\tvalidation_0-mlogloss:3.05570\n",
      "[28]\tvalidation_0-mlogloss:3.04175\n",
      "[29]\tvalidation_0-mlogloss:3.02835\n",
      "[30]\tvalidation_0-mlogloss:3.01667\n",
      "[31]\tvalidation_0-mlogloss:3.00582\n",
      "[32]\tvalidation_0-mlogloss:2.99331\n",
      "[33]\tvalidation_0-mlogloss:2.98052\n",
      "[34]\tvalidation_0-mlogloss:2.97081\n",
      "[35]\tvalidation_0-mlogloss:2.95943\n",
      "[36]\tvalidation_0-mlogloss:2.94866\n",
      "[37]\tvalidation_0-mlogloss:2.93999\n",
      "[38]\tvalidation_0-mlogloss:2.93083\n",
      "[39]\tvalidation_0-mlogloss:2.92168\n",
      "[40]\tvalidation_0-mlogloss:2.91167\n",
      "[41]\tvalidation_0-mlogloss:2.90458\n",
      "[42]\tvalidation_0-mlogloss:2.89522\n",
      "[43]\tvalidation_0-mlogloss:2.88536\n",
      "[44]\tvalidation_0-mlogloss:2.87603\n",
      "[45]\tvalidation_0-mlogloss:2.86684\n",
      "[46]\tvalidation_0-mlogloss:2.85790\n",
      "[47]\tvalidation_0-mlogloss:2.84752\n",
      "[48]\tvalidation_0-mlogloss:2.83933\n",
      "[49]\tvalidation_0-mlogloss:2.83314\n",
      "[50]\tvalidation_0-mlogloss:2.82598\n",
      "[51]\tvalidation_0-mlogloss:2.81944\n",
      "[52]\tvalidation_0-mlogloss:2.81175\n",
      "[53]\tvalidation_0-mlogloss:2.80475\n",
      "[54]\tvalidation_0-mlogloss:2.79780\n",
      "[55]\tvalidation_0-mlogloss:2.79214\n",
      "[56]\tvalidation_0-mlogloss:2.78588\n",
      "[57]\tvalidation_0-mlogloss:2.77854\n",
      "[58]\tvalidation_0-mlogloss:2.77160\n",
      "[59]\tvalidation_0-mlogloss:2.76679\n",
      "[60]\tvalidation_0-mlogloss:2.75951\n",
      "[61]\tvalidation_0-mlogloss:2.75333\n",
      "[62]\tvalidation_0-mlogloss:2.74697\n",
      "[63]\tvalidation_0-mlogloss:2.74094\n",
      "[64]\tvalidation_0-mlogloss:2.73383\n",
      "[65]\tvalidation_0-mlogloss:2.72773\n",
      "[66]\tvalidation_0-mlogloss:2.72047\n",
      "[67]\tvalidation_0-mlogloss:2.71289\n",
      "[68]\tvalidation_0-mlogloss:2.70733\n",
      "[69]\tvalidation_0-mlogloss:2.70166\n",
      "[70]\tvalidation_0-mlogloss:2.69641\n",
      "[71]\tvalidation_0-mlogloss:2.69228\n",
      "[72]\tvalidation_0-mlogloss:2.68746\n",
      "[73]\tvalidation_0-mlogloss:2.68291\n",
      "[74]\tvalidation_0-mlogloss:2.67727\n",
      "[75]\tvalidation_0-mlogloss:2.67309\n",
      "[76]\tvalidation_0-mlogloss:2.66889\n",
      "[77]\tvalidation_0-mlogloss:2.66436\n",
      "[78]\tvalidation_0-mlogloss:2.66075\n",
      "[79]\tvalidation_0-mlogloss:2.65620\n",
      "[80]\tvalidation_0-mlogloss:2.65337\n",
      "[81]\tvalidation_0-mlogloss:2.64792\n",
      "[82]\tvalidation_0-mlogloss:2.64265\n",
      "[83]\tvalidation_0-mlogloss:2.63791\n",
      "[84]\tvalidation_0-mlogloss:2.63260\n",
      "[85]\tvalidation_0-mlogloss:2.62869\n",
      "[86]\tvalidation_0-mlogloss:2.62372\n",
      "[87]\tvalidation_0-mlogloss:2.61959\n",
      "[88]\tvalidation_0-mlogloss:2.61532\n",
      "[89]\tvalidation_0-mlogloss:2.61239\n",
      "[90]\tvalidation_0-mlogloss:2.60839\n",
      "[91]\tvalidation_0-mlogloss:2.60344\n",
      "[92]\tvalidation_0-mlogloss:2.59983\n",
      "[93]\tvalidation_0-mlogloss:2.59629\n",
      "[94]\tvalidation_0-mlogloss:2.59336\n",
      "[95]\tvalidation_0-mlogloss:2.58951\n",
      "[96]\tvalidation_0-mlogloss:2.58520\n",
      "[97]\tvalidation_0-mlogloss:2.58018\n",
      "[98]\tvalidation_0-mlogloss:2.57552\n",
      "[99]\tvalidation_0-mlogloss:2.57224\n",
      "[100]\tvalidation_0-mlogloss:2.56851\n",
      "[101]\tvalidation_0-mlogloss:2.56463\n",
      "[102]\tvalidation_0-mlogloss:2.56136\n",
      "[103]\tvalidation_0-mlogloss:2.55907\n",
      "[104]\tvalidation_0-mlogloss:2.55537\n",
      "[105]\tvalidation_0-mlogloss:2.55154\n",
      "[106]\tvalidation_0-mlogloss:2.54851\n",
      "[107]\tvalidation_0-mlogloss:2.54561\n",
      "[108]\tvalidation_0-mlogloss:2.54264\n",
      "[109]\tvalidation_0-mlogloss:2.54061\n",
      "[110]\tvalidation_0-mlogloss:2.53720\n",
      "[111]\tvalidation_0-mlogloss:2.53335\n",
      "[112]\tvalidation_0-mlogloss:2.52938\n",
      "[113]\tvalidation_0-mlogloss:2.52540\n",
      "[114]\tvalidation_0-mlogloss:2.52196\n",
      "[115]\tvalidation_0-mlogloss:2.51818\n",
      "[116]\tvalidation_0-mlogloss:2.51463\n",
      "[117]\tvalidation_0-mlogloss:2.51232\n",
      "[118]\tvalidation_0-mlogloss:2.50906\n",
      "[119]\tvalidation_0-mlogloss:2.50652\n",
      "[120]\tvalidation_0-mlogloss:2.50300\n",
      "[121]\tvalidation_0-mlogloss:2.50016\n",
      "[122]\tvalidation_0-mlogloss:2.49796\n",
      "[123]\tvalidation_0-mlogloss:2.49442\n",
      "[124]\tvalidation_0-mlogloss:2.49183\n",
      "[125]\tvalidation_0-mlogloss:2.48952\n",
      "[126]\tvalidation_0-mlogloss:2.48678\n",
      "[127]\tvalidation_0-mlogloss:2.48426\n",
      "[128]\tvalidation_0-mlogloss:2.48167\n",
      "[129]\tvalidation_0-mlogloss:2.47905\n",
      "[130]\tvalidation_0-mlogloss:2.47634\n",
      "[131]\tvalidation_0-mlogloss:2.47411\n",
      "[132]\tvalidation_0-mlogloss:2.47180\n",
      "[133]\tvalidation_0-mlogloss:2.46896\n",
      "[134]\tvalidation_0-mlogloss:2.46683\n",
      "[135]\tvalidation_0-mlogloss:2.46468\n",
      "[136]\tvalidation_0-mlogloss:2.46237\n",
      "[137]\tvalidation_0-mlogloss:2.45979\n",
      "[138]\tvalidation_0-mlogloss:2.45704\n",
      "[139]\tvalidation_0-mlogloss:2.45465\n",
      "[140]\tvalidation_0-mlogloss:2.45195\n",
      "[141]\tvalidation_0-mlogloss:2.44986\n",
      "[142]\tvalidation_0-mlogloss:2.44738\n",
      "[143]\tvalidation_0-mlogloss:2.44502\n",
      "[144]\tvalidation_0-mlogloss:2.44293\n",
      "[145]\tvalidation_0-mlogloss:2.44113\n",
      "[146]\tvalidation_0-mlogloss:2.43940\n",
      "[147]\tvalidation_0-mlogloss:2.43726\n",
      "[148]\tvalidation_0-mlogloss:2.43515\n",
      "[149]\tvalidation_0-mlogloss:2.43234\n",
      "[150]\tvalidation_0-mlogloss:2.43079\n",
      "[151]\tvalidation_0-mlogloss:2.42880\n",
      "[152]\tvalidation_0-mlogloss:2.42689\n",
      "[153]\tvalidation_0-mlogloss:2.42487\n",
      "[154]\tvalidation_0-mlogloss:2.42255\n",
      "[155]\tvalidation_0-mlogloss:2.41965\n",
      "[156]\tvalidation_0-mlogloss:2.41750\n",
      "[157]\tvalidation_0-mlogloss:2.41587\n",
      "[158]\tvalidation_0-mlogloss:2.41435\n",
      "[159]\tvalidation_0-mlogloss:2.41256\n",
      "[160]\tvalidation_0-mlogloss:2.41070\n",
      "[161]\tvalidation_0-mlogloss:2.40976\n",
      "[162]\tvalidation_0-mlogloss:2.40812\n",
      "[163]\tvalidation_0-mlogloss:2.40654\n",
      "[164]\tvalidation_0-mlogloss:2.40524\n",
      "[165]\tvalidation_0-mlogloss:2.40276\n",
      "[166]\tvalidation_0-mlogloss:2.40123\n",
      "[167]\tvalidation_0-mlogloss:2.39943\n",
      "[168]\tvalidation_0-mlogloss:2.39776\n",
      "[169]\tvalidation_0-mlogloss:2.39610\n",
      "[170]\tvalidation_0-mlogloss:2.39486\n",
      "[171]\tvalidation_0-mlogloss:2.39298\n",
      "[172]\tvalidation_0-mlogloss:2.39146\n",
      "[173]\tvalidation_0-mlogloss:2.39038\n",
      "[174]\tvalidation_0-mlogloss:2.38857\n",
      "[175]\tvalidation_0-mlogloss:2.38752\n",
      "[176]\tvalidation_0-mlogloss:2.38560\n",
      "[177]\tvalidation_0-mlogloss:2.38391\n",
      "[178]\tvalidation_0-mlogloss:2.38240\n",
      "[179]\tvalidation_0-mlogloss:2.38019\n",
      "[180]\tvalidation_0-mlogloss:2.37862\n",
      "[181]\tvalidation_0-mlogloss:2.37706\n",
      "[182]\tvalidation_0-mlogloss:2.37524\n",
      "[183]\tvalidation_0-mlogloss:2.37390\n",
      "[184]\tvalidation_0-mlogloss:2.37255\n",
      "[185]\tvalidation_0-mlogloss:2.37127\n",
      "[186]\tvalidation_0-mlogloss:2.37005\n",
      "[187]\tvalidation_0-mlogloss:2.36847\n",
      "[188]\tvalidation_0-mlogloss:2.36682\n",
      "[189]\tvalidation_0-mlogloss:2.36543\n",
      "[190]\tvalidation_0-mlogloss:2.36341\n",
      "[191]\tvalidation_0-mlogloss:2.36215\n",
      "[192]\tvalidation_0-mlogloss:2.36073\n",
      "[193]\tvalidation_0-mlogloss:2.35837\n",
      "[194]\tvalidation_0-mlogloss:2.35630\n",
      "[195]\tvalidation_0-mlogloss:2.35509\n",
      "[196]\tvalidation_0-mlogloss:2.35410\n",
      "[197]\tvalidation_0-mlogloss:2.35273\n",
      "[198]\tvalidation_0-mlogloss:2.35222\n",
      "[199]\tvalidation_0-mlogloss:2.35119\n",
      "[200]\tvalidation_0-mlogloss:2.35024\n",
      "[201]\tvalidation_0-mlogloss:2.34904\n",
      "[202]\tvalidation_0-mlogloss:2.34844\n",
      "[203]\tvalidation_0-mlogloss:2.34704\n",
      "[204]\tvalidation_0-mlogloss:2.34482\n",
      "[205]\tvalidation_0-mlogloss:2.34333\n",
      "[206]\tvalidation_0-mlogloss:2.34135\n",
      "[207]\tvalidation_0-mlogloss:2.34007\n",
      "[208]\tvalidation_0-mlogloss:2.33881\n",
      "[209]\tvalidation_0-mlogloss:2.33734\n",
      "[210]\tvalidation_0-mlogloss:2.33669\n",
      "[211]\tvalidation_0-mlogloss:2.33508\n",
      "[212]\tvalidation_0-mlogloss:2.33420\n",
      "[213]\tvalidation_0-mlogloss:2.33282\n",
      "[214]\tvalidation_0-mlogloss:2.33171\n",
      "[215]\tvalidation_0-mlogloss:2.33104\n",
      "[216]\tvalidation_0-mlogloss:2.32994\n",
      "[217]\tvalidation_0-mlogloss:2.32849\n",
      "[218]\tvalidation_0-mlogloss:2.32734\n",
      "[219]\tvalidation_0-mlogloss:2.32605\n",
      "[220]\tvalidation_0-mlogloss:2.32535\n",
      "[221]\tvalidation_0-mlogloss:2.32467\n",
      "[222]\tvalidation_0-mlogloss:2.32332\n",
      "[223]\tvalidation_0-mlogloss:2.32256\n",
      "[224]\tvalidation_0-mlogloss:2.32101\n",
      "[225]\tvalidation_0-mlogloss:2.31952\n",
      "[226]\tvalidation_0-mlogloss:2.31859\n",
      "[227]\tvalidation_0-mlogloss:2.31771\n",
      "[228]\tvalidation_0-mlogloss:2.31683\n",
      "[229]\tvalidation_0-mlogloss:2.31593\n",
      "[230]\tvalidation_0-mlogloss:2.31526\n",
      "[231]\tvalidation_0-mlogloss:2.31400\n",
      "[232]\tvalidation_0-mlogloss:2.31341\n",
      "[233]\tvalidation_0-mlogloss:2.31240\n",
      "[234]\tvalidation_0-mlogloss:2.31135\n",
      "[235]\tvalidation_0-mlogloss:2.31056\n",
      "[236]\tvalidation_0-mlogloss:2.30996\n",
      "[237]\tvalidation_0-mlogloss:2.30907\n",
      "[238]\tvalidation_0-mlogloss:2.30832\n",
      "[239]\tvalidation_0-mlogloss:2.30757\n",
      "[240]\tvalidation_0-mlogloss:2.30697\n",
      "[241]\tvalidation_0-mlogloss:2.30621\n",
      "[242]\tvalidation_0-mlogloss:2.30536\n",
      "[243]\tvalidation_0-mlogloss:2.30415\n",
      "[244]\tvalidation_0-mlogloss:2.30345\n",
      "[245]\tvalidation_0-mlogloss:2.30234\n",
      "[246]\tvalidation_0-mlogloss:2.30154\n",
      "[247]\tvalidation_0-mlogloss:2.30094\n",
      "[248]\tvalidation_0-mlogloss:2.30093\n",
      "[249]\tvalidation_0-mlogloss:2.30013\n",
      "[250]\tvalidation_0-mlogloss:2.29971\n",
      "[251]\tvalidation_0-mlogloss:2.29898\n",
      "[252]\tvalidation_0-mlogloss:2.29843\n",
      "[253]\tvalidation_0-mlogloss:2.29786\n",
      "[254]\tvalidation_0-mlogloss:2.29751\n",
      "[255]\tvalidation_0-mlogloss:2.29797\n",
      "[256]\tvalidation_0-mlogloss:2.29726\n",
      "[257]\tvalidation_0-mlogloss:2.29586\n",
      "[258]\tvalidation_0-mlogloss:2.29508\n",
      "[259]\tvalidation_0-mlogloss:2.29435\n",
      "[260]\tvalidation_0-mlogloss:2.29354\n",
      "[261]\tvalidation_0-mlogloss:2.29252\n",
      "[262]\tvalidation_0-mlogloss:2.29170\n",
      "[263]\tvalidation_0-mlogloss:2.29083\n",
      "[264]\tvalidation_0-mlogloss:2.28942\n",
      "[265]\tvalidation_0-mlogloss:2.28836\n",
      "[266]\tvalidation_0-mlogloss:2.28784\n",
      "[267]\tvalidation_0-mlogloss:2.28631\n",
      "[268]\tvalidation_0-mlogloss:2.28609\n",
      "[269]\tvalidation_0-mlogloss:2.28545\n",
      "[270]\tvalidation_0-mlogloss:2.28413\n",
      "[271]\tvalidation_0-mlogloss:2.28394\n",
      "[272]\tvalidation_0-mlogloss:2.28304\n",
      "[273]\tvalidation_0-mlogloss:2.28209\n",
      "[274]\tvalidation_0-mlogloss:2.28155\n",
      "[275]\tvalidation_0-mlogloss:2.28091\n",
      "[276]\tvalidation_0-mlogloss:2.28014\n",
      "[277]\tvalidation_0-mlogloss:2.27894\n",
      "[278]\tvalidation_0-mlogloss:2.27842\n",
      "[279]\tvalidation_0-mlogloss:2.27808\n",
      "[280]\tvalidation_0-mlogloss:2.27741\n",
      "[281]\tvalidation_0-mlogloss:2.27748\n",
      "[282]\tvalidation_0-mlogloss:2.27717\n",
      "[283]\tvalidation_0-mlogloss:2.27672\n",
      "[284]\tvalidation_0-mlogloss:2.27607\n",
      "[285]\tvalidation_0-mlogloss:2.27570\n",
      "[286]\tvalidation_0-mlogloss:2.27497\n",
      "[287]\tvalidation_0-mlogloss:2.27414\n",
      "[288]\tvalidation_0-mlogloss:2.27392\n",
      "[289]\tvalidation_0-mlogloss:2.27344\n",
      "[290]\tvalidation_0-mlogloss:2.27282\n",
      "[291]\tvalidation_0-mlogloss:2.27191\n",
      "[292]\tvalidation_0-mlogloss:2.27146\n",
      "[293]\tvalidation_0-mlogloss:2.27138\n",
      "[294]\tvalidation_0-mlogloss:2.27102\n",
      "[295]\tvalidation_0-mlogloss:2.27022\n",
      "[296]\tvalidation_0-mlogloss:2.26989\n",
      "[297]\tvalidation_0-mlogloss:2.26948\n",
      "[298]\tvalidation_0-mlogloss:2.26847\n",
      "[299]\tvalidation_0-mlogloss:2.26823\n",
      "[300]\tvalidation_0-mlogloss:2.26814\n",
      "[301]\tvalidation_0-mlogloss:2.26795\n",
      "[302]\tvalidation_0-mlogloss:2.26720\n",
      "[303]\tvalidation_0-mlogloss:2.26630\n",
      "[304]\tvalidation_0-mlogloss:2.26583\n",
      "[305]\tvalidation_0-mlogloss:2.26557\n",
      "[306]\tvalidation_0-mlogloss:2.26483\n",
      "[307]\tvalidation_0-mlogloss:2.26440\n",
      "[308]\tvalidation_0-mlogloss:2.26415\n",
      "[309]\tvalidation_0-mlogloss:2.26406\n",
      "[310]\tvalidation_0-mlogloss:2.26366\n",
      "[311]\tvalidation_0-mlogloss:2.26312\n",
      "[312]\tvalidation_0-mlogloss:2.26302\n",
      "[313]\tvalidation_0-mlogloss:2.26269\n",
      "[314]\tvalidation_0-mlogloss:2.26269\n",
      "[315]\tvalidation_0-mlogloss:2.26236\n",
      "[316]\tvalidation_0-mlogloss:2.26196\n",
      "[317]\tvalidation_0-mlogloss:2.26163\n",
      "[318]\tvalidation_0-mlogloss:2.26122\n",
      "[319]\tvalidation_0-mlogloss:2.26054\n",
      "[320]\tvalidation_0-mlogloss:2.26009\n",
      "[321]\tvalidation_0-mlogloss:2.25882\n",
      "[322]\tvalidation_0-mlogloss:2.25794\n",
      "[323]\tvalidation_0-mlogloss:2.25779\n",
      "[324]\tvalidation_0-mlogloss:2.25744\n",
      "[325]\tvalidation_0-mlogloss:2.25693\n",
      "[326]\tvalidation_0-mlogloss:2.25684\n",
      "[327]\tvalidation_0-mlogloss:2.25602\n",
      "[328]\tvalidation_0-mlogloss:2.25593\n",
      "[329]\tvalidation_0-mlogloss:2.25583\n",
      "[330]\tvalidation_0-mlogloss:2.25541\n",
      "[331]\tvalidation_0-mlogloss:2.25455\n",
      "[332]\tvalidation_0-mlogloss:2.25401\n",
      "[333]\tvalidation_0-mlogloss:2.25363\n",
      "[334]\tvalidation_0-mlogloss:2.25370\n",
      "[335]\tvalidation_0-mlogloss:2.25284\n",
      "[336]\tvalidation_0-mlogloss:2.25336\n",
      "[337]\tvalidation_0-mlogloss:2.25293\n",
      "[338]\tvalidation_0-mlogloss:2.25262\n",
      "[339]\tvalidation_0-mlogloss:2.25237\n",
      "[340]\tvalidation_0-mlogloss:2.25210\n",
      "[341]\tvalidation_0-mlogloss:2.25190\n",
      "[342]\tvalidation_0-mlogloss:2.25149\n",
      "[343]\tvalidation_0-mlogloss:2.25099\n",
      "[344]\tvalidation_0-mlogloss:2.25092\n",
      "[345]\tvalidation_0-mlogloss:2.25070\n",
      "[346]\tvalidation_0-mlogloss:2.25009\n",
      "[347]\tvalidation_0-mlogloss:2.24898\n",
      "[348]\tvalidation_0-mlogloss:2.24871\n",
      "[349]\tvalidation_0-mlogloss:2.24757\n",
      "[350]\tvalidation_0-mlogloss:2.24692\n",
      "[351]\tvalidation_0-mlogloss:2.24651\n",
      "[352]\tvalidation_0-mlogloss:2.24579\n",
      "[353]\tvalidation_0-mlogloss:2.24549\n",
      "[354]\tvalidation_0-mlogloss:2.24476\n",
      "[355]\tvalidation_0-mlogloss:2.24502\n",
      "[356]\tvalidation_0-mlogloss:2.24512\n",
      "[357]\tvalidation_0-mlogloss:2.24511\n",
      "[358]\tvalidation_0-mlogloss:2.24547\n",
      "[359]\tvalidation_0-mlogloss:2.24460\n",
      "[360]\tvalidation_0-mlogloss:2.24412\n",
      "[361]\tvalidation_0-mlogloss:2.24404\n",
      "[362]\tvalidation_0-mlogloss:2.24391\n",
      "[363]\tvalidation_0-mlogloss:2.24395\n",
      "[364]\tvalidation_0-mlogloss:2.24457\n",
      "[365]\tvalidation_0-mlogloss:2.24475\n",
      "[366]\tvalidation_0-mlogloss:2.24467\n",
      "[367]\tvalidation_0-mlogloss:2.24469\n",
      "[368]\tvalidation_0-mlogloss:2.24473\n",
      "[369]\tvalidation_0-mlogloss:2.24450\n",
      "[370]\tvalidation_0-mlogloss:2.24473\n",
      "[371]\tvalidation_0-mlogloss:2.24366\n",
      "[372]\tvalidation_0-mlogloss:2.24343\n",
      "[373]\tvalidation_0-mlogloss:2.24296\n",
      "[374]\tvalidation_0-mlogloss:2.24275\n",
      "[375]\tvalidation_0-mlogloss:2.24270\n",
      "[376]\tvalidation_0-mlogloss:2.24285\n",
      "[377]\tvalidation_0-mlogloss:2.24286\n",
      "[378]\tvalidation_0-mlogloss:2.24270\n",
      "[379]\tvalidation_0-mlogloss:2.24258\n",
      "[380]\tvalidation_0-mlogloss:2.24227\n",
      "[381]\tvalidation_0-mlogloss:2.24215\n",
      "[382]\tvalidation_0-mlogloss:2.24206\n",
      "[383]\tvalidation_0-mlogloss:2.24155\n",
      "[384]\tvalidation_0-mlogloss:2.24134\n",
      "[385]\tvalidation_0-mlogloss:2.24094\n",
      "[386]\tvalidation_0-mlogloss:2.24089\n",
      "[387]\tvalidation_0-mlogloss:2.24101\n",
      "[388]\tvalidation_0-mlogloss:2.24112\n",
      "[389]\tvalidation_0-mlogloss:2.24081\n",
      "[390]\tvalidation_0-mlogloss:2.24088\n",
      "[391]\tvalidation_0-mlogloss:2.24077\n",
      "[392]\tvalidation_0-mlogloss:2.24025\n",
      "[393]\tvalidation_0-mlogloss:2.24014\n",
      "[394]\tvalidation_0-mlogloss:2.24011\n",
      "[395]\tvalidation_0-mlogloss:2.23991\n",
      "[396]\tvalidation_0-mlogloss:2.23944\n",
      "[397]\tvalidation_0-mlogloss:2.23889\n",
      "[398]\tvalidation_0-mlogloss:2.23847\n",
      "[399]\tvalidation_0-mlogloss:2.23819\n",
      "[400]\tvalidation_0-mlogloss:2.23817\n",
      "[401]\tvalidation_0-mlogloss:2.23784\n",
      "[402]\tvalidation_0-mlogloss:2.23772\n",
      "[403]\tvalidation_0-mlogloss:2.23726\n",
      "[404]\tvalidation_0-mlogloss:2.23697\n",
      "[405]\tvalidation_0-mlogloss:2.23719\n",
      "[406]\tvalidation_0-mlogloss:2.23694\n",
      "[407]\tvalidation_0-mlogloss:2.23722\n",
      "[408]\tvalidation_0-mlogloss:2.23687\n",
      "[409]\tvalidation_0-mlogloss:2.23663\n",
      "[410]\tvalidation_0-mlogloss:2.23685\n",
      "[411]\tvalidation_0-mlogloss:2.23707\n",
      "[412]\tvalidation_0-mlogloss:2.23746\n",
      "[413]\tvalidation_0-mlogloss:2.23741\n",
      "[414]\tvalidation_0-mlogloss:2.23753\n",
      "[415]\tvalidation_0-mlogloss:2.23723\n",
      "[416]\tvalidation_0-mlogloss:2.23708\n",
      "[417]\tvalidation_0-mlogloss:2.23734\n",
      "[418]\tvalidation_0-mlogloss:2.23711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.34      0.33      1499\n",
      "           1       0.33      0.39      0.36      1499\n",
      "           2       0.28      0.23      0.25      1499\n",
      "           3       0.71      0.44      0.54      1499\n",
      "           4       0.82      0.82      0.82      1499\n",
      "           5       0.41      0.45      0.43      1499\n",
      "           6       0.58      0.74      0.65      1499\n",
      "           7       0.85      0.76      0.80      1499\n",
      "           8       0.45      0.73      0.55      1499\n",
      "           9       0.61      0.54      0.57      1499\n",
      "          10       0.61      0.66      0.63      1499\n",
      "          11       0.38      0.45      0.41      1499\n",
      "          12       0.43      0.50      0.46      1499\n",
      "          13       0.32      0.34      0.33      1499\n",
      "          14       0.28      0.24      0.26      1499\n",
      "          15       0.40      0.11      0.17      1499\n",
      "          16       0.32      0.20      0.25      1499\n",
      "          17       0.61      0.60      0.60      1499\n",
      "          18       0.34      0.44      0.39      1499\n",
      "          19       0.38      0.31      0.34      1499\n",
      "          20       0.73      0.87      0.79      1499\n",
      "          21       0.47      0.43      0.45      1499\n",
      "          22       0.31      0.21      0.25      1499\n",
      "          23       0.59      0.77      0.67      1499\n",
      "          24       0.86      0.72      0.78      1499\n",
      "          25       0.33      0.13      0.18      1499\n",
      "          26       0.69      0.45      0.54      1499\n",
      "          27       0.26      0.28      0.27      1499\n",
      "          28       0.34      0.27      0.30      1499\n",
      "          29       0.52      0.54      0.53      1499\n",
      "          30       0.27      0.25      0.26      1499\n",
      "          31       0.21      0.38      0.27      1499\n",
      "          32       0.39      0.46      0.42      1499\n",
      "          33       0.46      0.28      0.35      1499\n",
      "          34       0.65      0.71      0.68      1499\n",
      "          35       0.58      0.75      0.65      1499\n",
      "          36       0.60      0.54      0.57      1499\n",
      "          37       0.40      0.41      0.41      1499\n",
      "          38       0.24      0.17      0.20      1499\n",
      "          39       0.59      0.54      0.57      1499\n",
      "          40       0.43      0.40      0.42      1499\n",
      "          41       0.55      0.48      0.51      1499\n",
      "          42       0.34      0.36      0.35      1499\n",
      "          43       0.30      0.35      0.32      1499\n",
      "          44       0.65      0.79      0.71      1499\n",
      "          45       0.29      0.25      0.27      1499\n",
      "          46       0.47      0.66      0.55      1499\n",
      "          47       0.61      0.42      0.50      1499\n",
      "          48       0.63      0.75      0.68      1499\n",
      "          49       0.40      0.34      0.37      1499\n",
      "          50       0.49      0.54      0.51      1499\n",
      "          51       0.33      0.35      0.34      1499\n",
      "          52       0.28      0.21      0.24      1499\n",
      "          53       0.33      0.33      0.33      1499\n",
      "          54       0.41      0.51      0.46      1499\n",
      "          55       0.39      0.50      0.44      1499\n",
      "          56       0.54      0.67      0.60      1499\n",
      "          57       0.46      0.50      0.48      1499\n",
      "          58       0.90      0.88      0.89      1499\n",
      "          59       0.37      0.34      0.36      1499\n",
      "          60       0.44      0.38      0.40      1499\n",
      "          61       0.03      0.01      0.02      1499\n",
      "          62       0.53      0.80      0.64      1499\n",
      "          63       0.29      0.29      0.29      1499\n",
      "          64       0.45      0.40      0.42      1499\n",
      "          65       0.30      0.48      0.37      1499\n",
      "          66       0.67      0.62      0.65      1499\n",
      "          67       0.60      0.26      0.36      1499\n",
      "          68       0.49      0.61      0.54      1499\n",
      "          69       0.22      0.23      0.23      1499\n",
      "          70       0.66      0.40      0.50      1499\n",
      "          71       0.31      0.19      0.24      1499\n",
      "          72       0.34      0.34      0.34      1499\n",
      "          73       0.41      0.40      0.41      1499\n",
      "          74       0.23      0.41      0.29      1499\n",
      "\n",
      "    accuracy                           0.45    112425\n",
      "   macro avg       0.45      0.45      0.44    112425\n",
      "weighted avg       0.45      0.45      0.44    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7980b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46f7c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 13:25:45.529341: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 57600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28125/28125 [==============================] - 51s 2ms/step - loss: 2.0967 - val_loss: 2.6360\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 1.4131 - val_loss: 2.4992\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 1.2081 - val_loss: 2.4241\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 1.1178 - val_loss: 2.4097\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 1.0646 - val_loss: 2.5147\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 1.0283 - val_loss: 2.6538\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 1.0006 - val_loss: 2.5147\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 0.9787 - val_loss: 2.6059\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.9583 - val_loss: 2.7531\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 0.9374 - val_loss: 2.5364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f507041beb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  48/3514 [..............................] - ETA: 3s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 4s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.52      0.37      1499\n",
      "           1       0.33      0.39      0.36      1499\n",
      "           2       0.29      0.20      0.24      1499\n",
      "           3       0.68      0.34      0.45      1499\n",
      "           4       0.83      0.80      0.81      1499\n",
      "           5       0.62      0.50      0.55      1499\n",
      "           6       0.64      0.78      0.70      1499\n",
      "           7       0.77      0.67      0.72      1499\n",
      "           8       0.48      0.68      0.56      1499\n",
      "           9       0.46      0.52      0.49      1499\n",
      "          10       0.50      0.39      0.44      1499\n",
      "          11       0.57      0.39      0.46      1499\n",
      "          12       0.37      0.40      0.39      1499\n",
      "          13       0.30      0.28      0.29      1499\n",
      "          14       0.30      0.15      0.20      1499\n",
      "          15       0.53      0.12      0.19      1499\n",
      "          16       0.29      0.32      0.30      1499\n",
      "          17       0.71      0.59      0.64      1499\n",
      "          18       0.34      0.49      0.40      1499\n",
      "          19       0.34      0.19      0.24      1499\n",
      "          20       0.69      0.89      0.78      1499\n",
      "          21       0.41      0.49      0.45      1499\n",
      "          22       0.41      0.13      0.20      1499\n",
      "          23       0.68      0.80      0.74      1499\n",
      "          24       0.69      0.78      0.73      1499\n",
      "          25       0.47      0.16      0.24      1499\n",
      "          26       0.47      0.46      0.46      1499\n",
      "          27       0.25      0.23      0.24      1499\n",
      "          28       0.37      0.34      0.36      1499\n",
      "          29       0.67      0.60      0.63      1499\n",
      "          30       0.43      0.26      0.33      1499\n",
      "          31       0.24      0.34      0.28      1499\n",
      "          32       0.45      0.39      0.42      1499\n",
      "          33       0.34      0.21      0.26      1499\n",
      "          34       0.70      0.78      0.74      1499\n",
      "          35       0.71      0.72      0.71      1499\n",
      "          36       0.57      0.18      0.28      1499\n",
      "          37       0.36      0.41      0.39      1499\n",
      "          38       0.31      0.18      0.23      1499\n",
      "          39       0.36      0.61      0.45      1499\n",
      "          40       0.42      0.38      0.40      1499\n",
      "          41       0.82      0.53      0.65      1499\n",
      "          42       0.29      0.29      0.29      1499\n",
      "          43       0.28      0.45      0.35      1499\n",
      "          44       0.71      0.73      0.72      1499\n",
      "          45       0.39      0.32      0.35      1499\n",
      "          46       0.52      0.62      0.56      1499\n",
      "          47       0.45      0.17      0.25      1499\n",
      "          48       0.64      0.65      0.64      1499\n",
      "          49       0.30      0.39      0.34      1499\n",
      "          50       0.72      0.47      0.57      1499\n",
      "          51       0.43      0.21      0.28      1499\n",
      "          52       0.27      0.20      0.23      1499\n",
      "          53       0.36      0.52      0.43      1499\n",
      "          54       0.38      0.49      0.43      1499\n",
      "          55       0.34      0.65      0.45      1499\n",
      "          56       0.41      0.66      0.50      1499\n",
      "          57       0.50      0.66      0.57      1499\n",
      "          58       0.76      0.86      0.81      1499\n",
      "          59       0.43      0.26      0.33      1499\n",
      "          60       0.54      0.22      0.32      1499\n",
      "          61       0.06      0.02      0.03      1499\n",
      "          62       0.67      0.67      0.67      1499\n",
      "          63       0.34      0.40      0.37      1499\n",
      "          64       0.36      0.36      0.36      1499\n",
      "          65       0.32      0.58      0.41      1499\n",
      "          66       0.57      0.75      0.65      1499\n",
      "          67       0.69      0.57      0.62      1499\n",
      "          68       0.48      0.66      0.55      1499\n",
      "          69       0.19      0.31      0.24      1499\n",
      "          70       0.33      0.27      0.30      1499\n",
      "          71       0.38      0.26      0.31      1499\n",
      "          72       0.29      0.29      0.29      1499\n",
      "          73       0.32      0.61      0.42      1499\n",
      "          74       0.21      0.27      0.24      1499\n",
      "\n",
      "    accuracy                           0.45    112425\n",
      "   macro avg       0.46      0.45      0.43    112425\n",
      "weighted avg       0.46      0.45      0.43    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2632/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_2632/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_2632/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.80627\n",
      "[1]\tvalidation_0-mlogloss:3.57801\n",
      "[2]\tvalidation_0-mlogloss:3.42164\n",
      "[3]\tvalidation_0-mlogloss:3.30383\n",
      "[4]\tvalidation_0-mlogloss:3.21032\n",
      "[5]\tvalidation_0-mlogloss:3.12607\n",
      "[6]\tvalidation_0-mlogloss:3.05996\n",
      "[7]\tvalidation_0-mlogloss:2.99354\n",
      "[8]\tvalidation_0-mlogloss:2.93950\n",
      "[9]\tvalidation_0-mlogloss:2.88963\n",
      "[10]\tvalidation_0-mlogloss:2.84718\n",
      "[11]\tvalidation_0-mlogloss:2.80616\n",
      "[12]\tvalidation_0-mlogloss:2.77230\n",
      "[13]\tvalidation_0-mlogloss:2.73996\n",
      "[14]\tvalidation_0-mlogloss:2.70987\n",
      "[15]\tvalidation_0-mlogloss:2.67843\n",
      "[16]\tvalidation_0-mlogloss:2.65195\n",
      "[17]\tvalidation_0-mlogloss:2.62570\n",
      "[18]\tvalidation_0-mlogloss:2.60021\n",
      "[19]\tvalidation_0-mlogloss:2.57735\n",
      "[20]\tvalidation_0-mlogloss:2.55719\n",
      "[21]\tvalidation_0-mlogloss:2.53395\n",
      "[22]\tvalidation_0-mlogloss:2.51226\n",
      "[23]\tvalidation_0-mlogloss:2.49490\n",
      "[24]\tvalidation_0-mlogloss:2.47483\n",
      "[25]\tvalidation_0-mlogloss:2.45383\n",
      "[26]\tvalidation_0-mlogloss:2.43668\n",
      "[27]\tvalidation_0-mlogloss:2.41632\n",
      "[28]\tvalidation_0-mlogloss:2.40023\n",
      "[29]\tvalidation_0-mlogloss:2.38205\n",
      "[30]\tvalidation_0-mlogloss:2.36429\n",
      "[31]\tvalidation_0-mlogloss:2.34864\n",
      "[32]\tvalidation_0-mlogloss:2.33467\n",
      "[33]\tvalidation_0-mlogloss:2.31676\n",
      "[34]\tvalidation_0-mlogloss:2.30407\n",
      "[35]\tvalidation_0-mlogloss:2.29010\n",
      "[36]\tvalidation_0-mlogloss:2.27687\n",
      "[37]\tvalidation_0-mlogloss:2.26271\n",
      "[38]\tvalidation_0-mlogloss:2.25080\n",
      "[39]\tvalidation_0-mlogloss:2.23743\n",
      "[40]\tvalidation_0-mlogloss:2.22558\n",
      "[41]\tvalidation_0-mlogloss:2.21154\n",
      "[42]\tvalidation_0-mlogloss:2.20034\n",
      "[43]\tvalidation_0-mlogloss:2.18869\n",
      "[44]\tvalidation_0-mlogloss:2.17724\n",
      "[45]\tvalidation_0-mlogloss:2.16605\n",
      "[46]\tvalidation_0-mlogloss:2.15372\n",
      "[47]\tvalidation_0-mlogloss:2.14250\n",
      "[48]\tvalidation_0-mlogloss:2.13168\n",
      "[49]\tvalidation_0-mlogloss:2.12051\n",
      "[50]\tvalidation_0-mlogloss:2.10846\n",
      "[51]\tvalidation_0-mlogloss:2.09814\n",
      "[52]\tvalidation_0-mlogloss:2.08690\n",
      "[53]\tvalidation_0-mlogloss:2.07733\n",
      "[54]\tvalidation_0-mlogloss:2.06854\n",
      "[55]\tvalidation_0-mlogloss:2.05981\n",
      "[56]\tvalidation_0-mlogloss:2.05119\n",
      "[57]\tvalidation_0-mlogloss:2.04272\n",
      "[58]\tvalidation_0-mlogloss:2.03355\n",
      "[59]\tvalidation_0-mlogloss:2.02601\n",
      "[60]\tvalidation_0-mlogloss:2.01734\n",
      "[61]\tvalidation_0-mlogloss:2.00973\n",
      "[62]\tvalidation_0-mlogloss:2.00154\n",
      "[63]\tvalidation_0-mlogloss:1.99408\n",
      "[64]\tvalidation_0-mlogloss:1.98599\n",
      "[65]\tvalidation_0-mlogloss:1.97962\n",
      "[66]\tvalidation_0-mlogloss:1.97353\n",
      "[67]\tvalidation_0-mlogloss:1.96581\n",
      "[68]\tvalidation_0-mlogloss:1.95953\n",
      "[69]\tvalidation_0-mlogloss:1.95182\n",
      "[70]\tvalidation_0-mlogloss:1.94441\n",
      "[71]\tvalidation_0-mlogloss:1.93744\n",
      "[72]\tvalidation_0-mlogloss:1.93067\n",
      "[73]\tvalidation_0-mlogloss:1.92473\n",
      "[74]\tvalidation_0-mlogloss:1.91843\n",
      "[75]\tvalidation_0-mlogloss:1.91281\n",
      "[76]\tvalidation_0-mlogloss:1.90646\n",
      "[77]\tvalidation_0-mlogloss:1.90086\n",
      "[78]\tvalidation_0-mlogloss:1.89483\n",
      "[79]\tvalidation_0-mlogloss:1.88882\n",
      "[80]\tvalidation_0-mlogloss:1.88381\n",
      "[81]\tvalidation_0-mlogloss:1.87747\n",
      "[82]\tvalidation_0-mlogloss:1.87109\n",
      "[83]\tvalidation_0-mlogloss:1.86511\n",
      "[84]\tvalidation_0-mlogloss:1.85832\n",
      "[85]\tvalidation_0-mlogloss:1.85409\n",
      "[86]\tvalidation_0-mlogloss:1.84817\n",
      "[87]\tvalidation_0-mlogloss:1.84312\n",
      "[88]\tvalidation_0-mlogloss:1.83655\n",
      "[89]\tvalidation_0-mlogloss:1.82999\n",
      "[90]\tvalidation_0-mlogloss:1.82459\n",
      "[91]\tvalidation_0-mlogloss:1.81830\n",
      "[92]\tvalidation_0-mlogloss:1.81402\n",
      "[93]\tvalidation_0-mlogloss:1.80947\n",
      "[94]\tvalidation_0-mlogloss:1.80482\n",
      "[95]\tvalidation_0-mlogloss:1.80049\n",
      "[96]\tvalidation_0-mlogloss:1.79598\n",
      "[97]\tvalidation_0-mlogloss:1.79157\n",
      "[98]\tvalidation_0-mlogloss:1.78620\n",
      "[99]\tvalidation_0-mlogloss:1.78145\n",
      "[100]\tvalidation_0-mlogloss:1.77735\n",
      "[101]\tvalidation_0-mlogloss:1.77337\n",
      "[102]\tvalidation_0-mlogloss:1.76815\n",
      "[103]\tvalidation_0-mlogloss:1.76481\n",
      "[104]\tvalidation_0-mlogloss:1.76038\n",
      "[105]\tvalidation_0-mlogloss:1.75540\n",
      "[106]\tvalidation_0-mlogloss:1.75179\n",
      "[107]\tvalidation_0-mlogloss:1.74628\n",
      "[108]\tvalidation_0-mlogloss:1.74254\n",
      "[109]\tvalidation_0-mlogloss:1.73825\n",
      "[110]\tvalidation_0-mlogloss:1.73429\n",
      "[111]\tvalidation_0-mlogloss:1.73004\n",
      "[112]\tvalidation_0-mlogloss:1.72588\n",
      "[113]\tvalidation_0-mlogloss:1.72165\n",
      "[114]\tvalidation_0-mlogloss:1.71759\n",
      "[115]\tvalidation_0-mlogloss:1.71316\n",
      "[116]\tvalidation_0-mlogloss:1.70998\n",
      "[117]\tvalidation_0-mlogloss:1.70660\n",
      "[118]\tvalidation_0-mlogloss:1.70296\n",
      "[119]\tvalidation_0-mlogloss:1.69809\n",
      "[120]\tvalidation_0-mlogloss:1.69419\n",
      "[121]\tvalidation_0-mlogloss:1.69175\n",
      "[122]\tvalidation_0-mlogloss:1.68884\n",
      "[123]\tvalidation_0-mlogloss:1.68531\n",
      "[124]\tvalidation_0-mlogloss:1.68260\n",
      "[125]\tvalidation_0-mlogloss:1.67841\n",
      "[126]\tvalidation_0-mlogloss:1.67533\n",
      "[127]\tvalidation_0-mlogloss:1.67298\n",
      "[128]\tvalidation_0-mlogloss:1.67075\n",
      "[129]\tvalidation_0-mlogloss:1.66783\n",
      "[130]\tvalidation_0-mlogloss:1.66557\n",
      "[131]\tvalidation_0-mlogloss:1.66297\n",
      "[132]\tvalidation_0-mlogloss:1.65992\n",
      "[133]\tvalidation_0-mlogloss:1.65668\n",
      "[134]\tvalidation_0-mlogloss:1.65254\n",
      "[135]\tvalidation_0-mlogloss:1.65066\n",
      "[136]\tvalidation_0-mlogloss:1.64839\n",
      "[137]\tvalidation_0-mlogloss:1.64447\n",
      "[138]\tvalidation_0-mlogloss:1.64260\n",
      "[139]\tvalidation_0-mlogloss:1.64040\n",
      "[140]\tvalidation_0-mlogloss:1.63820\n",
      "[141]\tvalidation_0-mlogloss:1.63600\n",
      "[142]\tvalidation_0-mlogloss:1.63367\n",
      "[143]\tvalidation_0-mlogloss:1.63096\n",
      "[144]\tvalidation_0-mlogloss:1.62817\n",
      "[145]\tvalidation_0-mlogloss:1.62485\n",
      "[146]\tvalidation_0-mlogloss:1.62267\n",
      "[147]\tvalidation_0-mlogloss:1.61954\n",
      "[148]\tvalidation_0-mlogloss:1.61800\n",
      "[149]\tvalidation_0-mlogloss:1.61609\n",
      "[150]\tvalidation_0-mlogloss:1.61320\n",
      "[151]\tvalidation_0-mlogloss:1.61168\n",
      "[152]\tvalidation_0-mlogloss:1.60856\n",
      "[153]\tvalidation_0-mlogloss:1.60638\n",
      "[154]\tvalidation_0-mlogloss:1.60381\n",
      "[155]\tvalidation_0-mlogloss:1.60000\n",
      "[156]\tvalidation_0-mlogloss:1.59746\n",
      "[157]\tvalidation_0-mlogloss:1.59507\n",
      "[158]\tvalidation_0-mlogloss:1.59275\n",
      "[159]\tvalidation_0-mlogloss:1.59100\n",
      "[160]\tvalidation_0-mlogloss:1.58894\n",
      "[161]\tvalidation_0-mlogloss:1.58621\n",
      "[162]\tvalidation_0-mlogloss:1.58429\n",
      "[163]\tvalidation_0-mlogloss:1.58220\n",
      "[164]\tvalidation_0-mlogloss:1.58092\n",
      "[165]\tvalidation_0-mlogloss:1.57897\n",
      "[166]\tvalidation_0-mlogloss:1.57773\n",
      "[167]\tvalidation_0-mlogloss:1.57599\n",
      "[168]\tvalidation_0-mlogloss:1.57431\n",
      "[169]\tvalidation_0-mlogloss:1.57125\n",
      "[170]\tvalidation_0-mlogloss:1.57010\n",
      "[171]\tvalidation_0-mlogloss:1.56855\n",
      "[172]\tvalidation_0-mlogloss:1.56710\n",
      "[173]\tvalidation_0-mlogloss:1.56432\n",
      "[174]\tvalidation_0-mlogloss:1.56198\n",
      "[175]\tvalidation_0-mlogloss:1.56041\n",
      "[176]\tvalidation_0-mlogloss:1.55873\n",
      "[177]\tvalidation_0-mlogloss:1.55618\n",
      "[178]\tvalidation_0-mlogloss:1.55436\n",
      "[179]\tvalidation_0-mlogloss:1.55291\n",
      "[180]\tvalidation_0-mlogloss:1.55108\n",
      "[181]\tvalidation_0-mlogloss:1.54941\n",
      "[182]\tvalidation_0-mlogloss:1.54815\n",
      "[183]\tvalidation_0-mlogloss:1.54628\n",
      "[184]\tvalidation_0-mlogloss:1.54404\n",
      "[185]\tvalidation_0-mlogloss:1.54268\n",
      "[186]\tvalidation_0-mlogloss:1.54130\n",
      "[187]\tvalidation_0-mlogloss:1.53986\n",
      "[188]\tvalidation_0-mlogloss:1.53858\n",
      "[189]\tvalidation_0-mlogloss:1.53691\n",
      "[190]\tvalidation_0-mlogloss:1.53469\n",
      "[191]\tvalidation_0-mlogloss:1.53253\n",
      "[192]\tvalidation_0-mlogloss:1.53144\n",
      "[193]\tvalidation_0-mlogloss:1.52911\n",
      "[194]\tvalidation_0-mlogloss:1.52779\n",
      "[195]\tvalidation_0-mlogloss:1.52617\n",
      "[196]\tvalidation_0-mlogloss:1.52487\n",
      "[197]\tvalidation_0-mlogloss:1.52361\n",
      "[198]\tvalidation_0-mlogloss:1.52319\n",
      "[199]\tvalidation_0-mlogloss:1.52183\n",
      "[200]\tvalidation_0-mlogloss:1.52106\n",
      "[201]\tvalidation_0-mlogloss:1.51942\n",
      "[202]\tvalidation_0-mlogloss:1.51880\n",
      "[203]\tvalidation_0-mlogloss:1.51688\n",
      "[204]\tvalidation_0-mlogloss:1.51471\n",
      "[205]\tvalidation_0-mlogloss:1.51341\n",
      "[206]\tvalidation_0-mlogloss:1.51147\n",
      "[207]\tvalidation_0-mlogloss:1.50994\n",
      "[208]\tvalidation_0-mlogloss:1.50873\n",
      "[209]\tvalidation_0-mlogloss:1.50795\n",
      "[210]\tvalidation_0-mlogloss:1.50662\n",
      "[211]\tvalidation_0-mlogloss:1.50559\n",
      "[212]\tvalidation_0-mlogloss:1.50436\n",
      "[213]\tvalidation_0-mlogloss:1.50287\n",
      "[214]\tvalidation_0-mlogloss:1.50198\n",
      "[215]\tvalidation_0-mlogloss:1.50085\n",
      "[216]\tvalidation_0-mlogloss:1.49926\n",
      "[217]\tvalidation_0-mlogloss:1.49704\n",
      "[218]\tvalidation_0-mlogloss:1.49576\n",
      "[219]\tvalidation_0-mlogloss:1.49578\n",
      "[220]\tvalidation_0-mlogloss:1.49435\n",
      "[221]\tvalidation_0-mlogloss:1.49284\n",
      "[222]\tvalidation_0-mlogloss:1.49179\n",
      "[223]\tvalidation_0-mlogloss:1.49117\n",
      "[224]\tvalidation_0-mlogloss:1.49011\n",
      "[225]\tvalidation_0-mlogloss:1.48882\n",
      "[226]\tvalidation_0-mlogloss:1.48743\n",
      "[227]\tvalidation_0-mlogloss:1.48724\n",
      "[228]\tvalidation_0-mlogloss:1.48676\n",
      "[229]\tvalidation_0-mlogloss:1.48558\n",
      "[230]\tvalidation_0-mlogloss:1.48457\n",
      "[231]\tvalidation_0-mlogloss:1.48422\n",
      "[232]\tvalidation_0-mlogloss:1.48323\n",
      "[233]\tvalidation_0-mlogloss:1.48233\n",
      "[234]\tvalidation_0-mlogloss:1.48144\n",
      "[235]\tvalidation_0-mlogloss:1.47961\n",
      "[236]\tvalidation_0-mlogloss:1.47964\n",
      "[237]\tvalidation_0-mlogloss:1.47835\n",
      "[238]\tvalidation_0-mlogloss:1.47747\n",
      "[239]\tvalidation_0-mlogloss:1.47618\n",
      "[240]\tvalidation_0-mlogloss:1.47581\n",
      "[241]\tvalidation_0-mlogloss:1.47489\n",
      "[242]\tvalidation_0-mlogloss:1.47463\n",
      "[243]\tvalidation_0-mlogloss:1.47443\n",
      "[244]\tvalidation_0-mlogloss:1.47352\n",
      "[245]\tvalidation_0-mlogloss:1.47260\n",
      "[246]\tvalidation_0-mlogloss:1.47185\n",
      "[247]\tvalidation_0-mlogloss:1.47041\n",
      "[248]\tvalidation_0-mlogloss:1.46982\n",
      "[249]\tvalidation_0-mlogloss:1.46881\n",
      "[250]\tvalidation_0-mlogloss:1.46810\n",
      "[251]\tvalidation_0-mlogloss:1.46711\n",
      "[252]\tvalidation_0-mlogloss:1.46597\n",
      "[253]\tvalidation_0-mlogloss:1.46458\n",
      "[254]\tvalidation_0-mlogloss:1.46420\n",
      "[255]\tvalidation_0-mlogloss:1.46299\n",
      "[256]\tvalidation_0-mlogloss:1.46226\n",
      "[257]\tvalidation_0-mlogloss:1.46233\n",
      "[258]\tvalidation_0-mlogloss:1.46181\n",
      "[259]\tvalidation_0-mlogloss:1.46090\n",
      "[260]\tvalidation_0-mlogloss:1.45981\n",
      "[261]\tvalidation_0-mlogloss:1.45879\n",
      "[262]\tvalidation_0-mlogloss:1.45855\n",
      "[263]\tvalidation_0-mlogloss:1.45796\n",
      "[264]\tvalidation_0-mlogloss:1.45743\n",
      "[265]\tvalidation_0-mlogloss:1.45666\n",
      "[266]\tvalidation_0-mlogloss:1.45581\n",
      "[267]\tvalidation_0-mlogloss:1.45520\n",
      "[268]\tvalidation_0-mlogloss:1.45443\n",
      "[269]\tvalidation_0-mlogloss:1.45311\n",
      "[270]\tvalidation_0-mlogloss:1.45273\n",
      "[271]\tvalidation_0-mlogloss:1.45229\n",
      "[272]\tvalidation_0-mlogloss:1.45160\n",
      "[273]\tvalidation_0-mlogloss:1.45089\n",
      "[274]\tvalidation_0-mlogloss:1.44994\n",
      "[275]\tvalidation_0-mlogloss:1.44942\n",
      "[276]\tvalidation_0-mlogloss:1.44904\n",
      "[277]\tvalidation_0-mlogloss:1.44822\n",
      "[278]\tvalidation_0-mlogloss:1.44733\n",
      "[279]\tvalidation_0-mlogloss:1.44609\n",
      "[280]\tvalidation_0-mlogloss:1.44544\n",
      "[281]\tvalidation_0-mlogloss:1.44442\n",
      "[282]\tvalidation_0-mlogloss:1.44332\n",
      "[283]\tvalidation_0-mlogloss:1.44250\n",
      "[284]\tvalidation_0-mlogloss:1.44210\n",
      "[285]\tvalidation_0-mlogloss:1.44171\n",
      "[286]\tvalidation_0-mlogloss:1.44121\n",
      "[287]\tvalidation_0-mlogloss:1.44055\n",
      "[288]\tvalidation_0-mlogloss:1.44041\n",
      "[289]\tvalidation_0-mlogloss:1.43996\n",
      "[290]\tvalidation_0-mlogloss:1.43892\n",
      "[291]\tvalidation_0-mlogloss:1.43833\n",
      "[292]\tvalidation_0-mlogloss:1.43730\n",
      "[293]\tvalidation_0-mlogloss:1.43704\n",
      "[294]\tvalidation_0-mlogloss:1.43675\n",
      "[295]\tvalidation_0-mlogloss:1.43624\n",
      "[296]\tvalidation_0-mlogloss:1.43538\n",
      "[297]\tvalidation_0-mlogloss:1.43532\n",
      "[298]\tvalidation_0-mlogloss:1.43470\n",
      "[299]\tvalidation_0-mlogloss:1.43393\n",
      "[300]\tvalidation_0-mlogloss:1.43363\n",
      "[301]\tvalidation_0-mlogloss:1.43251\n",
      "[302]\tvalidation_0-mlogloss:1.43233\n",
      "[303]\tvalidation_0-mlogloss:1.43107\n",
      "[304]\tvalidation_0-mlogloss:1.43054\n",
      "[305]\tvalidation_0-mlogloss:1.43017\n",
      "[306]\tvalidation_0-mlogloss:1.42946\n",
      "[307]\tvalidation_0-mlogloss:1.42927\n",
      "[308]\tvalidation_0-mlogloss:1.42853\n",
      "[309]\tvalidation_0-mlogloss:1.42774\n",
      "[310]\tvalidation_0-mlogloss:1.42737\n",
      "[311]\tvalidation_0-mlogloss:1.42743\n",
      "[312]\tvalidation_0-mlogloss:1.42639\n",
      "[313]\tvalidation_0-mlogloss:1.42618\n",
      "[314]\tvalidation_0-mlogloss:1.42548\n",
      "[315]\tvalidation_0-mlogloss:1.42560\n",
      "[316]\tvalidation_0-mlogloss:1.42510\n",
      "[317]\tvalidation_0-mlogloss:1.42462\n",
      "[318]\tvalidation_0-mlogloss:1.42418\n",
      "[319]\tvalidation_0-mlogloss:1.42366\n",
      "[320]\tvalidation_0-mlogloss:1.42410\n",
      "[321]\tvalidation_0-mlogloss:1.42371\n",
      "[322]\tvalidation_0-mlogloss:1.42335\n",
      "[323]\tvalidation_0-mlogloss:1.42240\n",
      "[324]\tvalidation_0-mlogloss:1.42227\n",
      "[325]\tvalidation_0-mlogloss:1.42140\n",
      "[326]\tvalidation_0-mlogloss:1.42134\n",
      "[327]\tvalidation_0-mlogloss:1.42106\n",
      "[328]\tvalidation_0-mlogloss:1.42007\n",
      "[329]\tvalidation_0-mlogloss:1.41955\n",
      "[330]\tvalidation_0-mlogloss:1.41898\n",
      "[331]\tvalidation_0-mlogloss:1.41787\n",
      "[332]\tvalidation_0-mlogloss:1.41702\n",
      "[333]\tvalidation_0-mlogloss:1.41660\n",
      "[334]\tvalidation_0-mlogloss:1.41681\n",
      "[335]\tvalidation_0-mlogloss:1.41585\n",
      "[336]\tvalidation_0-mlogloss:1.41530\n",
      "[337]\tvalidation_0-mlogloss:1.41508\n",
      "[338]\tvalidation_0-mlogloss:1.41443\n",
      "[339]\tvalidation_0-mlogloss:1.41358\n",
      "[340]\tvalidation_0-mlogloss:1.41352\n",
      "[341]\tvalidation_0-mlogloss:1.41288\n",
      "[342]\tvalidation_0-mlogloss:1.41227\n",
      "[343]\tvalidation_0-mlogloss:1.41178\n",
      "[344]\tvalidation_0-mlogloss:1.41182\n",
      "[345]\tvalidation_0-mlogloss:1.41169\n",
      "[346]\tvalidation_0-mlogloss:1.41179\n",
      "[347]\tvalidation_0-mlogloss:1.41123\n",
      "[348]\tvalidation_0-mlogloss:1.41083\n",
      "[349]\tvalidation_0-mlogloss:1.41055\n",
      "[350]\tvalidation_0-mlogloss:1.41019\n",
      "[351]\tvalidation_0-mlogloss:1.40924\n",
      "[352]\tvalidation_0-mlogloss:1.40834\n",
      "[353]\tvalidation_0-mlogloss:1.40809\n",
      "[354]\tvalidation_0-mlogloss:1.40760\n",
      "[355]\tvalidation_0-mlogloss:1.40749\n",
      "[356]\tvalidation_0-mlogloss:1.40696\n",
      "[357]\tvalidation_0-mlogloss:1.40646\n",
      "[358]\tvalidation_0-mlogloss:1.40624\n",
      "[359]\tvalidation_0-mlogloss:1.40578\n",
      "[360]\tvalidation_0-mlogloss:1.40502\n",
      "[361]\tvalidation_0-mlogloss:1.40498\n",
      "[362]\tvalidation_0-mlogloss:1.40474\n",
      "[363]\tvalidation_0-mlogloss:1.40473\n",
      "[364]\tvalidation_0-mlogloss:1.40466\n",
      "[365]\tvalidation_0-mlogloss:1.40462\n",
      "[366]\tvalidation_0-mlogloss:1.40445\n",
      "[367]\tvalidation_0-mlogloss:1.40412\n",
      "[368]\tvalidation_0-mlogloss:1.40438\n",
      "[369]\tvalidation_0-mlogloss:1.40421\n",
      "[370]\tvalidation_0-mlogloss:1.40427\n",
      "[371]\tvalidation_0-mlogloss:1.40417\n",
      "[372]\tvalidation_0-mlogloss:1.40390\n",
      "[373]\tvalidation_0-mlogloss:1.40331\n",
      "[374]\tvalidation_0-mlogloss:1.40288\n",
      "[375]\tvalidation_0-mlogloss:1.40288\n",
      "[376]\tvalidation_0-mlogloss:1.40230\n",
      "[377]\tvalidation_0-mlogloss:1.40186\n",
      "[378]\tvalidation_0-mlogloss:1.40113\n",
      "[379]\tvalidation_0-mlogloss:1.40093\n",
      "[380]\tvalidation_0-mlogloss:1.40050\n",
      "[381]\tvalidation_0-mlogloss:1.40006\n",
      "[382]\tvalidation_0-mlogloss:1.39949\n",
      "[383]\tvalidation_0-mlogloss:1.39882\n",
      "[384]\tvalidation_0-mlogloss:1.39869\n",
      "[385]\tvalidation_0-mlogloss:1.39854\n",
      "[386]\tvalidation_0-mlogloss:1.39825\n",
      "[387]\tvalidation_0-mlogloss:1.39779\n",
      "[388]\tvalidation_0-mlogloss:1.39791\n",
      "[389]\tvalidation_0-mlogloss:1.39775\n",
      "[390]\tvalidation_0-mlogloss:1.39751\n",
      "[391]\tvalidation_0-mlogloss:1.39748\n",
      "[392]\tvalidation_0-mlogloss:1.39728\n",
      "[393]\tvalidation_0-mlogloss:1.39725\n",
      "[394]\tvalidation_0-mlogloss:1.39693\n",
      "[395]\tvalidation_0-mlogloss:1.39627\n",
      "[396]\tvalidation_0-mlogloss:1.39564\n",
      "[397]\tvalidation_0-mlogloss:1.39508\n",
      "[398]\tvalidation_0-mlogloss:1.39460\n",
      "[399]\tvalidation_0-mlogloss:1.39465\n",
      "[400]\tvalidation_0-mlogloss:1.39448\n",
      "[401]\tvalidation_0-mlogloss:1.39504\n",
      "[402]\tvalidation_0-mlogloss:1.39494\n",
      "[403]\tvalidation_0-mlogloss:1.39414\n",
      "[404]\tvalidation_0-mlogloss:1.39400\n",
      "[405]\tvalidation_0-mlogloss:1.39421\n",
      "[406]\tvalidation_0-mlogloss:1.39345\n",
      "[407]\tvalidation_0-mlogloss:1.39353\n",
      "[408]\tvalidation_0-mlogloss:1.39338\n",
      "[409]\tvalidation_0-mlogloss:1.39378\n",
      "[410]\tvalidation_0-mlogloss:1.39350\n",
      "[411]\tvalidation_0-mlogloss:1.39305\n",
      "[412]\tvalidation_0-mlogloss:1.39241\n",
      "[413]\tvalidation_0-mlogloss:1.39224\n",
      "[414]\tvalidation_0-mlogloss:1.39214\n",
      "[415]\tvalidation_0-mlogloss:1.39160\n",
      "[416]\tvalidation_0-mlogloss:1.39177\n",
      "[417]\tvalidation_0-mlogloss:1.39175\n",
      "[418]\tvalidation_0-mlogloss:1.39171\n",
      "[419]\tvalidation_0-mlogloss:1.39114\n",
      "[420]\tvalidation_0-mlogloss:1.39099\n",
      "[421]\tvalidation_0-mlogloss:1.39074\n",
      "[422]\tvalidation_0-mlogloss:1.39052\n",
      "[423]\tvalidation_0-mlogloss:1.39021\n",
      "[424]\tvalidation_0-mlogloss:1.39051\n",
      "[425]\tvalidation_0-mlogloss:1.39041\n",
      "[426]\tvalidation_0-mlogloss:1.38965\n",
      "[427]\tvalidation_0-mlogloss:1.38948\n",
      "[428]\tvalidation_0-mlogloss:1.38957\n",
      "[429]\tvalidation_0-mlogloss:1.38937\n",
      "[430]\tvalidation_0-mlogloss:1.38972\n",
      "[431]\tvalidation_0-mlogloss:1.38920\n",
      "[432]\tvalidation_0-mlogloss:1.38964\n",
      "[433]\tvalidation_0-mlogloss:1.38919\n",
      "[434]\tvalidation_0-mlogloss:1.38916\n",
      "[435]\tvalidation_0-mlogloss:1.38893\n",
      "[436]\tvalidation_0-mlogloss:1.38854\n",
      "[437]\tvalidation_0-mlogloss:1.38820\n",
      "[438]\tvalidation_0-mlogloss:1.38773\n",
      "[439]\tvalidation_0-mlogloss:1.38724\n",
      "[440]\tvalidation_0-mlogloss:1.38680\n",
      "[441]\tvalidation_0-mlogloss:1.38642\n",
      "[442]\tvalidation_0-mlogloss:1.38594\n",
      "[443]\tvalidation_0-mlogloss:1.38560\n",
      "[444]\tvalidation_0-mlogloss:1.38510\n",
      "[445]\tvalidation_0-mlogloss:1.38508\n",
      "[446]\tvalidation_0-mlogloss:1.38490\n",
      "[447]\tvalidation_0-mlogloss:1.38442\n",
      "[448]\tvalidation_0-mlogloss:1.38414\n",
      "[449]\tvalidation_0-mlogloss:1.38357\n",
      "[450]\tvalidation_0-mlogloss:1.38352\n",
      "[451]\tvalidation_0-mlogloss:1.38326\n",
      "[452]\tvalidation_0-mlogloss:1.38306\n",
      "[453]\tvalidation_0-mlogloss:1.38309\n",
      "[454]\tvalidation_0-mlogloss:1.38297\n",
      "[455]\tvalidation_0-mlogloss:1.38267\n",
      "[456]\tvalidation_0-mlogloss:1.38240\n",
      "[457]\tvalidation_0-mlogloss:1.38196\n",
      "[458]\tvalidation_0-mlogloss:1.38188\n",
      "[459]\tvalidation_0-mlogloss:1.38119\n",
      "[460]\tvalidation_0-mlogloss:1.38121\n",
      "[461]\tvalidation_0-mlogloss:1.38102\n",
      "[462]\tvalidation_0-mlogloss:1.38100\n",
      "[463]\tvalidation_0-mlogloss:1.38064\n",
      "[464]\tvalidation_0-mlogloss:1.38066\n",
      "[465]\tvalidation_0-mlogloss:1.38082\n",
      "[466]\tvalidation_0-mlogloss:1.38070\n",
      "[467]\tvalidation_0-mlogloss:1.38089\n",
      "[468]\tvalidation_0-mlogloss:1.38079\n",
      "[469]\tvalidation_0-mlogloss:1.38033\n",
      "[470]\tvalidation_0-mlogloss:1.38025\n",
      "[471]\tvalidation_0-mlogloss:1.38014\n",
      "[472]\tvalidation_0-mlogloss:1.38009\n",
      "[473]\tvalidation_0-mlogloss:1.38013\n",
      "[474]\tvalidation_0-mlogloss:1.38021\n",
      "[475]\tvalidation_0-mlogloss:1.37988\n",
      "[476]\tvalidation_0-mlogloss:1.38023\n",
      "[477]\tvalidation_0-mlogloss:1.38018\n",
      "[478]\tvalidation_0-mlogloss:1.38050\n",
      "[479]\tvalidation_0-mlogloss:1.38015\n",
      "[480]\tvalidation_0-mlogloss:1.38031\n",
      "[481]\tvalidation_0-mlogloss:1.38013\n",
      "[482]\tvalidation_0-mlogloss:1.38022\n",
      "[483]\tvalidation_0-mlogloss:1.37981\n",
      "[484]\tvalidation_0-mlogloss:1.37981\n",
      "[485]\tvalidation_0-mlogloss:1.37981\n",
      "[486]\tvalidation_0-mlogloss:1.37973\n",
      "[487]\tvalidation_0-mlogloss:1.37938\n",
      "[488]\tvalidation_0-mlogloss:1.37918\n",
      "[489]\tvalidation_0-mlogloss:1.37916\n",
      "[490]\tvalidation_0-mlogloss:1.37917\n",
      "[491]\tvalidation_0-mlogloss:1.37919\n",
      "[492]\tvalidation_0-mlogloss:1.37895\n",
      "[493]\tvalidation_0-mlogloss:1.37861\n",
      "[494]\tvalidation_0-mlogloss:1.37840\n",
      "[495]\tvalidation_0-mlogloss:1.37826\n",
      "[496]\tvalidation_0-mlogloss:1.37797\n",
      "[497]\tvalidation_0-mlogloss:1.37782\n",
      "[498]\tvalidation_0-mlogloss:1.37737\n",
      "[499]\tvalidation_0-mlogloss:1.37690\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      1499\n",
      "           1       0.59      0.80      0.68      1499\n",
      "           2       0.65      0.61      0.63      1499\n",
      "           3       0.74      0.35      0.48      1499\n",
      "           4       0.91      0.93      0.92      1499\n",
      "           5       0.89      0.91      0.90      1499\n",
      "           6       0.86      0.95      0.90      1499\n",
      "           7       0.91      0.61      0.73      1499\n",
      "           8       0.42      0.81      0.55      1499\n",
      "           9       0.75      0.67      0.70      1499\n",
      "          10       0.97      0.90      0.93      1499\n",
      "          11       0.92      0.91      0.91      1499\n",
      "          12       0.66      0.81      0.73      1499\n",
      "          13       0.81      0.90      0.85      1499\n",
      "          14       0.57      0.75      0.65      1499\n",
      "          15       0.83      0.40      0.54      1499\n",
      "          16       0.24      0.14      0.17      1499\n",
      "          17       0.67      0.75      0.70      1499\n",
      "          18       0.68      0.68      0.68      1499\n",
      "          19       0.60      0.53      0.56      1499\n",
      "          20       0.94      0.98      0.96      1499\n",
      "          21       0.70      0.79      0.74      1499\n",
      "          22       0.68      0.73      0.71      1499\n",
      "          23       0.89      0.96      0.92      1499\n",
      "          24       0.96      0.88      0.92      1499\n",
      "          25       0.74      0.56      0.63      1499\n",
      "          26       0.55      0.39      0.45      1499\n",
      "          27       0.61      0.46      0.52      1499\n",
      "          28       0.63      0.44      0.52      1499\n",
      "          29       0.68      0.92      0.78      1499\n",
      "          30       0.61      0.83      0.70      1499\n",
      "          31       0.46      0.77      0.57      1499\n",
      "          32       0.75      0.93      0.83      1499\n",
      "          33       0.92      0.46      0.61      1499\n",
      "          34       0.91      0.96      0.93      1499\n",
      "          35       0.75      0.84      0.79      1499\n",
      "          36       0.87      0.65      0.74      1499\n",
      "          37       0.69      0.76      0.72      1499\n",
      "          38       0.57      0.25      0.35      1499\n",
      "          39       0.71      0.58      0.64      1499\n",
      "          40       0.89      0.58      0.70      1499\n",
      "          41       0.67      0.48      0.56      1499\n",
      "          42       0.54      0.42      0.48      1499\n",
      "          43       0.71      0.75      0.73      1499\n",
      "          44       0.86      0.94      0.89      1499\n",
      "          45       0.58      0.46      0.51      1499\n",
      "          46       0.59      0.78      0.67      1499\n",
      "          47       0.77      0.53      0.63      1499\n",
      "          48       0.79      0.88      0.83      1499\n",
      "          49       0.61      0.78      0.68      1499\n",
      "          50       0.81      0.68      0.74      1499\n",
      "          51       0.70      0.73      0.72      1499\n",
      "          52       0.64      0.74      0.69      1499\n",
      "          53       0.67      0.70      0.69      1499\n",
      "          54       0.70      0.82      0.75      1499\n",
      "          55       0.52      0.71      0.60      1499\n",
      "          56       0.75      0.85      0.80      1499\n",
      "          57       0.69      0.74      0.71      1499\n",
      "          58       0.88      0.64      0.74      1499\n",
      "          59       0.37      0.30      0.34      1499\n",
      "          60       0.65      0.66      0.65      1499\n",
      "          61       0.42      0.08      0.14      1499\n",
      "          62       0.87      0.93      0.90      1499\n",
      "          63       0.58      0.60      0.59      1499\n",
      "          64       0.59      0.64      0.61      1499\n",
      "          65       0.66      0.93      0.77      1499\n",
      "          66       0.87      0.80      0.83      1499\n",
      "          67       0.28      0.25      0.26      1499\n",
      "          68       0.70      0.83      0.76      1499\n",
      "          69       0.45      0.55      0.50      1499\n",
      "          70       0.79      0.56      0.65      1499\n",
      "          71       0.51      0.28      0.36      1499\n",
      "          72       0.69      0.74      0.71      1499\n",
      "          73       0.64      0.55      0.59      1499\n",
      "          74       0.34      0.72      0.46      1499\n",
      "\n",
      "    accuracy                           0.68    112425\n",
      "   macro avg       0.69      0.68      0.67    112425\n",
      "weighted avg       0.69      0.68      0.67    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300aa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baab7fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 39s 1ms/step - loss: 1.3949 - val_loss: 2.0504\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 45s 2ms/step - loss: 0.6215 - val_loss: 2.0341\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.4638 - val_loss: 1.9311\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.3971 - val_loss: 2.0182\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.3580 - val_loss: 1.9310\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 52s 2ms/step - loss: 0.3303 - val_loss: 1.8785\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 50s 2ms/step - loss: 0.3107 - val_loss: 1.9678\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.2952 - val_loss: 1.8815\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 51s 2ms/step - loss: 0.2813 - val_loss: 1.8971\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 52s 2ms/step - loss: 0.2704 - val_loss: 1.9719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5070096350>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  48/3514 [..............................] - ETA: 3s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 4s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73      1499\n",
      "           1       0.58      0.72      0.64      1499\n",
      "           2       0.70      0.56      0.62      1499\n",
      "           3       0.76      0.44      0.56      1499\n",
      "           4       0.93      0.91      0.92      1499\n",
      "           5       0.99      0.76      0.86      1499\n",
      "           6       0.98      0.81      0.89      1499\n",
      "           7       0.86      0.42      0.56      1499\n",
      "           8       0.32      0.74      0.45      1499\n",
      "           9       0.66      0.77      0.71      1499\n",
      "          10       0.63      0.90      0.74      1499\n",
      "          11       0.87      0.84      0.86      1499\n",
      "          12       0.69      0.67      0.68      1499\n",
      "          13       0.83      0.77      0.80      1499\n",
      "          14       0.56      0.62      0.59      1499\n",
      "          15       0.72      0.29      0.41      1499\n",
      "          16       0.17      0.06      0.09      1499\n",
      "          17       0.68      0.92      0.78      1499\n",
      "          18       0.64      0.64      0.64      1499\n",
      "          19       0.35      0.24      0.28      1499\n",
      "          20       0.90      0.97      0.93      1499\n",
      "          21       0.77      0.73      0.75      1499\n",
      "          22       0.63      0.59      0.61      1499\n",
      "          23       0.93      0.94      0.93      1499\n",
      "          24       0.97      0.64      0.77      1499\n",
      "          25       0.69      0.36      0.47      1499\n",
      "          26       0.55      0.42      0.48      1499\n",
      "          27       0.67      0.54      0.60      1499\n",
      "          28       0.53      0.57      0.55      1499\n",
      "          29       0.75      0.83      0.79      1499\n",
      "          30       0.78      0.78      0.78      1499\n",
      "          31       0.51      0.61      0.56      1499\n",
      "          32       0.78      0.73      0.76      1499\n",
      "          33       0.83      0.30      0.44      1499\n",
      "          34       0.94      0.92      0.93      1499\n",
      "          35       0.65      0.91      0.76      1499\n",
      "          36       0.89      0.54      0.67      1499\n",
      "          37       0.53      0.75      0.62      1499\n",
      "          38       0.60      0.40      0.48      1499\n",
      "          39       0.64      0.61      0.62      1499\n",
      "          40       0.89      0.43      0.58      1499\n",
      "          41       0.80      0.50      0.62      1499\n",
      "          42       0.51      0.59      0.55      1499\n",
      "          43       0.55      0.74      0.63      1499\n",
      "          44       0.66      0.93      0.77      1499\n",
      "          45       0.50      0.56      0.53      1499\n",
      "          46       0.54      0.80      0.64      1499\n",
      "          47       0.79      0.62      0.69      1499\n",
      "          48       0.90      0.81      0.85      1499\n",
      "          49       0.61      0.53      0.57      1499\n",
      "          50       0.90      0.54      0.67      1499\n",
      "          51       0.58      0.67      0.62      1499\n",
      "          52       0.59      0.76      0.66      1499\n",
      "          53       0.60      0.89      0.71      1499\n",
      "          54       0.59      0.65      0.62      1499\n",
      "          55       0.49      0.78      0.60      1499\n",
      "          56       0.61      0.81      0.70      1499\n",
      "          57       0.81      0.70      0.75      1499\n",
      "          58       0.78      0.41      0.54      1499\n",
      "          59       0.42      0.43      0.42      1499\n",
      "          60       0.80      0.39      0.53      1499\n",
      "          61       0.53      0.09      0.15      1499\n",
      "          62       0.86      0.89      0.87      1499\n",
      "          63       0.49      0.52      0.50      1499\n",
      "          64       0.58      0.68      0.63      1499\n",
      "          65       0.65      0.92      0.76      1499\n",
      "          66       0.80      0.69      0.74      1499\n",
      "          67       0.28      0.43      0.34      1499\n",
      "          68       0.71      0.80      0.76      1499\n",
      "          69       0.41      0.63      0.50      1499\n",
      "          70       0.58      0.47      0.52      1499\n",
      "          71       0.41      0.21      0.28      1499\n",
      "          72       0.68      0.55      0.61      1499\n",
      "          73       0.61      0.68      0.64      1499\n",
      "          74       0.29      0.74      0.41      1499\n",
      "\n",
      "    accuracy                           0.64    112425\n",
      "   macro avg       0.67      0.64      0.63    112425\n",
      "weighted avg       0.67      0.64      0.63    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.56699\n",
      "[1]\tvalidation_0-mlogloss:3.28247\n",
      "[2]\tvalidation_0-mlogloss:3.08448\n",
      "[3]\tvalidation_0-mlogloss:2.93867\n",
      "[4]\tvalidation_0-mlogloss:2.82752\n",
      "[5]\tvalidation_0-mlogloss:2.73256\n",
      "[6]\tvalidation_0-mlogloss:2.64895\n",
      "[7]\tvalidation_0-mlogloss:2.57560\n",
      "[8]\tvalidation_0-mlogloss:2.51188\n",
      "[9]\tvalidation_0-mlogloss:2.44918\n",
      "[10]\tvalidation_0-mlogloss:2.39711\n",
      "[11]\tvalidation_0-mlogloss:2.34857\n",
      "[12]\tvalidation_0-mlogloss:2.30457\n",
      "[13]\tvalidation_0-mlogloss:2.25777\n",
      "[14]\tvalidation_0-mlogloss:2.21734\n",
      "[15]\tvalidation_0-mlogloss:2.18389\n",
      "[16]\tvalidation_0-mlogloss:2.15231\n",
      "[17]\tvalidation_0-mlogloss:2.12102\n",
      "[18]\tvalidation_0-mlogloss:2.09310\n",
      "[19]\tvalidation_0-mlogloss:2.06643\n",
      "[20]\tvalidation_0-mlogloss:2.03634\n",
      "[21]\tvalidation_0-mlogloss:2.01174\n",
      "[22]\tvalidation_0-mlogloss:1.99032\n",
      "[23]\tvalidation_0-mlogloss:1.96759\n",
      "[24]\tvalidation_0-mlogloss:1.94592\n",
      "[25]\tvalidation_0-mlogloss:1.92589\n",
      "[26]\tvalidation_0-mlogloss:1.90526\n",
      "[27]\tvalidation_0-mlogloss:1.88710\n",
      "[28]\tvalidation_0-mlogloss:1.86826\n",
      "[29]\tvalidation_0-mlogloss:1.84967\n",
      "[30]\tvalidation_0-mlogloss:1.83223\n",
      "[31]\tvalidation_0-mlogloss:1.81480\n",
      "[32]\tvalidation_0-mlogloss:1.79846\n",
      "[33]\tvalidation_0-mlogloss:1.78527\n",
      "[34]\tvalidation_0-mlogloss:1.77178\n",
      "[35]\tvalidation_0-mlogloss:1.75781\n",
      "[36]\tvalidation_0-mlogloss:1.74296\n",
      "[37]\tvalidation_0-mlogloss:1.72851\n",
      "[38]\tvalidation_0-mlogloss:1.71397\n",
      "[39]\tvalidation_0-mlogloss:1.70105\n",
      "[40]\tvalidation_0-mlogloss:1.68830\n",
      "[41]\tvalidation_0-mlogloss:1.67460\n",
      "[42]\tvalidation_0-mlogloss:1.66132\n",
      "[43]\tvalidation_0-mlogloss:1.64963\n",
      "[44]\tvalidation_0-mlogloss:1.63645\n",
      "[45]\tvalidation_0-mlogloss:1.62460\n",
      "[46]\tvalidation_0-mlogloss:1.61589\n",
      "[47]\tvalidation_0-mlogloss:1.60635\n",
      "[48]\tvalidation_0-mlogloss:1.59765\n",
      "[49]\tvalidation_0-mlogloss:1.58665\n",
      "[50]\tvalidation_0-mlogloss:1.57596\n",
      "[51]\tvalidation_0-mlogloss:1.56544\n",
      "[52]\tvalidation_0-mlogloss:1.55666\n",
      "[53]\tvalidation_0-mlogloss:1.54570\n",
      "[54]\tvalidation_0-mlogloss:1.53897\n",
      "[55]\tvalidation_0-mlogloss:1.53022\n",
      "[56]\tvalidation_0-mlogloss:1.52046\n",
      "[57]\tvalidation_0-mlogloss:1.51175\n",
      "[58]\tvalidation_0-mlogloss:1.50507\n",
      "[59]\tvalidation_0-mlogloss:1.49685\n",
      "[60]\tvalidation_0-mlogloss:1.48990\n",
      "[61]\tvalidation_0-mlogloss:1.48170\n",
      "[62]\tvalidation_0-mlogloss:1.47163\n",
      "[63]\tvalidation_0-mlogloss:1.46095\n",
      "[64]\tvalidation_0-mlogloss:1.45182\n",
      "[65]\tvalidation_0-mlogloss:1.44433\n",
      "[66]\tvalidation_0-mlogloss:1.43714\n",
      "[67]\tvalidation_0-mlogloss:1.42969\n",
      "[68]\tvalidation_0-mlogloss:1.42343\n",
      "[69]\tvalidation_0-mlogloss:1.41617\n",
      "[70]\tvalidation_0-mlogloss:1.40799\n",
      "[71]\tvalidation_0-mlogloss:1.39966\n",
      "[72]\tvalidation_0-mlogloss:1.39281\n",
      "[73]\tvalidation_0-mlogloss:1.38595\n",
      "[74]\tvalidation_0-mlogloss:1.37980\n",
      "[75]\tvalidation_0-mlogloss:1.37312\n",
      "[76]\tvalidation_0-mlogloss:1.36773\n",
      "[77]\tvalidation_0-mlogloss:1.36200\n",
      "[78]\tvalidation_0-mlogloss:1.35593\n",
      "[79]\tvalidation_0-mlogloss:1.35062\n",
      "[80]\tvalidation_0-mlogloss:1.34625\n",
      "[81]\tvalidation_0-mlogloss:1.33973\n",
      "[82]\tvalidation_0-mlogloss:1.33415\n",
      "[83]\tvalidation_0-mlogloss:1.32882\n",
      "[84]\tvalidation_0-mlogloss:1.32383\n",
      "[85]\tvalidation_0-mlogloss:1.31897\n",
      "[86]\tvalidation_0-mlogloss:1.31419\n",
      "[87]\tvalidation_0-mlogloss:1.30836\n",
      "[88]\tvalidation_0-mlogloss:1.30354\n",
      "[89]\tvalidation_0-mlogloss:1.30014\n",
      "[90]\tvalidation_0-mlogloss:1.29548\n",
      "[91]\tvalidation_0-mlogloss:1.28988\n",
      "[92]\tvalidation_0-mlogloss:1.28510\n",
      "[93]\tvalidation_0-mlogloss:1.27910\n",
      "[94]\tvalidation_0-mlogloss:1.27408\n",
      "[95]\tvalidation_0-mlogloss:1.26936\n",
      "[96]\tvalidation_0-mlogloss:1.26405\n",
      "[97]\tvalidation_0-mlogloss:1.25955\n",
      "[98]\tvalidation_0-mlogloss:1.25545\n",
      "[99]\tvalidation_0-mlogloss:1.25095\n",
      "[100]\tvalidation_0-mlogloss:1.24701\n",
      "[101]\tvalidation_0-mlogloss:1.24054\n",
      "[102]\tvalidation_0-mlogloss:1.23658\n",
      "[103]\tvalidation_0-mlogloss:1.23218\n",
      "[104]\tvalidation_0-mlogloss:1.22827\n",
      "[105]\tvalidation_0-mlogloss:1.22479\n",
      "[106]\tvalidation_0-mlogloss:1.22053\n",
      "[107]\tvalidation_0-mlogloss:1.21603\n",
      "[108]\tvalidation_0-mlogloss:1.21137\n",
      "[109]\tvalidation_0-mlogloss:1.20757\n",
      "[110]\tvalidation_0-mlogloss:1.20449\n",
      "[111]\tvalidation_0-mlogloss:1.20101\n",
      "[112]\tvalidation_0-mlogloss:1.19823\n",
      "[113]\tvalidation_0-mlogloss:1.19422\n",
      "[114]\tvalidation_0-mlogloss:1.19089\n",
      "[115]\tvalidation_0-mlogloss:1.18800\n",
      "[116]\tvalidation_0-mlogloss:1.18554\n",
      "[117]\tvalidation_0-mlogloss:1.18068\n",
      "[118]\tvalidation_0-mlogloss:1.17663\n",
      "[119]\tvalidation_0-mlogloss:1.17376\n",
      "[120]\tvalidation_0-mlogloss:1.17135\n",
      "[121]\tvalidation_0-mlogloss:1.16790\n",
      "[122]\tvalidation_0-mlogloss:1.16465\n",
      "[123]\tvalidation_0-mlogloss:1.16179\n",
      "[124]\tvalidation_0-mlogloss:1.15748\n",
      "[125]\tvalidation_0-mlogloss:1.15482\n",
      "[126]\tvalidation_0-mlogloss:1.15160\n",
      "[127]\tvalidation_0-mlogloss:1.14849\n",
      "[128]\tvalidation_0-mlogloss:1.14543\n",
      "[129]\tvalidation_0-mlogloss:1.14252\n",
      "[130]\tvalidation_0-mlogloss:1.13995\n",
      "[131]\tvalidation_0-mlogloss:1.13770\n",
      "[132]\tvalidation_0-mlogloss:1.13551\n",
      "[133]\tvalidation_0-mlogloss:1.13249\n",
      "[134]\tvalidation_0-mlogloss:1.13007\n",
      "[135]\tvalidation_0-mlogloss:1.12754\n",
      "[136]\tvalidation_0-mlogloss:1.12452\n",
      "[137]\tvalidation_0-mlogloss:1.12230\n",
      "[138]\tvalidation_0-mlogloss:1.11939\n",
      "[139]\tvalidation_0-mlogloss:1.11654\n",
      "[140]\tvalidation_0-mlogloss:1.11422\n",
      "[141]\tvalidation_0-mlogloss:1.11238\n",
      "[142]\tvalidation_0-mlogloss:1.10973\n",
      "[143]\tvalidation_0-mlogloss:1.10747\n",
      "[144]\tvalidation_0-mlogloss:1.10403\n",
      "[145]\tvalidation_0-mlogloss:1.10203\n",
      "[146]\tvalidation_0-mlogloss:1.09925\n",
      "[147]\tvalidation_0-mlogloss:1.09790\n",
      "[148]\tvalidation_0-mlogloss:1.09666\n",
      "[149]\tvalidation_0-mlogloss:1.09437\n",
      "[150]\tvalidation_0-mlogloss:1.09055\n",
      "[151]\tvalidation_0-mlogloss:1.08819\n",
      "[152]\tvalidation_0-mlogloss:1.08551\n",
      "[153]\tvalidation_0-mlogloss:1.08334\n",
      "[154]\tvalidation_0-mlogloss:1.08194\n",
      "[155]\tvalidation_0-mlogloss:1.07908\n",
      "[156]\tvalidation_0-mlogloss:1.07729\n",
      "[157]\tvalidation_0-mlogloss:1.07486\n",
      "[158]\tvalidation_0-mlogloss:1.07228\n",
      "[159]\tvalidation_0-mlogloss:1.06931\n",
      "[160]\tvalidation_0-mlogloss:1.06688\n",
      "[161]\tvalidation_0-mlogloss:1.06459\n",
      "[162]\tvalidation_0-mlogloss:1.06265\n",
      "[163]\tvalidation_0-mlogloss:1.06083\n",
      "[164]\tvalidation_0-mlogloss:1.05892\n",
      "[165]\tvalidation_0-mlogloss:1.05693\n",
      "[166]\tvalidation_0-mlogloss:1.05515\n",
      "[167]\tvalidation_0-mlogloss:1.05394\n",
      "[168]\tvalidation_0-mlogloss:1.05218\n",
      "[169]\tvalidation_0-mlogloss:1.04935\n",
      "[170]\tvalidation_0-mlogloss:1.04765\n",
      "[171]\tvalidation_0-mlogloss:1.04567\n",
      "[172]\tvalidation_0-mlogloss:1.04372\n",
      "[173]\tvalidation_0-mlogloss:1.04287\n",
      "[174]\tvalidation_0-mlogloss:1.04107\n",
      "[175]\tvalidation_0-mlogloss:1.04021\n",
      "[176]\tvalidation_0-mlogloss:1.03860\n",
      "[177]\tvalidation_0-mlogloss:1.03661\n",
      "[178]\tvalidation_0-mlogloss:1.03496\n",
      "[179]\tvalidation_0-mlogloss:1.03384\n",
      "[180]\tvalidation_0-mlogloss:1.03233\n",
      "[181]\tvalidation_0-mlogloss:1.03072\n",
      "[182]\tvalidation_0-mlogloss:1.02965\n",
      "[183]\tvalidation_0-mlogloss:1.02802\n",
      "[184]\tvalidation_0-mlogloss:1.02704\n",
      "[185]\tvalidation_0-mlogloss:1.02526\n",
      "[186]\tvalidation_0-mlogloss:1.02379\n",
      "[187]\tvalidation_0-mlogloss:1.02236\n",
      "[188]\tvalidation_0-mlogloss:1.02083\n",
      "[189]\tvalidation_0-mlogloss:1.01905\n",
      "[190]\tvalidation_0-mlogloss:1.01767\n",
      "[191]\tvalidation_0-mlogloss:1.01676\n",
      "[192]\tvalidation_0-mlogloss:1.01562\n",
      "[193]\tvalidation_0-mlogloss:1.01393\n",
      "[194]\tvalidation_0-mlogloss:1.01291\n",
      "[195]\tvalidation_0-mlogloss:1.01110\n",
      "[196]\tvalidation_0-mlogloss:1.01062\n",
      "[197]\tvalidation_0-mlogloss:1.00864\n",
      "[198]\tvalidation_0-mlogloss:1.00773\n",
      "[199]\tvalidation_0-mlogloss:1.00613\n",
      "[200]\tvalidation_0-mlogloss:1.00434\n",
      "[201]\tvalidation_0-mlogloss:1.00311\n",
      "[202]\tvalidation_0-mlogloss:1.00178\n",
      "[203]\tvalidation_0-mlogloss:1.00052\n",
      "[204]\tvalidation_0-mlogloss:0.99954\n",
      "[205]\tvalidation_0-mlogloss:0.99821\n",
      "[206]\tvalidation_0-mlogloss:0.99771\n",
      "[207]\tvalidation_0-mlogloss:0.99733\n",
      "[208]\tvalidation_0-mlogloss:0.99564\n",
      "[209]\tvalidation_0-mlogloss:0.99484\n",
      "[210]\tvalidation_0-mlogloss:0.99338\n",
      "[211]\tvalidation_0-mlogloss:0.99293\n",
      "[212]\tvalidation_0-mlogloss:0.99158\n",
      "[213]\tvalidation_0-mlogloss:0.99055\n",
      "[214]\tvalidation_0-mlogloss:0.98948\n",
      "[215]\tvalidation_0-mlogloss:0.98862\n",
      "[216]\tvalidation_0-mlogloss:0.98771\n",
      "[217]\tvalidation_0-mlogloss:0.98620\n",
      "[218]\tvalidation_0-mlogloss:0.98539\n",
      "[219]\tvalidation_0-mlogloss:0.98461\n",
      "[220]\tvalidation_0-mlogloss:0.98359\n",
      "[221]\tvalidation_0-mlogloss:0.98237\n",
      "[222]\tvalidation_0-mlogloss:0.98144\n",
      "[223]\tvalidation_0-mlogloss:0.98059\n",
      "[224]\tvalidation_0-mlogloss:0.97965\n",
      "[225]\tvalidation_0-mlogloss:0.97865\n",
      "[226]\tvalidation_0-mlogloss:0.97774\n",
      "[227]\tvalidation_0-mlogloss:0.97729\n",
      "[228]\tvalidation_0-mlogloss:0.97633\n",
      "[229]\tvalidation_0-mlogloss:0.97549\n",
      "[230]\tvalidation_0-mlogloss:0.97476\n",
      "[231]\tvalidation_0-mlogloss:0.97372\n",
      "[232]\tvalidation_0-mlogloss:0.97243\n",
      "[233]\tvalidation_0-mlogloss:0.97132\n",
      "[234]\tvalidation_0-mlogloss:0.97030\n",
      "[235]\tvalidation_0-mlogloss:0.96887\n",
      "[236]\tvalidation_0-mlogloss:0.96800\n",
      "[237]\tvalidation_0-mlogloss:0.96719\n",
      "[238]\tvalidation_0-mlogloss:0.96640\n",
      "[239]\tvalidation_0-mlogloss:0.96562\n",
      "[240]\tvalidation_0-mlogloss:0.96427\n",
      "[241]\tvalidation_0-mlogloss:0.96314\n",
      "[242]\tvalidation_0-mlogloss:0.96121\n",
      "[243]\tvalidation_0-mlogloss:0.96101\n",
      "[244]\tvalidation_0-mlogloss:0.95962\n",
      "[245]\tvalidation_0-mlogloss:0.95803\n",
      "[246]\tvalidation_0-mlogloss:0.95637\n",
      "[247]\tvalidation_0-mlogloss:0.95532\n",
      "[248]\tvalidation_0-mlogloss:0.95441\n",
      "[249]\tvalidation_0-mlogloss:0.95352\n",
      "[250]\tvalidation_0-mlogloss:0.95242\n",
      "[251]\tvalidation_0-mlogloss:0.95187\n",
      "[252]\tvalidation_0-mlogloss:0.95089\n",
      "[253]\tvalidation_0-mlogloss:0.95039\n",
      "[254]\tvalidation_0-mlogloss:0.94930\n",
      "[255]\tvalidation_0-mlogloss:0.94829\n",
      "[256]\tvalidation_0-mlogloss:0.94771\n",
      "[257]\tvalidation_0-mlogloss:0.94730\n",
      "[258]\tvalidation_0-mlogloss:0.94644\n",
      "[259]\tvalidation_0-mlogloss:0.94580\n",
      "[260]\tvalidation_0-mlogloss:0.94512\n",
      "[261]\tvalidation_0-mlogloss:0.94442\n",
      "[262]\tvalidation_0-mlogloss:0.94371\n",
      "[263]\tvalidation_0-mlogloss:0.94280\n",
      "[264]\tvalidation_0-mlogloss:0.94209\n",
      "[265]\tvalidation_0-mlogloss:0.94155\n",
      "[266]\tvalidation_0-mlogloss:0.94118\n",
      "[267]\tvalidation_0-mlogloss:0.94033\n",
      "[268]\tvalidation_0-mlogloss:0.93980\n",
      "[269]\tvalidation_0-mlogloss:0.93942\n",
      "[270]\tvalidation_0-mlogloss:0.93824\n",
      "[271]\tvalidation_0-mlogloss:0.93729\n",
      "[272]\tvalidation_0-mlogloss:0.93582\n",
      "[273]\tvalidation_0-mlogloss:0.93524\n",
      "[274]\tvalidation_0-mlogloss:0.93469\n",
      "[275]\tvalidation_0-mlogloss:0.93427\n",
      "[276]\tvalidation_0-mlogloss:0.93385\n",
      "[277]\tvalidation_0-mlogloss:0.93300\n",
      "[278]\tvalidation_0-mlogloss:0.93222\n",
      "[279]\tvalidation_0-mlogloss:0.93140\n",
      "[280]\tvalidation_0-mlogloss:0.93071\n",
      "[281]\tvalidation_0-mlogloss:0.92984\n",
      "[282]\tvalidation_0-mlogloss:0.92902\n",
      "[283]\tvalidation_0-mlogloss:0.92868\n",
      "[284]\tvalidation_0-mlogloss:0.92845\n",
      "[285]\tvalidation_0-mlogloss:0.92770\n",
      "[286]\tvalidation_0-mlogloss:0.92699\n",
      "[287]\tvalidation_0-mlogloss:0.92644\n",
      "[288]\tvalidation_0-mlogloss:0.92563\n",
      "[289]\tvalidation_0-mlogloss:0.92511\n",
      "[290]\tvalidation_0-mlogloss:0.92455\n",
      "[291]\tvalidation_0-mlogloss:0.92385\n",
      "[292]\tvalidation_0-mlogloss:0.92333\n",
      "[293]\tvalidation_0-mlogloss:0.92240\n",
      "[294]\tvalidation_0-mlogloss:0.92194\n",
      "[295]\tvalidation_0-mlogloss:0.92110\n",
      "[296]\tvalidation_0-mlogloss:0.92059\n",
      "[297]\tvalidation_0-mlogloss:0.92008\n",
      "[298]\tvalidation_0-mlogloss:0.91942\n",
      "[299]\tvalidation_0-mlogloss:0.91889\n",
      "[300]\tvalidation_0-mlogloss:0.91816\n",
      "[301]\tvalidation_0-mlogloss:0.91732\n",
      "[302]\tvalidation_0-mlogloss:0.91652\n",
      "[303]\tvalidation_0-mlogloss:0.91614\n",
      "[304]\tvalidation_0-mlogloss:0.91556\n",
      "[305]\tvalidation_0-mlogloss:0.91486\n",
      "[306]\tvalidation_0-mlogloss:0.91471\n",
      "[307]\tvalidation_0-mlogloss:0.91399\n",
      "[308]\tvalidation_0-mlogloss:0.91355\n",
      "[309]\tvalidation_0-mlogloss:0.91289\n",
      "[310]\tvalidation_0-mlogloss:0.91249\n",
      "[311]\tvalidation_0-mlogloss:0.91223\n",
      "[312]\tvalidation_0-mlogloss:0.91198\n",
      "[313]\tvalidation_0-mlogloss:0.91159\n",
      "[314]\tvalidation_0-mlogloss:0.91088\n",
      "[315]\tvalidation_0-mlogloss:0.91001\n",
      "[316]\tvalidation_0-mlogloss:0.90925\n",
      "[317]\tvalidation_0-mlogloss:0.90886\n",
      "[318]\tvalidation_0-mlogloss:0.90813\n",
      "[319]\tvalidation_0-mlogloss:0.90767\n",
      "[320]\tvalidation_0-mlogloss:0.90685\n",
      "[321]\tvalidation_0-mlogloss:0.90617\n",
      "[322]\tvalidation_0-mlogloss:0.90557\n",
      "[323]\tvalidation_0-mlogloss:0.90537\n",
      "[324]\tvalidation_0-mlogloss:0.90434\n",
      "[325]\tvalidation_0-mlogloss:0.90396\n",
      "[326]\tvalidation_0-mlogloss:0.90319\n",
      "[327]\tvalidation_0-mlogloss:0.90291\n",
      "[328]\tvalidation_0-mlogloss:0.90244\n",
      "[329]\tvalidation_0-mlogloss:0.90187\n",
      "[330]\tvalidation_0-mlogloss:0.90102\n",
      "[331]\tvalidation_0-mlogloss:0.90069\n",
      "[332]\tvalidation_0-mlogloss:0.90044\n",
      "[333]\tvalidation_0-mlogloss:0.89999\n",
      "[334]\tvalidation_0-mlogloss:0.89953\n",
      "[335]\tvalidation_0-mlogloss:0.89881\n",
      "[336]\tvalidation_0-mlogloss:0.89866\n",
      "[337]\tvalidation_0-mlogloss:0.89804\n",
      "[338]\tvalidation_0-mlogloss:0.89743\n",
      "[339]\tvalidation_0-mlogloss:0.89725\n",
      "[340]\tvalidation_0-mlogloss:0.89672\n",
      "[341]\tvalidation_0-mlogloss:0.89613\n",
      "[342]\tvalidation_0-mlogloss:0.89535\n",
      "[343]\tvalidation_0-mlogloss:0.89481\n",
      "[344]\tvalidation_0-mlogloss:0.89440\n",
      "[345]\tvalidation_0-mlogloss:0.89381\n",
      "[346]\tvalidation_0-mlogloss:0.89343\n",
      "[347]\tvalidation_0-mlogloss:0.89276\n",
      "[348]\tvalidation_0-mlogloss:0.89237\n",
      "[349]\tvalidation_0-mlogloss:0.89235\n",
      "[350]\tvalidation_0-mlogloss:0.89162\n",
      "[351]\tvalidation_0-mlogloss:0.89110\n",
      "[352]\tvalidation_0-mlogloss:0.89067\n",
      "[353]\tvalidation_0-mlogloss:0.89008\n",
      "[354]\tvalidation_0-mlogloss:0.88971\n",
      "[355]\tvalidation_0-mlogloss:0.88960\n",
      "[356]\tvalidation_0-mlogloss:0.88919\n",
      "[357]\tvalidation_0-mlogloss:0.88875\n",
      "[358]\tvalidation_0-mlogloss:0.88815\n",
      "[359]\tvalidation_0-mlogloss:0.88789\n",
      "[360]\tvalidation_0-mlogloss:0.88759\n",
      "[361]\tvalidation_0-mlogloss:0.88724\n",
      "[362]\tvalidation_0-mlogloss:0.88687\n",
      "[363]\tvalidation_0-mlogloss:0.88663\n",
      "[364]\tvalidation_0-mlogloss:0.88644\n",
      "[365]\tvalidation_0-mlogloss:0.88620\n",
      "[366]\tvalidation_0-mlogloss:0.88548\n",
      "[367]\tvalidation_0-mlogloss:0.88504\n",
      "[368]\tvalidation_0-mlogloss:0.88452\n",
      "[369]\tvalidation_0-mlogloss:0.88453\n",
      "[370]\tvalidation_0-mlogloss:0.88424\n",
      "[371]\tvalidation_0-mlogloss:0.88377\n",
      "[372]\tvalidation_0-mlogloss:0.88352\n",
      "[373]\tvalidation_0-mlogloss:0.88295\n",
      "[374]\tvalidation_0-mlogloss:0.88252\n",
      "[375]\tvalidation_0-mlogloss:0.88238\n",
      "[376]\tvalidation_0-mlogloss:0.88209\n",
      "[377]\tvalidation_0-mlogloss:0.88202\n",
      "[378]\tvalidation_0-mlogloss:0.88156\n",
      "[379]\tvalidation_0-mlogloss:0.88162\n",
      "[380]\tvalidation_0-mlogloss:0.88130\n",
      "[381]\tvalidation_0-mlogloss:0.88106\n",
      "[382]\tvalidation_0-mlogloss:0.88057\n",
      "[383]\tvalidation_0-mlogloss:0.88006\n",
      "[384]\tvalidation_0-mlogloss:0.87940\n",
      "[385]\tvalidation_0-mlogloss:0.87903\n",
      "[386]\tvalidation_0-mlogloss:0.87873\n",
      "[387]\tvalidation_0-mlogloss:0.87865\n",
      "[388]\tvalidation_0-mlogloss:0.87852\n",
      "[389]\tvalidation_0-mlogloss:0.87802\n",
      "[390]\tvalidation_0-mlogloss:0.87746\n",
      "[391]\tvalidation_0-mlogloss:0.87699\n",
      "[392]\tvalidation_0-mlogloss:0.87678\n",
      "[393]\tvalidation_0-mlogloss:0.87648\n",
      "[394]\tvalidation_0-mlogloss:0.87613\n",
      "[395]\tvalidation_0-mlogloss:0.87595\n",
      "[396]\tvalidation_0-mlogloss:0.87557\n",
      "[397]\tvalidation_0-mlogloss:0.87541\n",
      "[398]\tvalidation_0-mlogloss:0.87501\n",
      "[399]\tvalidation_0-mlogloss:0.87483\n",
      "[400]\tvalidation_0-mlogloss:0.87445\n",
      "[401]\tvalidation_0-mlogloss:0.87401\n",
      "[402]\tvalidation_0-mlogloss:0.87323\n",
      "[403]\tvalidation_0-mlogloss:0.87302\n",
      "[404]\tvalidation_0-mlogloss:0.87254\n",
      "[405]\tvalidation_0-mlogloss:0.87232\n",
      "[406]\tvalidation_0-mlogloss:0.87202\n",
      "[407]\tvalidation_0-mlogloss:0.87190\n",
      "[408]\tvalidation_0-mlogloss:0.87149\n",
      "[409]\tvalidation_0-mlogloss:0.87129\n",
      "[410]\tvalidation_0-mlogloss:0.87059\n",
      "[411]\tvalidation_0-mlogloss:0.87040\n",
      "[412]\tvalidation_0-mlogloss:0.87023\n",
      "[413]\tvalidation_0-mlogloss:0.86995\n",
      "[414]\tvalidation_0-mlogloss:0.86974\n",
      "[415]\tvalidation_0-mlogloss:0.86965\n",
      "[416]\tvalidation_0-mlogloss:0.86960\n",
      "[417]\tvalidation_0-mlogloss:0.86949\n",
      "[418]\tvalidation_0-mlogloss:0.86920\n",
      "[419]\tvalidation_0-mlogloss:0.86890\n",
      "[420]\tvalidation_0-mlogloss:0.86898\n",
      "[421]\tvalidation_0-mlogloss:0.86902\n",
      "[422]\tvalidation_0-mlogloss:0.86884\n",
      "[423]\tvalidation_0-mlogloss:0.86847\n",
      "[424]\tvalidation_0-mlogloss:0.86807\n",
      "[425]\tvalidation_0-mlogloss:0.86810\n",
      "[426]\tvalidation_0-mlogloss:0.86769\n",
      "[427]\tvalidation_0-mlogloss:0.86732\n",
      "[428]\tvalidation_0-mlogloss:0.86722\n",
      "[429]\tvalidation_0-mlogloss:0.86709\n",
      "[430]\tvalidation_0-mlogloss:0.86711\n",
      "[431]\tvalidation_0-mlogloss:0.86686\n",
      "[432]\tvalidation_0-mlogloss:0.86654\n",
      "[433]\tvalidation_0-mlogloss:0.86628\n",
      "[434]\tvalidation_0-mlogloss:0.86613\n",
      "[435]\tvalidation_0-mlogloss:0.86572\n",
      "[436]\tvalidation_0-mlogloss:0.86562\n",
      "[437]\tvalidation_0-mlogloss:0.86537\n",
      "[438]\tvalidation_0-mlogloss:0.86514\n",
      "[439]\tvalidation_0-mlogloss:0.86485\n",
      "[440]\tvalidation_0-mlogloss:0.86428\n",
      "[441]\tvalidation_0-mlogloss:0.86424\n",
      "[442]\tvalidation_0-mlogloss:0.86387\n",
      "[443]\tvalidation_0-mlogloss:0.86358\n",
      "[444]\tvalidation_0-mlogloss:0.86324\n",
      "[445]\tvalidation_0-mlogloss:0.86330\n",
      "[446]\tvalidation_0-mlogloss:0.86339\n",
      "[447]\tvalidation_0-mlogloss:0.86302\n",
      "[448]\tvalidation_0-mlogloss:0.86262\n",
      "[449]\tvalidation_0-mlogloss:0.86212\n",
      "[450]\tvalidation_0-mlogloss:0.86203\n",
      "[451]\tvalidation_0-mlogloss:0.86171\n",
      "[452]\tvalidation_0-mlogloss:0.86121\n",
      "[453]\tvalidation_0-mlogloss:0.86076\n",
      "[454]\tvalidation_0-mlogloss:0.86039\n",
      "[455]\tvalidation_0-mlogloss:0.86044\n",
      "[456]\tvalidation_0-mlogloss:0.86019\n",
      "[457]\tvalidation_0-mlogloss:0.86017\n",
      "[458]\tvalidation_0-mlogloss:0.85996\n",
      "[459]\tvalidation_0-mlogloss:0.86003\n",
      "[460]\tvalidation_0-mlogloss:0.85960\n",
      "[461]\tvalidation_0-mlogloss:0.85951\n",
      "[462]\tvalidation_0-mlogloss:0.85960\n",
      "[463]\tvalidation_0-mlogloss:0.85926\n",
      "[464]\tvalidation_0-mlogloss:0.85904\n",
      "[465]\tvalidation_0-mlogloss:0.85862\n",
      "[466]\tvalidation_0-mlogloss:0.85819\n",
      "[467]\tvalidation_0-mlogloss:0.85808\n",
      "[468]\tvalidation_0-mlogloss:0.85786\n",
      "[469]\tvalidation_0-mlogloss:0.85755\n",
      "[470]\tvalidation_0-mlogloss:0.85733\n",
      "[471]\tvalidation_0-mlogloss:0.85725\n",
      "[472]\tvalidation_0-mlogloss:0.85723\n",
      "[473]\tvalidation_0-mlogloss:0.85738\n",
      "[474]\tvalidation_0-mlogloss:0.85719\n",
      "[475]\tvalidation_0-mlogloss:0.85701\n",
      "[476]\tvalidation_0-mlogloss:0.85684\n",
      "[477]\tvalidation_0-mlogloss:0.85673\n",
      "[478]\tvalidation_0-mlogloss:0.85667\n",
      "[479]\tvalidation_0-mlogloss:0.85662\n",
      "[480]\tvalidation_0-mlogloss:0.85666\n",
      "[481]\tvalidation_0-mlogloss:0.85647\n",
      "[482]\tvalidation_0-mlogloss:0.85628\n",
      "[483]\tvalidation_0-mlogloss:0.85621\n",
      "[484]\tvalidation_0-mlogloss:0.85625\n",
      "[485]\tvalidation_0-mlogloss:0.85626\n",
      "[486]\tvalidation_0-mlogloss:0.85603\n",
      "[487]\tvalidation_0-mlogloss:0.85593\n",
      "[488]\tvalidation_0-mlogloss:0.85545\n",
      "[489]\tvalidation_0-mlogloss:0.85519\n",
      "[490]\tvalidation_0-mlogloss:0.85536\n",
      "[491]\tvalidation_0-mlogloss:0.85492\n",
      "[492]\tvalidation_0-mlogloss:0.85489\n",
      "[493]\tvalidation_0-mlogloss:0.85477\n",
      "[494]\tvalidation_0-mlogloss:0.85460\n",
      "[495]\tvalidation_0-mlogloss:0.85470\n",
      "[496]\tvalidation_0-mlogloss:0.85452\n",
      "[497]\tvalidation_0-mlogloss:0.85443\n",
      "[498]\tvalidation_0-mlogloss:0.85405\n",
      "[499]\tvalidation_0-mlogloss:0.85391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82      1499\n",
      "           1       0.86      0.96      0.91      1499\n",
      "           2       0.62      0.67      0.64      1499\n",
      "           3       0.97      0.83      0.90      1499\n",
      "           4       0.96      0.99      0.98      1499\n",
      "           5       0.99      0.99      0.99      1499\n",
      "           6       0.96      0.98      0.97      1499\n",
      "           7       0.97      0.91      0.94      1499\n",
      "           8       0.50      0.89      0.64      1499\n",
      "           9       0.87      0.73      0.79      1499\n",
      "          10       0.97      0.92      0.95      1499\n",
      "          11       0.98      0.99      0.98      1499\n",
      "          12       0.82      0.93      0.87      1499\n",
      "          13       0.91      0.94      0.92      1499\n",
      "          14       0.76      0.91      0.83      1499\n",
      "          15       0.98      0.67      0.80      1499\n",
      "          16       0.46      0.21      0.29      1499\n",
      "          17       0.81      0.88      0.84      1499\n",
      "          18       0.87      0.86      0.87      1499\n",
      "          19       0.84      0.71      0.77      1499\n",
      "          20       0.98      1.00      0.99      1499\n",
      "          21       0.81      0.90      0.85      1499\n",
      "          22       0.75      0.76      0.76      1499\n",
      "          23       0.97      0.99      0.98      1499\n",
      "          24       0.97      0.96      0.97      1499\n",
      "          25       0.92      0.68      0.78      1499\n",
      "          26       0.58      0.38      0.46      1499\n",
      "          27       0.72      0.74      0.73      1499\n",
      "          28       0.69      0.60      0.64      1499\n",
      "          29       0.70      0.98      0.82      1499\n",
      "          30       0.90      0.94      0.92      1499\n",
      "          31       0.69      0.95      0.80      1499\n",
      "          32       0.89      0.97      0.93      1499\n",
      "          33       0.97      0.46      0.62      1499\n",
      "          34       0.96      0.99      0.97      1499\n",
      "          35       0.80      0.92      0.86      1499\n",
      "          36       0.83      0.85      0.84      1499\n",
      "          37       0.81      0.85      0.83      1499\n",
      "          38       0.45      0.09      0.15      1499\n",
      "          39       0.83      0.82      0.82      1499\n",
      "          40       0.93      0.86      0.89      1499\n",
      "          41       0.91      0.74      0.82      1499\n",
      "          42       0.79      0.84      0.81      1499\n",
      "          43       0.91      0.86      0.89      1499\n",
      "          44       0.94      0.98      0.96      1499\n",
      "          45       0.70      0.69      0.70      1499\n",
      "          46       0.66      0.88      0.75      1499\n",
      "          47       0.69      0.51      0.58      1499\n",
      "          48       0.84      0.94      0.88      1499\n",
      "          49       0.78      0.91      0.84      1499\n",
      "          50       0.91      0.73      0.81      1499\n",
      "          51       0.41      0.77      0.53      1499\n",
      "          52       0.82      0.90      0.86      1499\n",
      "          53       0.87      0.85      0.86      1499\n",
      "          54       0.84      0.87      0.85      1499\n",
      "          55       0.76      0.92      0.84      1499\n",
      "          56       0.88      0.92      0.90      1499\n",
      "          57       0.90      0.96      0.93      1499\n",
      "          58       0.92      0.32      0.47      1499\n",
      "          59       0.49      0.38      0.43      1499\n",
      "          60       0.73      0.79      0.76      1499\n",
      "          61       0.76      0.14      0.24      1499\n",
      "          62       0.96      0.96      0.96      1499\n",
      "          63       0.70      0.74      0.72      1499\n",
      "          64       0.64      0.77      0.69      1499\n",
      "          65       0.89      0.98      0.93      1499\n",
      "          66       0.98      0.91      0.94      1499\n",
      "          67       0.51      0.27      0.35      1499\n",
      "          68       0.80      0.93      0.86      1499\n",
      "          69       0.70      0.84      0.76      1499\n",
      "          70       0.84      0.56      0.67      1499\n",
      "          71       0.78      0.50      0.61      1499\n",
      "          72       0.87      0.92      0.90      1499\n",
      "          73       0.86      0.89      0.87      1499\n",
      "          74       0.38      0.84      0.52      1499\n",
      "\n",
      "    accuracy                           0.79    112425\n",
      "   macro avg       0.81      0.79      0.78    112425\n",
      "weighted avg       0.81      0.79      0.78    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3361defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a896a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 53s 2ms/step - loss: 1.0463 - val_loss: 2.0202\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 49s 2ms/step - loss: 0.4225 - val_loss: 2.2758\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 40s 1ms/step - loss: 0.3050 - val_loss: 2.0507\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 40s 1ms/step - loss: 0.2393 - val_loss: 1.9247\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 40s 1ms/step - loss: 0.1998 - val_loss: 1.7235\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 40s 1ms/step - loss: 0.1712 - val_loss: 1.8004\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 40s 1ms/step - loss: 0.1519 - val_loss: 1.6581\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 40s 1ms/step - loss: 0.1373 - val_loss: 1.7251\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 40s 1ms/step - loss: 0.1249 - val_loss: 1.8172\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 40s 1ms/step - loss: 0.1163 - val_loss: 1.6203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f506864fe80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  60/3514 [..............................] - ETA: 2s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 3s 883us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82      1499\n",
      "           1       0.71      0.96      0.82      1499\n",
      "           2       0.62      0.68      0.65      1499\n",
      "           3       0.88      0.88      0.88      1499\n",
      "           4       0.91      0.99      0.95      1499\n",
      "           5       0.99      0.88      0.93      1499\n",
      "           6       0.97      0.98      0.98      1499\n",
      "           7       0.95      0.86      0.91      1499\n",
      "           8       0.43      0.89      0.58      1499\n",
      "           9       0.60      0.77      0.68      1499\n",
      "          10       0.85      0.91      0.88      1499\n",
      "          11       0.89      0.89      0.89      1499\n",
      "          12       0.77      0.86      0.81      1499\n",
      "          13       0.89      0.81      0.85      1499\n",
      "          14       0.68      0.87      0.76      1499\n",
      "          15       0.91      0.22      0.36      1499\n",
      "          16       0.69      0.28      0.40      1499\n",
      "          17       0.91      0.63      0.74      1499\n",
      "          18       0.83      0.80      0.81      1499\n",
      "          19       0.79      0.51      0.62      1499\n",
      "          20       0.98      0.99      0.98      1499\n",
      "          21       0.75      0.88      0.81      1499\n",
      "          22       0.83      0.68      0.75      1499\n",
      "          23       0.95      0.99      0.97      1499\n",
      "          24       0.97      0.80      0.88      1499\n",
      "          25       0.72      0.26      0.39      1499\n",
      "          26       0.90      0.40      0.55      1499\n",
      "          27       0.67      0.75      0.71      1499\n",
      "          28       0.70      0.42      0.52      1499\n",
      "          29       0.68      0.96      0.80      1499\n",
      "          30       0.87      0.92      0.89      1499\n",
      "          31       0.68      0.95      0.80      1499\n",
      "          32       0.81      0.94      0.87      1499\n",
      "          33       0.74      0.38      0.50      1499\n",
      "          34       0.97      1.00      0.98      1499\n",
      "          35       0.80      0.89      0.84      1499\n",
      "          36       0.46      0.73      0.57      1499\n",
      "          37       0.76      0.81      0.78      1499\n",
      "          38       0.61      0.18      0.28      1499\n",
      "          39       0.69      0.66      0.67      1499\n",
      "          40       0.60      0.81      0.69      1499\n",
      "          41       0.64      0.33      0.44      1499\n",
      "          42       0.50      0.78      0.61      1499\n",
      "          43       0.79      0.83      0.81      1499\n",
      "          44       0.91      0.90      0.91      1499\n",
      "          45       0.86      0.53      0.65      1499\n",
      "          46       0.36      0.65      0.46      1499\n",
      "          47       0.67      0.49      0.57      1499\n",
      "          48       0.70      0.87      0.78      1499\n",
      "          49       0.82      0.69      0.75      1499\n",
      "          50       0.84      0.36      0.51      1499\n",
      "          51       0.55      0.73      0.63      1499\n",
      "          52       0.85      0.90      0.87      1499\n",
      "          53       0.90      0.87      0.89      1499\n",
      "          54       0.79      0.62      0.70      1499\n",
      "          55       0.70      0.93      0.80      1499\n",
      "          56       0.80      0.64      0.71      1499\n",
      "          57       0.85      0.89      0.87      1499\n",
      "          58       0.54      0.32      0.40      1499\n",
      "          59       0.50      0.28      0.36      1499\n",
      "          60       0.66      0.72      0.69      1499\n",
      "          61       0.30      0.19      0.24      1499\n",
      "          62       0.96      0.97      0.96      1499\n",
      "          63       0.55      0.85      0.67      1499\n",
      "          64       0.62      0.70      0.66      1499\n",
      "          65       0.96      0.91      0.94      1499\n",
      "          66       0.84      0.95      0.89      1499\n",
      "          67       0.21      0.33      0.26      1499\n",
      "          68       0.80      0.94      0.86      1499\n",
      "          69       0.59      0.84      0.69      1499\n",
      "          70       0.94      0.44      0.60      1499\n",
      "          71       0.48      0.30      0.37      1499\n",
      "          72       0.66      0.79      0.72      1499\n",
      "          73       0.79      0.48      0.60      1499\n",
      "          74       0.45      0.86      0.59      1499\n",
      "\n",
      "    accuracy                           0.72    112425\n",
      "   macro avg       0.74      0.72      0.71    112425\n",
      "weighted avg       0.74      0.72      0.71    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
