{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 75 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 10:13:25.400596: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 10:13:27.698659: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 10:13:27.698783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 10:13:28.052234: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 10:13:28.789714: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 10:13:28.791039: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 10:13:31.962203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files3/S001R05.edf',\n",
       " 'files3/S002R05.edf',\n",
       " 'files3/S003R05.edf',\n",
       " 'files3/S004R05.edf',\n",
       " 'files3/S005R05.edf',\n",
       " 'files3/S006R05.edf',\n",
       " 'files3/S007R05.edf',\n",
       " 'files3/S008R05.edf',\n",
       " 'files3/S009R05.edf',\n",
       " 'files3/S010R05.edf',\n",
       " 'files3/S011R05.edf',\n",
       " 'files3/S012R05.edf',\n",
       " 'files3/S013R05.edf',\n",
       " 'files3/S014R05.edf',\n",
       " 'files3/S015R05.edf',\n",
       " 'files3/S016R05.edf',\n",
       " 'files3/S017R05.edf',\n",
       " 'files3/S018R05.edf',\n",
       " 'files3/S019R05.edf',\n",
       " 'files3/S020R05.edf',\n",
       " 'files3/S021R05.edf',\n",
       " 'files3/S022R05.edf',\n",
       " 'files3/S023R05.edf',\n",
       " 'files3/S024R05.edf',\n",
       " 'files3/S025R05.edf',\n",
       " 'files3/S026R05.edf',\n",
       " 'files3/S027R05.edf',\n",
       " 'files3/S028R05.edf',\n",
       " 'files3/S029R05.edf',\n",
       " 'files3/S030R05.edf',\n",
       " 'files3/S031R05.edf',\n",
       " 'files3/S032R05.edf',\n",
       " 'files3/S033R05.edf',\n",
       " 'files3/S034R05.edf',\n",
       " 'files3/S035R05.edf',\n",
       " 'files3/S036R05.edf',\n",
       " 'files3/S037R05.edf',\n",
       " 'files3/S038R05.edf',\n",
       " 'files3/S039R05.edf',\n",
       " 'files3/S040R05.edf',\n",
       " 'files3/S041R05.edf',\n",
       " 'files3/S042R05.edf',\n",
       " 'files3/S043R05.edf',\n",
       " 'files3/S044R05.edf',\n",
       " 'files3/S045R05.edf',\n",
       " 'files3/S046R05.edf',\n",
       " 'files3/S047R05.edf',\n",
       " 'files3/S048R05.edf',\n",
       " 'files3/S049R05.edf',\n",
       " 'files3/S050R05.edf',\n",
       " 'files3/S051R05.edf',\n",
       " 'files3/S052R05.edf',\n",
       " 'files3/S053R05.edf',\n",
       " 'files3/S054R05.edf',\n",
       " 'files3/S055R05.edf',\n",
       " 'files3/S056R05.edf',\n",
       " 'files3/S057R05.edf',\n",
       " 'files3/S058R05.edf',\n",
       " 'files3/S059R05.edf',\n",
       " 'files3/S060R05.edf',\n",
       " 'files3/S061R05.edf',\n",
       " 'files3/S062R05.edf',\n",
       " 'files3/S063R05.edf',\n",
       " 'files3/S064R05.edf',\n",
       " 'files3/S065R05.edf',\n",
       " 'files3/S066R05.edf',\n",
       " 'files3/S067R05.edf',\n",
       " 'files3/S068R05.edf',\n",
       " 'files3/S069R05.edf',\n",
       " 'files3/S070R05.edf',\n",
       " 'files3/S071R05.edf',\n",
       " 'files3/S072R05.edf',\n",
       " 'files3/S073R05.edf',\n",
       " 'files3/S074R05.edf',\n",
       " 'files3/S075R05.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files3/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ca7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 112425, 900000, 112425)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.01e-04  1.30e-05 -1.50e-05 ... -3.00e-05  1.00e-05  1.00e-05]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000042   0.000014   0.000028   0.000021   0.000018   0.000058   \n",
       "899996  -0.000008  -0.000014  -0.000018  -0.000019  -0.000005  -0.000012   \n",
       "899997   0.000003  -0.000015  -0.000017  -0.000018  -0.000001  -0.000014   \n",
       "899998   0.000063   0.000026   0.000021   0.000018   0.000032  -0.000003   \n",
       "899999   0.000101   0.000062   0.000067   0.000044   0.000063  -0.000023   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995  -0.000033   0.000031   0.000055   0.000040  ...   -0.000015   \n",
       "899996  -0.000056   0.000001   0.000026   0.000005  ...   -0.000020   \n",
       "899997  -0.000050   0.000010   0.000018   0.000002  ...   -0.000001   \n",
       "899998  -0.000011   0.000049   0.000055   0.000046  ...    0.000014   \n",
       "899999   0.000006   0.000061   0.000093   0.000080  ...    0.000011   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995   -0.000003    0.000044   -0.000002   -0.000017   -0.000032   \n",
       "899996   -0.000024    0.000017   -0.000032   -0.000049   -0.000034   \n",
       "899997   -0.000029    0.000021   -0.000014   -0.000042   -0.000046   \n",
       "899998    0.000046    0.000100    0.000050    0.000015   -0.000002   \n",
       "899999    0.000063    0.000135    0.000061    0.000030    0.000006   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "899995   -0.000032    0.000053   -0.000053    0.000021  \n",
       "899996   -0.000055    0.000032   -0.000056    0.000002  \n",
       "899997   -0.000050    0.000022   -0.000082   -0.000003  \n",
       "899998    0.000037    0.000114   -0.000044    0.000064  \n",
       "899999    0.000048    0.000124   -0.000014    0.000085  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21844/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_21844/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_21844/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000042   0.000014   0.000028   0.000021   0.000018   0.000058   \n",
       "899996  -0.000008  -0.000014  -0.000018  -0.000019  -0.000005  -0.000012   \n",
       "899997   0.000003  -0.000015  -0.000017  -0.000018  -0.000001  -0.000014   \n",
       "899998   0.000063   0.000026   0.000021   0.000018   0.000032  -0.000003   \n",
       "899999   0.000101   0.000062   0.000067   0.000044   0.000063  -0.000023   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995  -0.000033   0.000031   0.000055   0.000040  ...   -0.000015   \n",
       "899996  -0.000056   0.000001   0.000026   0.000005  ...   -0.000020   \n",
       "899997  -0.000050   0.000010   0.000018   0.000002  ...   -0.000001   \n",
       "899998  -0.000011   0.000049   0.000055   0.000046  ...    0.000014   \n",
       "899999   0.000006   0.000061   0.000093   0.000080  ...    0.000011   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995   -0.000003    0.000044   -0.000002   -0.000017   -0.000032   \n",
       "899996   -0.000024    0.000017   -0.000032   -0.000049   -0.000034   \n",
       "899997   -0.000029    0.000021   -0.000014   -0.000042   -0.000046   \n",
       "899998    0.000046    0.000100    0.000050    0.000015   -0.000002   \n",
       "899999    0.000063    0.000135    0.000061    0.000030    0.000006   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "899995   -0.000032    0.000053   -0.000053    0.000021  \n",
       "899996   -0.000055    0.000032   -0.000056    0.000002  \n",
       "899997   -0.000050    0.000022   -0.000082   -0.000003  \n",
       "899998    0.000037    0.000114   -0.000044    0.000064  \n",
       "899999    0.000048    0.000124   -0.000014    0.000085  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:4.03149\n",
      "[1]\tvalidation_0-mlogloss:3.91394\n",
      "[2]\tvalidation_0-mlogloss:3.83347\n",
      "[3]\tvalidation_0-mlogloss:3.77260\n",
      "[4]\tvalidation_0-mlogloss:3.72437\n",
      "[5]\tvalidation_0-mlogloss:3.68302\n",
      "[6]\tvalidation_0-mlogloss:3.64872\n",
      "[7]\tvalidation_0-mlogloss:3.61775\n",
      "[8]\tvalidation_0-mlogloss:3.59197\n",
      "[9]\tvalidation_0-mlogloss:3.57038\n",
      "[10]\tvalidation_0-mlogloss:3.54968\n",
      "[11]\tvalidation_0-mlogloss:3.53214\n",
      "[12]\tvalidation_0-mlogloss:3.51541\n",
      "[13]\tvalidation_0-mlogloss:3.49913\n",
      "[14]\tvalidation_0-mlogloss:3.48363\n",
      "[15]\tvalidation_0-mlogloss:3.46972\n",
      "[16]\tvalidation_0-mlogloss:3.45697\n",
      "[17]\tvalidation_0-mlogloss:3.44542\n",
      "[18]\tvalidation_0-mlogloss:3.43528\n",
      "[19]\tvalidation_0-mlogloss:3.42168\n",
      "[20]\tvalidation_0-mlogloss:3.41323\n",
      "[21]\tvalidation_0-mlogloss:3.40196\n",
      "[22]\tvalidation_0-mlogloss:3.39286\n",
      "[23]\tvalidation_0-mlogloss:3.38379\n",
      "[24]\tvalidation_0-mlogloss:3.37387\n",
      "[25]\tvalidation_0-mlogloss:3.36519\n",
      "[26]\tvalidation_0-mlogloss:3.35823\n",
      "[27]\tvalidation_0-mlogloss:3.34968\n",
      "[28]\tvalidation_0-mlogloss:3.34363\n",
      "[29]\tvalidation_0-mlogloss:3.33430\n",
      "[30]\tvalidation_0-mlogloss:3.32724\n",
      "[31]\tvalidation_0-mlogloss:3.32027\n",
      "[32]\tvalidation_0-mlogloss:3.31174\n",
      "[33]\tvalidation_0-mlogloss:3.30533\n",
      "[34]\tvalidation_0-mlogloss:3.29765\n",
      "[35]\tvalidation_0-mlogloss:3.29096\n",
      "[36]\tvalidation_0-mlogloss:3.28325\n",
      "[37]\tvalidation_0-mlogloss:3.27810\n",
      "[38]\tvalidation_0-mlogloss:3.27051\n",
      "[39]\tvalidation_0-mlogloss:3.26564\n",
      "[40]\tvalidation_0-mlogloss:3.25875\n",
      "[41]\tvalidation_0-mlogloss:3.25161\n",
      "[42]\tvalidation_0-mlogloss:3.24282\n",
      "[43]\tvalidation_0-mlogloss:3.23804\n",
      "[44]\tvalidation_0-mlogloss:3.23117\n",
      "[45]\tvalidation_0-mlogloss:3.22458\n",
      "[46]\tvalidation_0-mlogloss:3.21748\n",
      "[47]\tvalidation_0-mlogloss:3.21190\n",
      "[48]\tvalidation_0-mlogloss:3.20613\n",
      "[49]\tvalidation_0-mlogloss:3.19988\n",
      "[50]\tvalidation_0-mlogloss:3.19440\n",
      "[51]\tvalidation_0-mlogloss:3.18841\n",
      "[52]\tvalidation_0-mlogloss:3.18415\n",
      "[53]\tvalidation_0-mlogloss:3.17876\n",
      "[54]\tvalidation_0-mlogloss:3.17329\n",
      "[55]\tvalidation_0-mlogloss:3.16875\n",
      "[56]\tvalidation_0-mlogloss:3.16417\n",
      "[57]\tvalidation_0-mlogloss:3.16048\n",
      "[58]\tvalidation_0-mlogloss:3.15617\n",
      "[59]\tvalidation_0-mlogloss:3.15263\n",
      "[60]\tvalidation_0-mlogloss:3.14897\n",
      "[61]\tvalidation_0-mlogloss:3.14503\n",
      "[62]\tvalidation_0-mlogloss:3.14104\n",
      "[63]\tvalidation_0-mlogloss:3.13686\n",
      "[64]\tvalidation_0-mlogloss:3.13383\n",
      "[65]\tvalidation_0-mlogloss:3.13026\n",
      "[66]\tvalidation_0-mlogloss:3.12548\n",
      "[67]\tvalidation_0-mlogloss:3.12174\n",
      "[68]\tvalidation_0-mlogloss:3.11777\n",
      "[69]\tvalidation_0-mlogloss:3.11382\n",
      "[70]\tvalidation_0-mlogloss:3.11003\n",
      "[71]\tvalidation_0-mlogloss:3.10694\n",
      "[72]\tvalidation_0-mlogloss:3.10390\n",
      "[73]\tvalidation_0-mlogloss:3.10162\n",
      "[74]\tvalidation_0-mlogloss:3.09947\n",
      "[75]\tvalidation_0-mlogloss:3.09654\n",
      "[76]\tvalidation_0-mlogloss:3.09328\n",
      "[77]\tvalidation_0-mlogloss:3.08913\n",
      "[78]\tvalidation_0-mlogloss:3.08581\n",
      "[79]\tvalidation_0-mlogloss:3.08219\n",
      "[80]\tvalidation_0-mlogloss:3.07944\n",
      "[81]\tvalidation_0-mlogloss:3.07772\n",
      "[82]\tvalidation_0-mlogloss:3.07443\n",
      "[83]\tvalidation_0-mlogloss:3.07164\n",
      "[84]\tvalidation_0-mlogloss:3.06890\n",
      "[85]\tvalidation_0-mlogloss:3.06709\n",
      "[86]\tvalidation_0-mlogloss:3.06400\n",
      "[87]\tvalidation_0-mlogloss:3.06113\n",
      "[88]\tvalidation_0-mlogloss:3.05788\n",
      "[89]\tvalidation_0-mlogloss:3.05499\n",
      "[90]\tvalidation_0-mlogloss:3.05305\n",
      "[91]\tvalidation_0-mlogloss:3.05063\n",
      "[92]\tvalidation_0-mlogloss:3.04898\n",
      "[93]\tvalidation_0-mlogloss:3.04622\n",
      "[94]\tvalidation_0-mlogloss:3.04473\n",
      "[95]\tvalidation_0-mlogloss:3.04227\n",
      "[96]\tvalidation_0-mlogloss:3.03902\n",
      "[97]\tvalidation_0-mlogloss:3.03654\n",
      "[98]\tvalidation_0-mlogloss:3.03432\n",
      "[99]\tvalidation_0-mlogloss:3.03169\n",
      "[100]\tvalidation_0-mlogloss:3.02857\n",
      "[101]\tvalidation_0-mlogloss:3.02641\n",
      "[102]\tvalidation_0-mlogloss:3.02469\n",
      "[103]\tvalidation_0-mlogloss:3.02273\n",
      "[104]\tvalidation_0-mlogloss:3.02034\n",
      "[105]\tvalidation_0-mlogloss:3.01811\n",
      "[106]\tvalidation_0-mlogloss:3.01613\n",
      "[107]\tvalidation_0-mlogloss:3.01430\n",
      "[108]\tvalidation_0-mlogloss:3.01247\n",
      "[109]\tvalidation_0-mlogloss:3.01044\n",
      "[110]\tvalidation_0-mlogloss:3.00817\n",
      "[111]\tvalidation_0-mlogloss:3.00473\n",
      "[112]\tvalidation_0-mlogloss:3.00298\n",
      "[113]\tvalidation_0-mlogloss:3.00054\n",
      "[114]\tvalidation_0-mlogloss:2.99827\n",
      "[115]\tvalidation_0-mlogloss:2.99656\n",
      "[116]\tvalidation_0-mlogloss:2.99387\n",
      "[117]\tvalidation_0-mlogloss:2.99213\n",
      "[118]\tvalidation_0-mlogloss:2.99044\n",
      "[119]\tvalidation_0-mlogloss:2.98904\n",
      "[120]\tvalidation_0-mlogloss:2.98672\n",
      "[121]\tvalidation_0-mlogloss:2.98508\n",
      "[122]\tvalidation_0-mlogloss:2.98340\n",
      "[123]\tvalidation_0-mlogloss:2.98213\n",
      "[124]\tvalidation_0-mlogloss:2.97981\n",
      "[125]\tvalidation_0-mlogloss:2.97861\n",
      "[126]\tvalidation_0-mlogloss:2.97660\n",
      "[127]\tvalidation_0-mlogloss:2.97521\n",
      "[128]\tvalidation_0-mlogloss:2.97360\n",
      "[129]\tvalidation_0-mlogloss:2.97158\n",
      "[130]\tvalidation_0-mlogloss:2.97007\n",
      "[131]\tvalidation_0-mlogloss:2.96898\n",
      "[132]\tvalidation_0-mlogloss:2.96695\n",
      "[133]\tvalidation_0-mlogloss:2.96589\n",
      "[134]\tvalidation_0-mlogloss:2.96460\n",
      "[135]\tvalidation_0-mlogloss:2.96334\n",
      "[136]\tvalidation_0-mlogloss:2.96182\n",
      "[137]\tvalidation_0-mlogloss:2.96021\n",
      "[138]\tvalidation_0-mlogloss:2.95906\n",
      "[139]\tvalidation_0-mlogloss:2.95782\n",
      "[140]\tvalidation_0-mlogloss:2.95652\n",
      "[141]\tvalidation_0-mlogloss:2.95454\n",
      "[142]\tvalidation_0-mlogloss:2.95329\n",
      "[143]\tvalidation_0-mlogloss:2.95166\n",
      "[144]\tvalidation_0-mlogloss:2.95061\n",
      "[145]\tvalidation_0-mlogloss:2.94962\n",
      "[146]\tvalidation_0-mlogloss:2.94852\n",
      "[147]\tvalidation_0-mlogloss:2.94697\n",
      "[148]\tvalidation_0-mlogloss:2.94555\n",
      "[149]\tvalidation_0-mlogloss:2.94434\n",
      "[150]\tvalidation_0-mlogloss:2.94325\n",
      "[151]\tvalidation_0-mlogloss:2.94255\n",
      "[152]\tvalidation_0-mlogloss:2.94112\n",
      "[153]\tvalidation_0-mlogloss:2.93985\n",
      "[154]\tvalidation_0-mlogloss:2.93833\n",
      "[155]\tvalidation_0-mlogloss:2.93690\n",
      "[156]\tvalidation_0-mlogloss:2.93603\n",
      "[157]\tvalidation_0-mlogloss:2.93504\n",
      "[158]\tvalidation_0-mlogloss:2.93443\n",
      "[159]\tvalidation_0-mlogloss:2.93331\n",
      "[160]\tvalidation_0-mlogloss:2.93238\n",
      "[161]\tvalidation_0-mlogloss:2.93134\n",
      "[162]\tvalidation_0-mlogloss:2.93028\n",
      "[163]\tvalidation_0-mlogloss:2.92844\n",
      "[164]\tvalidation_0-mlogloss:2.92706\n",
      "[165]\tvalidation_0-mlogloss:2.92610\n",
      "[166]\tvalidation_0-mlogloss:2.92482\n",
      "[167]\tvalidation_0-mlogloss:2.92290\n",
      "[168]\tvalidation_0-mlogloss:2.92215\n",
      "[169]\tvalidation_0-mlogloss:2.92037\n",
      "[170]\tvalidation_0-mlogloss:2.91859\n",
      "[171]\tvalidation_0-mlogloss:2.91797\n",
      "[172]\tvalidation_0-mlogloss:2.91640\n",
      "[173]\tvalidation_0-mlogloss:2.91507\n",
      "[174]\tvalidation_0-mlogloss:2.91350\n",
      "[175]\tvalidation_0-mlogloss:2.91224\n",
      "[176]\tvalidation_0-mlogloss:2.91118\n",
      "[177]\tvalidation_0-mlogloss:2.90995\n",
      "[178]\tvalidation_0-mlogloss:2.90893\n",
      "[179]\tvalidation_0-mlogloss:2.90799\n",
      "[180]\tvalidation_0-mlogloss:2.90695\n",
      "[181]\tvalidation_0-mlogloss:2.90592\n",
      "[182]\tvalidation_0-mlogloss:2.90477\n",
      "[183]\tvalidation_0-mlogloss:2.90398\n",
      "[184]\tvalidation_0-mlogloss:2.90340\n",
      "[185]\tvalidation_0-mlogloss:2.90265\n",
      "[186]\tvalidation_0-mlogloss:2.90151\n",
      "[187]\tvalidation_0-mlogloss:2.90050\n",
      "[188]\tvalidation_0-mlogloss:2.90002\n",
      "[189]\tvalidation_0-mlogloss:2.89930\n",
      "[190]\tvalidation_0-mlogloss:2.89770\n",
      "[191]\tvalidation_0-mlogloss:2.89628\n",
      "[192]\tvalidation_0-mlogloss:2.89533\n",
      "[193]\tvalidation_0-mlogloss:2.89405\n",
      "[194]\tvalidation_0-mlogloss:2.89303\n",
      "[195]\tvalidation_0-mlogloss:2.89237\n",
      "[196]\tvalidation_0-mlogloss:2.89155\n",
      "[197]\tvalidation_0-mlogloss:2.89078\n",
      "[198]\tvalidation_0-mlogloss:2.89019\n",
      "[199]\tvalidation_0-mlogloss:2.88993\n",
      "[200]\tvalidation_0-mlogloss:2.88885\n",
      "[201]\tvalidation_0-mlogloss:2.88833\n",
      "[202]\tvalidation_0-mlogloss:2.88809\n",
      "[203]\tvalidation_0-mlogloss:2.88723\n",
      "[204]\tvalidation_0-mlogloss:2.88740\n",
      "[205]\tvalidation_0-mlogloss:2.88725\n",
      "[206]\tvalidation_0-mlogloss:2.88641\n",
      "[207]\tvalidation_0-mlogloss:2.88533\n",
      "[208]\tvalidation_0-mlogloss:2.88408\n",
      "[209]\tvalidation_0-mlogloss:2.88318\n",
      "[210]\tvalidation_0-mlogloss:2.88274\n",
      "[211]\tvalidation_0-mlogloss:2.88243\n",
      "[212]\tvalidation_0-mlogloss:2.88169\n",
      "[213]\tvalidation_0-mlogloss:2.88062\n",
      "[214]\tvalidation_0-mlogloss:2.87972\n",
      "[215]\tvalidation_0-mlogloss:2.87954\n",
      "[216]\tvalidation_0-mlogloss:2.87896\n",
      "[217]\tvalidation_0-mlogloss:2.87807\n",
      "[218]\tvalidation_0-mlogloss:2.87789\n",
      "[219]\tvalidation_0-mlogloss:2.87739\n",
      "[220]\tvalidation_0-mlogloss:2.87738\n",
      "[221]\tvalidation_0-mlogloss:2.87688\n",
      "[222]\tvalidation_0-mlogloss:2.87631\n",
      "[223]\tvalidation_0-mlogloss:2.87552\n",
      "[224]\tvalidation_0-mlogloss:2.87532\n",
      "[225]\tvalidation_0-mlogloss:2.87471\n",
      "[226]\tvalidation_0-mlogloss:2.87402\n",
      "[227]\tvalidation_0-mlogloss:2.87348\n",
      "[228]\tvalidation_0-mlogloss:2.87328\n",
      "[229]\tvalidation_0-mlogloss:2.87256\n",
      "[230]\tvalidation_0-mlogloss:2.87178\n",
      "[231]\tvalidation_0-mlogloss:2.87131\n",
      "[232]\tvalidation_0-mlogloss:2.87086\n",
      "[233]\tvalidation_0-mlogloss:2.87027\n",
      "[234]\tvalidation_0-mlogloss:2.86955\n",
      "[235]\tvalidation_0-mlogloss:2.86903\n",
      "[236]\tvalidation_0-mlogloss:2.86914\n",
      "[237]\tvalidation_0-mlogloss:2.86848\n",
      "[238]\tvalidation_0-mlogloss:2.86829\n",
      "[239]\tvalidation_0-mlogloss:2.86778\n",
      "[240]\tvalidation_0-mlogloss:2.86771\n",
      "[241]\tvalidation_0-mlogloss:2.86729\n",
      "[242]\tvalidation_0-mlogloss:2.86682\n",
      "[243]\tvalidation_0-mlogloss:2.86611\n",
      "[244]\tvalidation_0-mlogloss:2.86525\n",
      "[245]\tvalidation_0-mlogloss:2.86485\n",
      "[246]\tvalidation_0-mlogloss:2.86451\n",
      "[247]\tvalidation_0-mlogloss:2.86410\n",
      "[248]\tvalidation_0-mlogloss:2.86386\n",
      "[249]\tvalidation_0-mlogloss:2.86337\n",
      "[250]\tvalidation_0-mlogloss:2.86276\n",
      "[251]\tvalidation_0-mlogloss:2.86221\n",
      "[252]\tvalidation_0-mlogloss:2.86178\n",
      "[253]\tvalidation_0-mlogloss:2.86136\n",
      "[254]\tvalidation_0-mlogloss:2.86115\n",
      "[255]\tvalidation_0-mlogloss:2.86150\n",
      "[256]\tvalidation_0-mlogloss:2.86112\n",
      "[257]\tvalidation_0-mlogloss:2.86084\n",
      "[258]\tvalidation_0-mlogloss:2.86069\n",
      "[259]\tvalidation_0-mlogloss:2.86063\n",
      "[260]\tvalidation_0-mlogloss:2.85974\n",
      "[261]\tvalidation_0-mlogloss:2.85973\n",
      "[262]\tvalidation_0-mlogloss:2.85905\n",
      "[263]\tvalidation_0-mlogloss:2.85874\n",
      "[264]\tvalidation_0-mlogloss:2.85846\n",
      "[265]\tvalidation_0-mlogloss:2.85807\n",
      "[266]\tvalidation_0-mlogloss:2.85804\n",
      "[267]\tvalidation_0-mlogloss:2.85722\n",
      "[268]\tvalidation_0-mlogloss:2.85681\n",
      "[269]\tvalidation_0-mlogloss:2.85618\n",
      "[270]\tvalidation_0-mlogloss:2.85599\n",
      "[271]\tvalidation_0-mlogloss:2.85568\n",
      "[272]\tvalidation_0-mlogloss:2.85527\n",
      "[273]\tvalidation_0-mlogloss:2.85486\n",
      "[274]\tvalidation_0-mlogloss:2.85463\n",
      "[275]\tvalidation_0-mlogloss:2.85364\n",
      "[276]\tvalidation_0-mlogloss:2.85416\n",
      "[277]\tvalidation_0-mlogloss:2.85461\n",
      "[278]\tvalidation_0-mlogloss:2.85439\n",
      "[279]\tvalidation_0-mlogloss:2.85461\n",
      "[280]\tvalidation_0-mlogloss:2.85462\n",
      "[281]\tvalidation_0-mlogloss:2.85461\n",
      "[282]\tvalidation_0-mlogloss:2.85463\n",
      "[283]\tvalidation_0-mlogloss:2.85465\n",
      "[284]\tvalidation_0-mlogloss:2.85433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.04      0.06      1499\n",
      "           1       0.11      0.08      0.09      1499\n",
      "           2       0.27      0.24      0.25      1499\n",
      "           3       0.38      0.35      0.37      1499\n",
      "           4       0.66      0.69      0.67      1499\n",
      "           5       0.34      0.50      0.41      1499\n",
      "           6       0.28      0.50      0.36      1499\n",
      "           7       0.36      0.29      0.32      1499\n",
      "           8       0.45      0.57      0.50      1499\n",
      "           9       0.45      0.34      0.39      1499\n",
      "          10       0.28      0.31      0.29      1499\n",
      "          11       0.31      0.26      0.28      1499\n",
      "          12       0.33      0.29      0.30      1499\n",
      "          13       0.19      0.15      0.17      1499\n",
      "          14       0.12      0.06      0.08      1499\n",
      "          15       0.53      0.70      0.60      1499\n",
      "          16       0.20      0.09      0.12      1499\n",
      "          17       0.33      0.41      0.37      1499\n",
      "          18       0.15      0.09      0.11      1499\n",
      "          19       0.09      0.09      0.09      1499\n",
      "          20       0.51      0.59      0.55      1499\n",
      "          21       0.50      0.49      0.49      1499\n",
      "          22       0.29      0.16      0.20      1499\n",
      "          23       0.62      0.64      0.63      1499\n",
      "          24       0.49      0.59      0.53      1499\n",
      "          25       0.23      0.21      0.22      1499\n",
      "          26       0.28      0.24      0.26      1499\n",
      "          27       0.26      0.26      0.26      1499\n",
      "          28       0.21      0.26      0.23      1499\n",
      "          29       0.25      0.52      0.34      1499\n",
      "          30       0.17      0.16      0.17      1499\n",
      "          31       0.29      0.22      0.25      1499\n",
      "          32       0.12      0.08      0.09      1499\n",
      "          33       0.13      0.11      0.12      1499\n",
      "          34       0.49      0.62      0.54      1499\n",
      "          35       0.43      0.66      0.52      1499\n",
      "          36       0.23      0.41      0.30      1499\n",
      "          37       0.14      0.09      0.11      1499\n",
      "          38       0.20      0.13      0.16      1499\n",
      "          39       0.37      0.35      0.36      1499\n",
      "          40       0.36      0.48      0.41      1499\n",
      "          41       0.28      0.71      0.40      1499\n",
      "          42       0.23      0.17      0.20      1499\n",
      "          43       0.24      0.21      0.22      1499\n",
      "          44       0.32      0.41      0.36      1499\n",
      "          45       0.15      0.10      0.12      1499\n",
      "          46       0.20      0.33      0.25      1499\n",
      "          47       0.45      0.54      0.49      1499\n",
      "          48       0.56      0.69      0.62      1499\n",
      "          49       0.15      0.08      0.11      1499\n",
      "          50       0.24      0.59      0.34      1499\n",
      "          51       0.18      0.12      0.14      1499\n",
      "          52       0.21      0.09      0.13      1499\n",
      "          53       0.13      0.09      0.10      1499\n",
      "          54       0.26      0.33      0.29      1499\n",
      "          55       0.31      0.35      0.33      1499\n",
      "          56       0.13      0.09      0.10      1499\n",
      "          57       0.35      0.38      0.37      1499\n",
      "          58       0.95      0.75      0.84      1499\n",
      "          59       0.37      0.26      0.31      1499\n",
      "          60       0.37      0.42      0.39      1499\n",
      "          61       0.11      0.08      0.10      1499\n",
      "          62       0.38      0.43      0.41      1499\n",
      "          63       0.05      0.04      0.04      1499\n",
      "          64       0.18      0.10      0.13      1499\n",
      "          65       0.15      0.14      0.14      1499\n",
      "          66       0.19      0.22      0.20      1499\n",
      "          67       0.54      0.25      0.34      1499\n",
      "          68       0.43      0.44      0.43      1499\n",
      "          69       0.17      0.12      0.14      1499\n",
      "          70       0.43      0.33      0.38      1499\n",
      "          71       0.09      0.06      0.07      1499\n",
      "          72       0.10      0.11      0.10      1499\n",
      "          73       0.21      0.22      0.21      1499\n",
      "          74       0.16      0.12      0.13      1499\n",
      "\n",
      "    accuracy                           0.30    112425\n",
      "   macro avg       0.29      0.30      0.29    112425\n",
      "weighted avg       0.29      0.30      0.29    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b181cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 19s 682us/step - loss: 2.7639 - val_loss: 2.8109\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 19s 670us/step - loss: 2.1651 - val_loss: 2.7504\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 19s 677us/step - loss: 2.0587 - val_loss: 2.8817\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 23s 822us/step - loss: 2.0073 - val_loss: 2.8386\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 23s 811us/step - loss: 1.9742 - val_loss: 2.8400\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 23s 828us/step - loss: 1.9503 - val_loss: 2.7953\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 22s 791us/step - loss: 1.9318 - val_loss: 2.8855\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 23s 806us/step - loss: 1.9164 - val_loss: 2.8617\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 23s 833us/step - loss: 1.9043 - val_loss: 2.8096\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 24s 836us/step - loss: 1.8954 - val_loss: 2.8148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ac000f40>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 1s 346us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.10      0.10      1499\n",
      "           1       0.11      0.05      0.07      1499\n",
      "           2       0.43      0.26      0.33      1499\n",
      "           3       0.56      0.31      0.40      1499\n",
      "           4       0.52      0.74      0.61      1499\n",
      "           5       0.41      0.63      0.49      1499\n",
      "           6       0.31      0.49      0.38      1499\n",
      "           7       0.50      0.47      0.48      1499\n",
      "           8       0.54      0.52      0.53      1499\n",
      "           9       0.48      0.17      0.25      1499\n",
      "          10       0.30      0.40      0.34      1499\n",
      "          11       0.33      0.43      0.37      1499\n",
      "          12       0.27      0.24      0.25      1499\n",
      "          13       0.18      0.12      0.15      1499\n",
      "          14       0.14      0.06      0.09      1499\n",
      "          15       0.61      0.65      0.63      1499\n",
      "          16       0.20      0.14      0.16      1499\n",
      "          17       0.42      0.50      0.46      1499\n",
      "          18       0.18      0.15      0.16      1499\n",
      "          19       0.11      0.12      0.12      1499\n",
      "          20       0.51      0.63      0.56      1499\n",
      "          21       0.47      0.51      0.49      1499\n",
      "          22       0.55      0.10      0.17      1499\n",
      "          23       0.58      0.65      0.61      1499\n",
      "          24       0.49      0.45      0.47      1499\n",
      "          25       0.29      0.19      0.23      1499\n",
      "          26       0.35      0.25      0.29      1499\n",
      "          27       0.23      0.32      0.26      1499\n",
      "          28       0.25      0.37      0.30      1499\n",
      "          29       0.35      0.80      0.49      1499\n",
      "          30       0.23      0.31      0.27      1499\n",
      "          31       0.30      0.20      0.24      1499\n",
      "          32       0.14      0.07      0.09      1499\n",
      "          33       0.16      0.06      0.09      1499\n",
      "          34       0.75      0.63      0.69      1499\n",
      "          35       0.52      0.54      0.53      1499\n",
      "          36       0.27      0.13      0.18      1499\n",
      "          37       0.15      0.15      0.15      1499\n",
      "          38       0.19      0.20      0.19      1499\n",
      "          39       0.26      0.42      0.32      1499\n",
      "          40       0.41      0.57      0.48      1499\n",
      "          41       0.39      0.47      0.42      1499\n",
      "          42       0.19      0.15      0.17      1499\n",
      "          43       0.24      0.34      0.28      1499\n",
      "          44       0.32      0.47      0.38      1499\n",
      "          45       0.17      0.21      0.19      1499\n",
      "          46       0.22      0.41      0.29      1499\n",
      "          47       0.52      0.47      0.50      1499\n",
      "          48       0.54      0.78      0.64      1499\n",
      "          49       0.15      0.18      0.16      1499\n",
      "          50       0.28      0.37      0.32      1499\n",
      "          51       0.18      0.05      0.08      1499\n",
      "          52       0.20      0.12      0.15      1499\n",
      "          53       0.16      0.10      0.12      1499\n",
      "          54       0.28      0.40      0.33      1499\n",
      "          55       0.35      0.38      0.37      1499\n",
      "          56       0.09      0.08      0.08      1499\n",
      "          57       0.49      0.50      0.49      1499\n",
      "          58       0.94      0.73      0.82      1499\n",
      "          59       0.38      0.30      0.33      1499\n",
      "          60       0.57      0.57      0.57      1499\n",
      "          61       0.16      0.14      0.15      1499\n",
      "          62       0.62      0.54      0.58      1499\n",
      "          63       0.07      0.06      0.07      1499\n",
      "          64       0.15      0.15      0.15      1499\n",
      "          65       0.15      0.15      0.15      1499\n",
      "          66       0.24      0.21      0.22      1499\n",
      "          67       0.52      0.33      0.40      1499\n",
      "          68       0.42      0.61      0.50      1499\n",
      "          69       0.16      0.19      0.17      1499\n",
      "          70       0.52      0.28      0.36      1499\n",
      "          71       0.13      0.14      0.13      1499\n",
      "          72       0.12      0.16      0.14      1499\n",
      "          73       0.17      0.15      0.16      1499\n",
      "          74       0.17      0.12      0.14      1499\n",
      "\n",
      "    accuracy                           0.33    112425\n",
      "   macro avg       0.33      0.33      0.31    112425\n",
      "weighted avg       0.33      0.33      0.31    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21844/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_21844/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_21844/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "899995   0.000042   0.000014   0.000028   0.000021   0.000018   0.000058   \n",
       "899996  -0.000008  -0.000014  -0.000018  -0.000019  -0.000005  -0.000012   \n",
       "899997   0.000003  -0.000015  -0.000017  -0.000018  -0.000001  -0.000014   \n",
       "899998   0.000063   0.000026   0.000021   0.000018   0.000032  -0.000003   \n",
       "899999   0.000101   0.000062   0.000067   0.000044   0.000063  -0.000023   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "899995  -0.000033   0.000031   0.000055   0.000040  ...   -0.000015   \n",
       "899996  -0.000056   0.000001   0.000026   0.000005  ...   -0.000020   \n",
       "899997  -0.000050   0.000010   0.000018   0.000002  ...   -0.000001   \n",
       "899998  -0.000011   0.000049   0.000055   0.000046  ...    0.000014   \n",
       "899999   0.000006   0.000061   0.000093   0.000080  ...    0.000011   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "899995   -0.000003    0.000044   -0.000002   -0.000017   -0.000032   \n",
       "899996   -0.000024    0.000017   -0.000032   -0.000049   -0.000034   \n",
       "899997   -0.000029    0.000021   -0.000014   -0.000042   -0.000046   \n",
       "899998    0.000046    0.000100    0.000050    0.000015   -0.000002   \n",
       "899999    0.000063    0.000135    0.000061    0.000030    0.000006   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "899995   -0.000032    0.000053   -0.000053    0.000021  \n",
       "899996   -0.000055    0.000032   -0.000056    0.000002  \n",
       "899997   -0.000050    0.000022   -0.000082   -0.000003  \n",
       "899998    0.000037    0.000114   -0.000044    0.000064  \n",
       "899999    0.000048    0.000124   -0.000014    0.000085  \n",
       "\n",
       "[900000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.97048\n",
      "[1]\tvalidation_0-mlogloss:3.81442\n",
      "[2]\tvalidation_0-mlogloss:3.71154\n",
      "[3]\tvalidation_0-mlogloss:3.63355\n",
      "[4]\tvalidation_0-mlogloss:3.56809\n",
      "[5]\tvalidation_0-mlogloss:3.51260\n",
      "[6]\tvalidation_0-mlogloss:3.46377\n",
      "[7]\tvalidation_0-mlogloss:3.41870\n",
      "[8]\tvalidation_0-mlogloss:3.38130\n",
      "[9]\tvalidation_0-mlogloss:3.34891\n",
      "[10]\tvalidation_0-mlogloss:3.31777\n",
      "[11]\tvalidation_0-mlogloss:3.29204\n",
      "[12]\tvalidation_0-mlogloss:3.26906\n",
      "[13]\tvalidation_0-mlogloss:3.24562\n",
      "[14]\tvalidation_0-mlogloss:3.22182\n",
      "[15]\tvalidation_0-mlogloss:3.20076\n",
      "[16]\tvalidation_0-mlogloss:3.18285\n",
      "[17]\tvalidation_0-mlogloss:3.16361\n",
      "[18]\tvalidation_0-mlogloss:3.14614\n",
      "[19]\tvalidation_0-mlogloss:3.12971\n",
      "[20]\tvalidation_0-mlogloss:3.11554\n",
      "[21]\tvalidation_0-mlogloss:3.09928\n",
      "[22]\tvalidation_0-mlogloss:3.08313\n",
      "[23]\tvalidation_0-mlogloss:3.07136\n",
      "[24]\tvalidation_0-mlogloss:3.05768\n",
      "[25]\tvalidation_0-mlogloss:3.04453\n",
      "[26]\tvalidation_0-mlogloss:3.03198\n",
      "[27]\tvalidation_0-mlogloss:3.01974\n",
      "[28]\tvalidation_0-mlogloss:3.00709\n",
      "[29]\tvalidation_0-mlogloss:2.99628\n",
      "[30]\tvalidation_0-mlogloss:2.98618\n",
      "[31]\tvalidation_0-mlogloss:2.97532\n",
      "[32]\tvalidation_0-mlogloss:2.96392\n",
      "[33]\tvalidation_0-mlogloss:2.95305\n",
      "[34]\tvalidation_0-mlogloss:2.94013\n",
      "[35]\tvalidation_0-mlogloss:2.92762\n",
      "[36]\tvalidation_0-mlogloss:2.91606\n",
      "[37]\tvalidation_0-mlogloss:2.90510\n",
      "[38]\tvalidation_0-mlogloss:2.89394\n",
      "[39]\tvalidation_0-mlogloss:2.88511\n",
      "[40]\tvalidation_0-mlogloss:2.87548\n",
      "[41]\tvalidation_0-mlogloss:2.86701\n",
      "[42]\tvalidation_0-mlogloss:2.85698\n",
      "[43]\tvalidation_0-mlogloss:2.84757\n",
      "[44]\tvalidation_0-mlogloss:2.84051\n",
      "[45]\tvalidation_0-mlogloss:2.83209\n",
      "[46]\tvalidation_0-mlogloss:2.82473\n",
      "[47]\tvalidation_0-mlogloss:2.81584\n",
      "[48]\tvalidation_0-mlogloss:2.80806\n",
      "[49]\tvalidation_0-mlogloss:2.79903\n",
      "[50]\tvalidation_0-mlogloss:2.79039\n",
      "[51]\tvalidation_0-mlogloss:2.78240\n",
      "[52]\tvalidation_0-mlogloss:2.77521\n",
      "[53]\tvalidation_0-mlogloss:2.76941\n",
      "[54]\tvalidation_0-mlogloss:2.76252\n",
      "[55]\tvalidation_0-mlogloss:2.75559\n",
      "[56]\tvalidation_0-mlogloss:2.74905\n",
      "[57]\tvalidation_0-mlogloss:2.74135\n",
      "[58]\tvalidation_0-mlogloss:2.73514\n",
      "[59]\tvalidation_0-mlogloss:2.72854\n",
      "[60]\tvalidation_0-mlogloss:2.72102\n",
      "[61]\tvalidation_0-mlogloss:2.71539\n",
      "[62]\tvalidation_0-mlogloss:2.70970\n",
      "[63]\tvalidation_0-mlogloss:2.70254\n",
      "[64]\tvalidation_0-mlogloss:2.69674\n",
      "[65]\tvalidation_0-mlogloss:2.69058\n",
      "[66]\tvalidation_0-mlogloss:2.68544\n",
      "[67]\tvalidation_0-mlogloss:2.68119\n",
      "[68]\tvalidation_0-mlogloss:2.67580\n",
      "[69]\tvalidation_0-mlogloss:2.67126\n",
      "[70]\tvalidation_0-mlogloss:2.66592\n",
      "[71]\tvalidation_0-mlogloss:2.66088\n",
      "[72]\tvalidation_0-mlogloss:2.65671\n",
      "[73]\tvalidation_0-mlogloss:2.65286\n",
      "[74]\tvalidation_0-mlogloss:2.64595\n",
      "[75]\tvalidation_0-mlogloss:2.64134\n",
      "[76]\tvalidation_0-mlogloss:2.63630\n",
      "[77]\tvalidation_0-mlogloss:2.63080\n",
      "[78]\tvalidation_0-mlogloss:2.62770\n",
      "[79]\tvalidation_0-mlogloss:2.62238\n",
      "[80]\tvalidation_0-mlogloss:2.61751\n",
      "[81]\tvalidation_0-mlogloss:2.61343\n",
      "[82]\tvalidation_0-mlogloss:2.60936\n",
      "[83]\tvalidation_0-mlogloss:2.60620\n",
      "[84]\tvalidation_0-mlogloss:2.60185\n",
      "[85]\tvalidation_0-mlogloss:2.59822\n",
      "[86]\tvalidation_0-mlogloss:2.59463\n",
      "[87]\tvalidation_0-mlogloss:2.59038\n",
      "[88]\tvalidation_0-mlogloss:2.58589\n",
      "[89]\tvalidation_0-mlogloss:2.58184\n",
      "[90]\tvalidation_0-mlogloss:2.57734\n",
      "[91]\tvalidation_0-mlogloss:2.57396\n",
      "[92]\tvalidation_0-mlogloss:2.57074\n",
      "[93]\tvalidation_0-mlogloss:2.56755\n",
      "[94]\tvalidation_0-mlogloss:2.56429\n",
      "[95]\tvalidation_0-mlogloss:2.56036\n",
      "[96]\tvalidation_0-mlogloss:2.55731\n",
      "[97]\tvalidation_0-mlogloss:2.55360\n",
      "[98]\tvalidation_0-mlogloss:2.54980\n",
      "[99]\tvalidation_0-mlogloss:2.54676\n",
      "[100]\tvalidation_0-mlogloss:2.54305\n",
      "[101]\tvalidation_0-mlogloss:2.53952\n",
      "[102]\tvalidation_0-mlogloss:2.53558\n",
      "[103]\tvalidation_0-mlogloss:2.53195\n",
      "[104]\tvalidation_0-mlogloss:2.52947\n",
      "[105]\tvalidation_0-mlogloss:2.52619\n",
      "[106]\tvalidation_0-mlogloss:2.52368\n",
      "[107]\tvalidation_0-mlogloss:2.52029\n",
      "[108]\tvalidation_0-mlogloss:2.51744\n",
      "[109]\tvalidation_0-mlogloss:2.51397\n",
      "[110]\tvalidation_0-mlogloss:2.51084\n",
      "[111]\tvalidation_0-mlogloss:2.50792\n",
      "[112]\tvalidation_0-mlogloss:2.50515\n",
      "[113]\tvalidation_0-mlogloss:2.50178\n",
      "[114]\tvalidation_0-mlogloss:2.49894\n",
      "[115]\tvalidation_0-mlogloss:2.49592\n",
      "[116]\tvalidation_0-mlogloss:2.49373\n",
      "[117]\tvalidation_0-mlogloss:2.49129\n",
      "[118]\tvalidation_0-mlogloss:2.48903\n",
      "[119]\tvalidation_0-mlogloss:2.48620\n",
      "[120]\tvalidation_0-mlogloss:2.48354\n",
      "[121]\tvalidation_0-mlogloss:2.48044\n",
      "[122]\tvalidation_0-mlogloss:2.47766\n",
      "[123]\tvalidation_0-mlogloss:2.47390\n",
      "[124]\tvalidation_0-mlogloss:2.47160\n",
      "[125]\tvalidation_0-mlogloss:2.46864\n",
      "[126]\tvalidation_0-mlogloss:2.46631\n",
      "[127]\tvalidation_0-mlogloss:2.46356\n",
      "[128]\tvalidation_0-mlogloss:2.46046\n",
      "[129]\tvalidation_0-mlogloss:2.45932\n",
      "[130]\tvalidation_0-mlogloss:2.45873\n",
      "[131]\tvalidation_0-mlogloss:2.45538\n",
      "[132]\tvalidation_0-mlogloss:2.45272\n",
      "[133]\tvalidation_0-mlogloss:2.45071\n",
      "[134]\tvalidation_0-mlogloss:2.44818\n",
      "[135]\tvalidation_0-mlogloss:2.44628\n",
      "[136]\tvalidation_0-mlogloss:2.44385\n",
      "[137]\tvalidation_0-mlogloss:2.44209\n",
      "[138]\tvalidation_0-mlogloss:2.43998\n",
      "[139]\tvalidation_0-mlogloss:2.43735\n",
      "[140]\tvalidation_0-mlogloss:2.43595\n",
      "[141]\tvalidation_0-mlogloss:2.43340\n",
      "[142]\tvalidation_0-mlogloss:2.43179\n",
      "[143]\tvalidation_0-mlogloss:2.43061\n",
      "[144]\tvalidation_0-mlogloss:2.42834\n",
      "[145]\tvalidation_0-mlogloss:2.42585\n",
      "[146]\tvalidation_0-mlogloss:2.42336\n",
      "[147]\tvalidation_0-mlogloss:2.42113\n",
      "[148]\tvalidation_0-mlogloss:2.41989\n",
      "[149]\tvalidation_0-mlogloss:2.41803\n",
      "[150]\tvalidation_0-mlogloss:2.41449\n",
      "[151]\tvalidation_0-mlogloss:2.41199\n",
      "[152]\tvalidation_0-mlogloss:2.40942\n",
      "[153]\tvalidation_0-mlogloss:2.40754\n",
      "[154]\tvalidation_0-mlogloss:2.40555\n",
      "[155]\tvalidation_0-mlogloss:2.40391\n",
      "[156]\tvalidation_0-mlogloss:2.40200\n",
      "[157]\tvalidation_0-mlogloss:2.39988\n",
      "[158]\tvalidation_0-mlogloss:2.39826\n",
      "[159]\tvalidation_0-mlogloss:2.39642\n",
      "[160]\tvalidation_0-mlogloss:2.39470\n",
      "[161]\tvalidation_0-mlogloss:2.39325\n",
      "[162]\tvalidation_0-mlogloss:2.39159\n",
      "[163]\tvalidation_0-mlogloss:2.38955\n",
      "[164]\tvalidation_0-mlogloss:2.38850\n",
      "[165]\tvalidation_0-mlogloss:2.38710\n",
      "[166]\tvalidation_0-mlogloss:2.38561\n",
      "[167]\tvalidation_0-mlogloss:2.38425\n",
      "[168]\tvalidation_0-mlogloss:2.38300\n",
      "[169]\tvalidation_0-mlogloss:2.38116\n",
      "[170]\tvalidation_0-mlogloss:2.37990\n",
      "[171]\tvalidation_0-mlogloss:2.37803\n",
      "[172]\tvalidation_0-mlogloss:2.37701\n",
      "[173]\tvalidation_0-mlogloss:2.37518\n",
      "[174]\tvalidation_0-mlogloss:2.37396\n",
      "[175]\tvalidation_0-mlogloss:2.37282\n",
      "[176]\tvalidation_0-mlogloss:2.37152\n",
      "[177]\tvalidation_0-mlogloss:2.37003\n",
      "[178]\tvalidation_0-mlogloss:2.36920\n",
      "[179]\tvalidation_0-mlogloss:2.36785\n",
      "[180]\tvalidation_0-mlogloss:2.36588\n",
      "[181]\tvalidation_0-mlogloss:2.36500\n",
      "[182]\tvalidation_0-mlogloss:2.36338\n",
      "[183]\tvalidation_0-mlogloss:2.36175\n",
      "[184]\tvalidation_0-mlogloss:2.36093\n",
      "[185]\tvalidation_0-mlogloss:2.35967\n",
      "[186]\tvalidation_0-mlogloss:2.35852\n",
      "[187]\tvalidation_0-mlogloss:2.35654\n",
      "[188]\tvalidation_0-mlogloss:2.35452\n",
      "[189]\tvalidation_0-mlogloss:2.35271\n",
      "[190]\tvalidation_0-mlogloss:2.35087\n",
      "[191]\tvalidation_0-mlogloss:2.34884\n",
      "[192]\tvalidation_0-mlogloss:2.34838\n",
      "[193]\tvalidation_0-mlogloss:2.34684\n",
      "[194]\tvalidation_0-mlogloss:2.34604\n",
      "[195]\tvalidation_0-mlogloss:2.34511\n",
      "[196]\tvalidation_0-mlogloss:2.34461\n",
      "[197]\tvalidation_0-mlogloss:2.34436\n",
      "[198]\tvalidation_0-mlogloss:2.34302\n",
      "[199]\tvalidation_0-mlogloss:2.34249\n",
      "[200]\tvalidation_0-mlogloss:2.34131\n",
      "[201]\tvalidation_0-mlogloss:2.33948\n",
      "[202]\tvalidation_0-mlogloss:2.33850\n",
      "[203]\tvalidation_0-mlogloss:2.33771\n",
      "[204]\tvalidation_0-mlogloss:2.33666\n",
      "[205]\tvalidation_0-mlogloss:2.33433\n",
      "[206]\tvalidation_0-mlogloss:2.33282\n",
      "[207]\tvalidation_0-mlogloss:2.33188\n",
      "[208]\tvalidation_0-mlogloss:2.33116\n",
      "[209]\tvalidation_0-mlogloss:2.32999\n",
      "[210]\tvalidation_0-mlogloss:2.32909\n",
      "[211]\tvalidation_0-mlogloss:2.32801\n",
      "[212]\tvalidation_0-mlogloss:2.32718\n",
      "[213]\tvalidation_0-mlogloss:2.32595\n",
      "[214]\tvalidation_0-mlogloss:2.32511\n",
      "[215]\tvalidation_0-mlogloss:2.32474\n",
      "[216]\tvalidation_0-mlogloss:2.32356\n",
      "[217]\tvalidation_0-mlogloss:2.32262\n",
      "[218]\tvalidation_0-mlogloss:2.32205\n",
      "[219]\tvalidation_0-mlogloss:2.32096\n",
      "[220]\tvalidation_0-mlogloss:2.31964\n",
      "[221]\tvalidation_0-mlogloss:2.31871\n",
      "[222]\tvalidation_0-mlogloss:2.31791\n",
      "[223]\tvalidation_0-mlogloss:2.31680\n",
      "[224]\tvalidation_0-mlogloss:2.31569\n",
      "[225]\tvalidation_0-mlogloss:2.31484\n",
      "[226]\tvalidation_0-mlogloss:2.31353\n",
      "[227]\tvalidation_0-mlogloss:2.31312\n",
      "[228]\tvalidation_0-mlogloss:2.31202\n",
      "[229]\tvalidation_0-mlogloss:2.31094\n",
      "[230]\tvalidation_0-mlogloss:2.31057\n",
      "[231]\tvalidation_0-mlogloss:2.30963\n",
      "[232]\tvalidation_0-mlogloss:2.30914\n",
      "[233]\tvalidation_0-mlogloss:2.30790\n",
      "[234]\tvalidation_0-mlogloss:2.30751\n",
      "[235]\tvalidation_0-mlogloss:2.30657\n",
      "[236]\tvalidation_0-mlogloss:2.30617\n",
      "[237]\tvalidation_0-mlogloss:2.30524\n",
      "[238]\tvalidation_0-mlogloss:2.30454\n",
      "[239]\tvalidation_0-mlogloss:2.30428\n",
      "[240]\tvalidation_0-mlogloss:2.30341\n",
      "[241]\tvalidation_0-mlogloss:2.30268\n",
      "[242]\tvalidation_0-mlogloss:2.30211\n",
      "[243]\tvalidation_0-mlogloss:2.30159\n",
      "[244]\tvalidation_0-mlogloss:2.30051\n",
      "[245]\tvalidation_0-mlogloss:2.29948\n",
      "[246]\tvalidation_0-mlogloss:2.29849\n",
      "[247]\tvalidation_0-mlogloss:2.29786\n",
      "[248]\tvalidation_0-mlogloss:2.29752\n",
      "[249]\tvalidation_0-mlogloss:2.29638\n",
      "[250]\tvalidation_0-mlogloss:2.29523\n",
      "[251]\tvalidation_0-mlogloss:2.29476\n",
      "[252]\tvalidation_0-mlogloss:2.29447\n",
      "[253]\tvalidation_0-mlogloss:2.29418\n",
      "[254]\tvalidation_0-mlogloss:2.29395\n",
      "[255]\tvalidation_0-mlogloss:2.29275\n",
      "[256]\tvalidation_0-mlogloss:2.29258\n",
      "[257]\tvalidation_0-mlogloss:2.29237\n",
      "[258]\tvalidation_0-mlogloss:2.29193\n",
      "[259]\tvalidation_0-mlogloss:2.29073\n",
      "[260]\tvalidation_0-mlogloss:2.28995\n",
      "[261]\tvalidation_0-mlogloss:2.28919\n",
      "[262]\tvalidation_0-mlogloss:2.28828\n",
      "[263]\tvalidation_0-mlogloss:2.28832\n",
      "[264]\tvalidation_0-mlogloss:2.28758\n",
      "[265]\tvalidation_0-mlogloss:2.28698\n",
      "[266]\tvalidation_0-mlogloss:2.28651\n",
      "[267]\tvalidation_0-mlogloss:2.28603\n",
      "[268]\tvalidation_0-mlogloss:2.28542\n",
      "[269]\tvalidation_0-mlogloss:2.28534\n",
      "[270]\tvalidation_0-mlogloss:2.28540\n",
      "[271]\tvalidation_0-mlogloss:2.28467\n",
      "[272]\tvalidation_0-mlogloss:2.28392\n",
      "[273]\tvalidation_0-mlogloss:2.28310\n",
      "[274]\tvalidation_0-mlogloss:2.28204\n",
      "[275]\tvalidation_0-mlogloss:2.28139\n",
      "[276]\tvalidation_0-mlogloss:2.28077\n",
      "[277]\tvalidation_0-mlogloss:2.28007\n",
      "[278]\tvalidation_0-mlogloss:2.27961\n",
      "[279]\tvalidation_0-mlogloss:2.27915\n",
      "[280]\tvalidation_0-mlogloss:2.27880\n",
      "[281]\tvalidation_0-mlogloss:2.27787\n",
      "[282]\tvalidation_0-mlogloss:2.27786\n",
      "[283]\tvalidation_0-mlogloss:2.27755\n",
      "[284]\tvalidation_0-mlogloss:2.27756\n",
      "[285]\tvalidation_0-mlogloss:2.27668\n",
      "[286]\tvalidation_0-mlogloss:2.27620\n",
      "[287]\tvalidation_0-mlogloss:2.27507\n",
      "[288]\tvalidation_0-mlogloss:2.27425\n",
      "[289]\tvalidation_0-mlogloss:2.27381\n",
      "[290]\tvalidation_0-mlogloss:2.27322\n",
      "[291]\tvalidation_0-mlogloss:2.27297\n",
      "[292]\tvalidation_0-mlogloss:2.27238\n",
      "[293]\tvalidation_0-mlogloss:2.27178\n",
      "[294]\tvalidation_0-mlogloss:2.27194\n",
      "[295]\tvalidation_0-mlogloss:2.27190\n",
      "[296]\tvalidation_0-mlogloss:2.27144\n",
      "[297]\tvalidation_0-mlogloss:2.27095\n",
      "[298]\tvalidation_0-mlogloss:2.27075\n",
      "[299]\tvalidation_0-mlogloss:2.26996\n",
      "[300]\tvalidation_0-mlogloss:2.26958\n",
      "[301]\tvalidation_0-mlogloss:2.26947\n",
      "[302]\tvalidation_0-mlogloss:2.26953\n",
      "[303]\tvalidation_0-mlogloss:2.26939\n",
      "[304]\tvalidation_0-mlogloss:2.26951\n",
      "[305]\tvalidation_0-mlogloss:2.26883\n",
      "[306]\tvalidation_0-mlogloss:2.26862\n",
      "[307]\tvalidation_0-mlogloss:2.26791\n",
      "[308]\tvalidation_0-mlogloss:2.26730\n",
      "[309]\tvalidation_0-mlogloss:2.26642\n",
      "[310]\tvalidation_0-mlogloss:2.26644\n",
      "[311]\tvalidation_0-mlogloss:2.26660\n",
      "[312]\tvalidation_0-mlogloss:2.26619\n",
      "[313]\tvalidation_0-mlogloss:2.26588\n",
      "[314]\tvalidation_0-mlogloss:2.26542\n",
      "[315]\tvalidation_0-mlogloss:2.26529\n",
      "[316]\tvalidation_0-mlogloss:2.26477\n",
      "[317]\tvalidation_0-mlogloss:2.26440\n",
      "[318]\tvalidation_0-mlogloss:2.26416\n",
      "[319]\tvalidation_0-mlogloss:2.26416\n",
      "[320]\tvalidation_0-mlogloss:2.26404\n",
      "[321]\tvalidation_0-mlogloss:2.26390\n",
      "[322]\tvalidation_0-mlogloss:2.26417\n",
      "[323]\tvalidation_0-mlogloss:2.26349\n",
      "[324]\tvalidation_0-mlogloss:2.26327\n",
      "[325]\tvalidation_0-mlogloss:2.26259\n",
      "[326]\tvalidation_0-mlogloss:2.26272\n",
      "[327]\tvalidation_0-mlogloss:2.26229\n",
      "[328]\tvalidation_0-mlogloss:2.26232\n",
      "[329]\tvalidation_0-mlogloss:2.26250\n",
      "[330]\tvalidation_0-mlogloss:2.26308\n",
      "[331]\tvalidation_0-mlogloss:2.26298\n",
      "[332]\tvalidation_0-mlogloss:2.26284\n",
      "[333]\tvalidation_0-mlogloss:2.26291\n",
      "[334]\tvalidation_0-mlogloss:2.26277\n",
      "[335]\tvalidation_0-mlogloss:2.26261\n",
      "[336]\tvalidation_0-mlogloss:2.26305\n",
      "[337]\tvalidation_0-mlogloss:2.26209\n",
      "[338]\tvalidation_0-mlogloss:2.26202\n",
      "[339]\tvalidation_0-mlogloss:2.26113\n",
      "[340]\tvalidation_0-mlogloss:2.26032\n",
      "[341]\tvalidation_0-mlogloss:2.26016\n",
      "[342]\tvalidation_0-mlogloss:2.25979\n",
      "[343]\tvalidation_0-mlogloss:2.25967\n",
      "[344]\tvalidation_0-mlogloss:2.25996\n",
      "[345]\tvalidation_0-mlogloss:2.25977\n",
      "[346]\tvalidation_0-mlogloss:2.25949\n",
      "[347]\tvalidation_0-mlogloss:2.25901\n",
      "[348]\tvalidation_0-mlogloss:2.25895\n",
      "[349]\tvalidation_0-mlogloss:2.25915\n",
      "[350]\tvalidation_0-mlogloss:2.25899\n",
      "[351]\tvalidation_0-mlogloss:2.25874\n",
      "[352]\tvalidation_0-mlogloss:2.25859\n",
      "[353]\tvalidation_0-mlogloss:2.25833\n",
      "[354]\tvalidation_0-mlogloss:2.25827\n",
      "[355]\tvalidation_0-mlogloss:2.25912\n",
      "[356]\tvalidation_0-mlogloss:2.25923\n",
      "[357]\tvalidation_0-mlogloss:2.25912\n",
      "[358]\tvalidation_0-mlogloss:2.25920\n",
      "[359]\tvalidation_0-mlogloss:2.25812\n",
      "[360]\tvalidation_0-mlogloss:2.25790\n",
      "[361]\tvalidation_0-mlogloss:2.25731\n",
      "[362]\tvalidation_0-mlogloss:2.25739\n",
      "[363]\tvalidation_0-mlogloss:2.25723\n",
      "[364]\tvalidation_0-mlogloss:2.25748\n",
      "[365]\tvalidation_0-mlogloss:2.25716\n",
      "[366]\tvalidation_0-mlogloss:2.25725\n",
      "[367]\tvalidation_0-mlogloss:2.25742\n",
      "[368]\tvalidation_0-mlogloss:2.25748\n",
      "[369]\tvalidation_0-mlogloss:2.25745\n",
      "[370]\tvalidation_0-mlogloss:2.25684\n",
      "[371]\tvalidation_0-mlogloss:2.25682\n",
      "[372]\tvalidation_0-mlogloss:2.25629\n",
      "[373]\tvalidation_0-mlogloss:2.25637\n",
      "[374]\tvalidation_0-mlogloss:2.25580\n",
      "[375]\tvalidation_0-mlogloss:2.25576\n",
      "[376]\tvalidation_0-mlogloss:2.25521\n",
      "[377]\tvalidation_0-mlogloss:2.25469\n",
      "[378]\tvalidation_0-mlogloss:2.25454\n",
      "[379]\tvalidation_0-mlogloss:2.25418\n",
      "[380]\tvalidation_0-mlogloss:2.25416\n",
      "[381]\tvalidation_0-mlogloss:2.25436\n",
      "[382]\tvalidation_0-mlogloss:2.25412\n",
      "[383]\tvalidation_0-mlogloss:2.25349\n",
      "[384]\tvalidation_0-mlogloss:2.25354\n",
      "[385]\tvalidation_0-mlogloss:2.25377\n",
      "[386]\tvalidation_0-mlogloss:2.25381\n",
      "[387]\tvalidation_0-mlogloss:2.25411\n",
      "[388]\tvalidation_0-mlogloss:2.25376\n",
      "[389]\tvalidation_0-mlogloss:2.25380\n",
      "[390]\tvalidation_0-mlogloss:2.25362\n",
      "[391]\tvalidation_0-mlogloss:2.25379\n",
      "[392]\tvalidation_0-mlogloss:2.25389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.16      0.18      1499\n",
      "           1       0.18      0.20      0.19      1499\n",
      "           2       0.33      0.30      0.31      1499\n",
      "           3       0.51      0.27      0.35      1499\n",
      "           4       0.82      0.88      0.84      1499\n",
      "           5       0.52      0.63      0.57      1499\n",
      "           6       0.55      0.84      0.66      1499\n",
      "           7       0.85      0.63      0.72      1499\n",
      "           8       0.56      0.70      0.62      1499\n",
      "           9       0.66      0.60      0.63      1499\n",
      "          10       0.58      0.48      0.53      1499\n",
      "          11       0.50      0.55      0.52      1499\n",
      "          12       0.51      0.66      0.57      1499\n",
      "          13       0.27      0.37      0.31      1499\n",
      "          14       0.20      0.20      0.20      1499\n",
      "          15       0.81      0.71      0.76      1499\n",
      "          16       0.59      0.43      0.49      1499\n",
      "          17       0.55      0.51      0.53      1499\n",
      "          18       0.33      0.49      0.39      1499\n",
      "          19       0.30      0.25      0.27      1499\n",
      "          20       0.65      0.81      0.72      1499\n",
      "          21       0.52      0.58      0.55      1499\n",
      "          22       0.35      0.28      0.31      1499\n",
      "          23       0.80      0.81      0.80      1499\n",
      "          24       0.86      0.74      0.80      1499\n",
      "          25       0.71      0.37      0.49      1499\n",
      "          26       0.27      0.20      0.23      1499\n",
      "          27       0.30      0.23      0.26      1499\n",
      "          28       0.40      0.44      0.42      1499\n",
      "          29       0.43      0.82      0.56      1499\n",
      "          30       0.26      0.31      0.28      1499\n",
      "          31       0.45      0.51      0.48      1499\n",
      "          32       0.27      0.57      0.37      1499\n",
      "          33       0.44      0.32      0.37      1499\n",
      "          34       0.68      0.73      0.71      1499\n",
      "          35       0.55      0.70      0.61      1499\n",
      "          36       0.47      0.46      0.46      1499\n",
      "          37       0.24      0.35      0.28      1499\n",
      "          38       0.59      0.54      0.56      1499\n",
      "          39       0.55      0.44      0.49      1499\n",
      "          40       0.77      0.63      0.69      1499\n",
      "          41       0.73      0.68      0.70      1499\n",
      "          42       0.39      0.37      0.38      1499\n",
      "          43       0.35      0.27      0.30      1499\n",
      "          44       0.52      0.58      0.55      1499\n",
      "          45       0.29      0.28      0.28      1499\n",
      "          46       0.30      0.42      0.35      1499\n",
      "          47       0.78      0.63      0.70      1499\n",
      "          48       0.65      0.74      0.69      1499\n",
      "          49       0.41      0.38      0.39      1499\n",
      "          50       0.41      0.54      0.47      1499\n",
      "          51       0.32      0.20      0.25      1499\n",
      "          52       0.27      0.28      0.28      1499\n",
      "          53       0.24      0.20      0.22      1499\n",
      "          54       0.44      0.43      0.44      1499\n",
      "          55       0.38      0.36      0.37      1499\n",
      "          56       0.20      0.15      0.17      1499\n",
      "          57       0.41      0.41      0.41      1499\n",
      "          58       0.97      0.87      0.92      1499\n",
      "          59       0.59      0.37      0.45      1499\n",
      "          60       0.48      0.47      0.48      1499\n",
      "          61       0.20      0.12      0.15      1499\n",
      "          62       0.56      0.70      0.62      1499\n",
      "          63       0.12      0.06      0.08      1499\n",
      "          64       0.30      0.22      0.25      1499\n",
      "          65       0.26      0.45      0.33      1499\n",
      "          66       0.31      0.34      0.32      1499\n",
      "          67       0.74      0.15      0.24      1499\n",
      "          68       0.54      0.60      0.57      1499\n",
      "          69       0.26      0.21      0.23      1499\n",
      "          70       0.82      0.32      0.46      1499\n",
      "          71       0.29      0.22      0.25      1499\n",
      "          72       0.37      0.42      0.39      1499\n",
      "          73       0.38      0.46      0.42      1499\n",
      "          74       0.24      0.32      0.27      1499\n",
      "\n",
      "    accuracy                           0.45    112425\n",
      "   macro avg       0.46      0.45      0.45    112425\n",
      "weighted avg       0.46      0.45      0.45    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7980b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46f7c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 25s 871us/step - loss: 2.0717 - val_loss: 2.4269\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 24s 843us/step - loss: 1.3700 - val_loss: 2.4978\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 24s 846us/step - loss: 1.2189 - val_loss: 2.4654\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 24s 843us/step - loss: 1.1444 - val_loss: 2.5653\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 24s 842us/step - loss: 1.0991 - val_loss: 2.4636\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 24s 851us/step - loss: 1.0644 - val_loss: 2.5869\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 24s 853us/step - loss: 1.0391 - val_loss: 2.5670\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 24s 848us/step - loss: 1.0161 - val_loss: 2.6999\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 24s 855us/step - loss: 0.9977 - val_loss: 2.5645\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 26s 913us/step - loss: 0.9790 - val_loss: 2.6497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x31419b4f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 1s 352us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.26      0.28      1499\n",
      "           1       0.24      0.22      0.23      1499\n",
      "           2       0.31      0.33      0.32      1499\n",
      "           3       0.57      0.40      0.47      1499\n",
      "           4       0.83      0.85      0.84      1499\n",
      "           5       0.54      0.70      0.61      1499\n",
      "           6       0.73      0.86      0.79      1499\n",
      "           7       0.93      0.55      0.69      1499\n",
      "           8       0.53      0.36      0.43      1499\n",
      "           9       0.49      0.53      0.51      1499\n",
      "          10       0.62      0.61      0.61      1499\n",
      "          11       0.45      0.79      0.57      1499\n",
      "          12       0.41      0.58      0.48      1499\n",
      "          13       0.31      0.29      0.30      1499\n",
      "          14       0.24      0.13      0.17      1499\n",
      "          15       0.85      0.73      0.78      1499\n",
      "          16       0.40      0.26      0.31      1499\n",
      "          17       0.47      0.67      0.55      1499\n",
      "          18       0.33      0.55      0.41      1499\n",
      "          19       0.24      0.08      0.12      1499\n",
      "          20       0.74      0.86      0.79      1499\n",
      "          21       0.49      0.60      0.54      1499\n",
      "          22       0.53      0.14      0.22      1499\n",
      "          23       0.71      0.90      0.79      1499\n",
      "          24       0.75      0.41      0.53      1499\n",
      "          25       0.70      0.70      0.70      1499\n",
      "          26       0.23      0.26      0.24      1499\n",
      "          27       0.13      0.17      0.15      1499\n",
      "          28       0.62      0.51      0.56      1499\n",
      "          29       0.57      0.76      0.65      1499\n",
      "          30       0.35      0.45      0.39      1499\n",
      "          31       0.46      0.47      0.47      1499\n",
      "          32       0.30      0.54      0.38      1499\n",
      "          33       0.45      0.30      0.36      1499\n",
      "          34       0.86      0.90      0.88      1499\n",
      "          35       0.57      0.69      0.63      1499\n",
      "          36       0.69      0.17      0.27      1499\n",
      "          37       0.23      0.42      0.30      1499\n",
      "          38       0.47      0.42      0.45      1499\n",
      "          39       0.37      0.36      0.37      1499\n",
      "          40       0.76      0.64      0.70      1499\n",
      "          41       0.72      0.70      0.71      1499\n",
      "          42       0.32      0.31      0.31      1499\n",
      "          43       0.42      0.34      0.37      1499\n",
      "          44       0.49      0.69      0.57      1499\n",
      "          45       0.24      0.32      0.27      1499\n",
      "          46       0.38      0.24      0.30      1499\n",
      "          47       0.57      0.38      0.46      1499\n",
      "          48       0.66      0.81      0.73      1499\n",
      "          49       0.29      0.65      0.40      1499\n",
      "          50       0.46      0.62      0.52      1499\n",
      "          51       0.36      0.12      0.18      1499\n",
      "          52       0.26      0.28      0.27      1499\n",
      "          53       0.19      0.19      0.19      1499\n",
      "          54       0.43      0.50      0.46      1499\n",
      "          55       0.42      0.42      0.42      1499\n",
      "          56       0.13      0.26      0.17      1499\n",
      "          57       0.52      0.45      0.48      1499\n",
      "          58       0.97      0.78      0.87      1499\n",
      "          59       0.80      0.25      0.39      1499\n",
      "          60       0.46      0.42      0.44      1499\n",
      "          61       0.31      0.20      0.25      1499\n",
      "          62       0.78      0.79      0.79      1499\n",
      "          63       0.17      0.10      0.13      1499\n",
      "          64       0.39      0.36      0.37      1499\n",
      "          65       0.37      0.50      0.43      1499\n",
      "          66       0.36      0.24      0.29      1499\n",
      "          67       0.27      0.05      0.09      1499\n",
      "          68       0.58      0.59      0.59      1499\n",
      "          69       0.29      0.28      0.28      1499\n",
      "          70       0.65      0.18      0.28      1499\n",
      "          71       0.43      0.26      0.33      1499\n",
      "          72       0.30      0.46      0.36      1499\n",
      "          73       0.35      0.37      0.36      1499\n",
      "          74       0.20      0.24      0.22      1499\n",
      "\n",
      "    accuracy                           0.45    112425\n",
      "   macro avg       0.47      0.45      0.44    112425\n",
      "weighted avg       0.47      0.45      0.44    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21844/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_21844/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_21844/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.78198\n",
      "[1]\tvalidation_0-mlogloss:3.56577\n",
      "[2]\tvalidation_0-mlogloss:3.41848\n",
      "[3]\tvalidation_0-mlogloss:3.30050\n",
      "[4]\tvalidation_0-mlogloss:3.21090\n",
      "[5]\tvalidation_0-mlogloss:3.12435\n",
      "[6]\tvalidation_0-mlogloss:3.05434\n",
      "[7]\tvalidation_0-mlogloss:2.99031\n",
      "[8]\tvalidation_0-mlogloss:2.93154\n",
      "[9]\tvalidation_0-mlogloss:2.88247\n",
      "[10]\tvalidation_0-mlogloss:2.83831\n",
      "[11]\tvalidation_0-mlogloss:2.79809\n",
      "[12]\tvalidation_0-mlogloss:2.75996\n",
      "[13]\tvalidation_0-mlogloss:2.72445\n",
      "[14]\tvalidation_0-mlogloss:2.69246\n",
      "[15]\tvalidation_0-mlogloss:2.66259\n",
      "[16]\tvalidation_0-mlogloss:2.63346\n",
      "[17]\tvalidation_0-mlogloss:2.60690\n",
      "[18]\tvalidation_0-mlogloss:2.57859\n",
      "[19]\tvalidation_0-mlogloss:2.55227\n",
      "[20]\tvalidation_0-mlogloss:2.53067\n",
      "[21]\tvalidation_0-mlogloss:2.50680\n",
      "[22]\tvalidation_0-mlogloss:2.48669\n",
      "[23]\tvalidation_0-mlogloss:2.46623\n",
      "[24]\tvalidation_0-mlogloss:2.44582\n",
      "[25]\tvalidation_0-mlogloss:2.42645\n",
      "[26]\tvalidation_0-mlogloss:2.40596\n",
      "[27]\tvalidation_0-mlogloss:2.38764\n",
      "[28]\tvalidation_0-mlogloss:2.37051\n",
      "[29]\tvalidation_0-mlogloss:2.35506\n",
      "[30]\tvalidation_0-mlogloss:2.34065\n",
      "[31]\tvalidation_0-mlogloss:2.32419\n",
      "[32]\tvalidation_0-mlogloss:2.30939\n",
      "[33]\tvalidation_0-mlogloss:2.29318\n",
      "[34]\tvalidation_0-mlogloss:2.27988\n",
      "[35]\tvalidation_0-mlogloss:2.26676\n",
      "[36]\tvalidation_0-mlogloss:2.25372\n",
      "[37]\tvalidation_0-mlogloss:2.23995\n",
      "[38]\tvalidation_0-mlogloss:2.22508\n",
      "[39]\tvalidation_0-mlogloss:2.21175\n",
      "[40]\tvalidation_0-mlogloss:2.19707\n",
      "[41]\tvalidation_0-mlogloss:2.18233\n",
      "[42]\tvalidation_0-mlogloss:2.16977\n",
      "[43]\tvalidation_0-mlogloss:2.15942\n",
      "[44]\tvalidation_0-mlogloss:2.14834\n",
      "[45]\tvalidation_0-mlogloss:2.13600\n",
      "[46]\tvalidation_0-mlogloss:2.12504\n",
      "[47]\tvalidation_0-mlogloss:2.11464\n",
      "[48]\tvalidation_0-mlogloss:2.10381\n",
      "[49]\tvalidation_0-mlogloss:2.09316\n",
      "[50]\tvalidation_0-mlogloss:2.08193\n",
      "[51]\tvalidation_0-mlogloss:2.07015\n",
      "[52]\tvalidation_0-mlogloss:2.05838\n",
      "[53]\tvalidation_0-mlogloss:2.04836\n",
      "[54]\tvalidation_0-mlogloss:2.03904\n",
      "[55]\tvalidation_0-mlogloss:2.02964\n",
      "[56]\tvalidation_0-mlogloss:2.02153\n",
      "[57]\tvalidation_0-mlogloss:2.01131\n",
      "[58]\tvalidation_0-mlogloss:2.00160\n",
      "[59]\tvalidation_0-mlogloss:1.99183\n",
      "[60]\tvalidation_0-mlogloss:1.98378\n",
      "[61]\tvalidation_0-mlogloss:1.97562\n",
      "[62]\tvalidation_0-mlogloss:1.96664\n",
      "[63]\tvalidation_0-mlogloss:1.95924\n",
      "[64]\tvalidation_0-mlogloss:1.95202\n",
      "[65]\tvalidation_0-mlogloss:1.94417\n",
      "[66]\tvalidation_0-mlogloss:1.93401\n",
      "[67]\tvalidation_0-mlogloss:1.92489\n",
      "[68]\tvalidation_0-mlogloss:1.91856\n",
      "[69]\tvalidation_0-mlogloss:1.91240\n",
      "[70]\tvalidation_0-mlogloss:1.90443\n",
      "[71]\tvalidation_0-mlogloss:1.89598\n",
      "[72]\tvalidation_0-mlogloss:1.88917\n",
      "[73]\tvalidation_0-mlogloss:1.88144\n",
      "[74]\tvalidation_0-mlogloss:1.87572\n",
      "[75]\tvalidation_0-mlogloss:1.86934\n",
      "[76]\tvalidation_0-mlogloss:1.86397\n",
      "[77]\tvalidation_0-mlogloss:1.85734\n",
      "[78]\tvalidation_0-mlogloss:1.84957\n",
      "[79]\tvalidation_0-mlogloss:1.84271\n",
      "[80]\tvalidation_0-mlogloss:1.83632\n",
      "[81]\tvalidation_0-mlogloss:1.83028\n",
      "[82]\tvalidation_0-mlogloss:1.82355\n",
      "[83]\tvalidation_0-mlogloss:1.81689\n",
      "[84]\tvalidation_0-mlogloss:1.81131\n",
      "[85]\tvalidation_0-mlogloss:1.80484\n",
      "[86]\tvalidation_0-mlogloss:1.79963\n",
      "[87]\tvalidation_0-mlogloss:1.79360\n",
      "[88]\tvalidation_0-mlogloss:1.78768\n",
      "[89]\tvalidation_0-mlogloss:1.78182\n",
      "[90]\tvalidation_0-mlogloss:1.77649\n",
      "[91]\tvalidation_0-mlogloss:1.77163\n",
      "[92]\tvalidation_0-mlogloss:1.76671\n",
      "[93]\tvalidation_0-mlogloss:1.76176\n",
      "[94]\tvalidation_0-mlogloss:1.75641\n",
      "[95]\tvalidation_0-mlogloss:1.75118\n",
      "[96]\tvalidation_0-mlogloss:1.74706\n",
      "[97]\tvalidation_0-mlogloss:1.74135\n",
      "[98]\tvalidation_0-mlogloss:1.73765\n",
      "[99]\tvalidation_0-mlogloss:1.73288\n",
      "[100]\tvalidation_0-mlogloss:1.72786\n",
      "[101]\tvalidation_0-mlogloss:1.72366\n",
      "[102]\tvalidation_0-mlogloss:1.71893\n",
      "[103]\tvalidation_0-mlogloss:1.71365\n",
      "[104]\tvalidation_0-mlogloss:1.70913\n",
      "[105]\tvalidation_0-mlogloss:1.70412\n",
      "[106]\tvalidation_0-mlogloss:1.69882\n",
      "[107]\tvalidation_0-mlogloss:1.69397\n",
      "[108]\tvalidation_0-mlogloss:1.68900\n",
      "[109]\tvalidation_0-mlogloss:1.68510\n",
      "[110]\tvalidation_0-mlogloss:1.68089\n",
      "[111]\tvalidation_0-mlogloss:1.67663\n",
      "[112]\tvalidation_0-mlogloss:1.67215\n",
      "[113]\tvalidation_0-mlogloss:1.66765\n",
      "[114]\tvalidation_0-mlogloss:1.66400\n",
      "[115]\tvalidation_0-mlogloss:1.66027\n",
      "[116]\tvalidation_0-mlogloss:1.65632\n",
      "[117]\tvalidation_0-mlogloss:1.65371\n",
      "[118]\tvalidation_0-mlogloss:1.64961\n",
      "[119]\tvalidation_0-mlogloss:1.64525\n",
      "[120]\tvalidation_0-mlogloss:1.64140\n",
      "[121]\tvalidation_0-mlogloss:1.63753\n",
      "[122]\tvalidation_0-mlogloss:1.63458\n",
      "[123]\tvalidation_0-mlogloss:1.63094\n",
      "[124]\tvalidation_0-mlogloss:1.62600\n",
      "[125]\tvalidation_0-mlogloss:1.62210\n",
      "[126]\tvalidation_0-mlogloss:1.61864\n",
      "[127]\tvalidation_0-mlogloss:1.61625\n",
      "[128]\tvalidation_0-mlogloss:1.61362\n",
      "[129]\tvalidation_0-mlogloss:1.61128\n",
      "[130]\tvalidation_0-mlogloss:1.60752\n",
      "[131]\tvalidation_0-mlogloss:1.60338\n",
      "[132]\tvalidation_0-mlogloss:1.59918\n",
      "[133]\tvalidation_0-mlogloss:1.59712\n",
      "[134]\tvalidation_0-mlogloss:1.59420\n",
      "[135]\tvalidation_0-mlogloss:1.59078\n",
      "[136]\tvalidation_0-mlogloss:1.58817\n",
      "[137]\tvalidation_0-mlogloss:1.58580\n",
      "[138]\tvalidation_0-mlogloss:1.58331\n",
      "[139]\tvalidation_0-mlogloss:1.57966\n",
      "[140]\tvalidation_0-mlogloss:1.57622\n",
      "[141]\tvalidation_0-mlogloss:1.57273\n",
      "[142]\tvalidation_0-mlogloss:1.56952\n",
      "[143]\tvalidation_0-mlogloss:1.56743\n",
      "[144]\tvalidation_0-mlogloss:1.56452\n",
      "[145]\tvalidation_0-mlogloss:1.56183\n",
      "[146]\tvalidation_0-mlogloss:1.55904\n",
      "[147]\tvalidation_0-mlogloss:1.55557\n",
      "[148]\tvalidation_0-mlogloss:1.55323\n",
      "[149]\tvalidation_0-mlogloss:1.55116\n",
      "[150]\tvalidation_0-mlogloss:1.54895\n",
      "[151]\tvalidation_0-mlogloss:1.54645\n",
      "[152]\tvalidation_0-mlogloss:1.54352\n",
      "[153]\tvalidation_0-mlogloss:1.54086\n",
      "[154]\tvalidation_0-mlogloss:1.53798\n",
      "[155]\tvalidation_0-mlogloss:1.53497\n",
      "[156]\tvalidation_0-mlogloss:1.53265\n",
      "[157]\tvalidation_0-mlogloss:1.53039\n",
      "[158]\tvalidation_0-mlogloss:1.52824\n",
      "[159]\tvalidation_0-mlogloss:1.52596\n",
      "[160]\tvalidation_0-mlogloss:1.52283\n",
      "[161]\tvalidation_0-mlogloss:1.52015\n",
      "[162]\tvalidation_0-mlogloss:1.51760\n",
      "[163]\tvalidation_0-mlogloss:1.51503\n",
      "[164]\tvalidation_0-mlogloss:1.51245\n",
      "[165]\tvalidation_0-mlogloss:1.50996\n",
      "[166]\tvalidation_0-mlogloss:1.50742\n",
      "[167]\tvalidation_0-mlogloss:1.50608\n",
      "[168]\tvalidation_0-mlogloss:1.50376\n",
      "[169]\tvalidation_0-mlogloss:1.50088\n",
      "[170]\tvalidation_0-mlogloss:1.49958\n",
      "[171]\tvalidation_0-mlogloss:1.49751\n",
      "[172]\tvalidation_0-mlogloss:1.49504\n",
      "[173]\tvalidation_0-mlogloss:1.49362\n",
      "[174]\tvalidation_0-mlogloss:1.49144\n",
      "[175]\tvalidation_0-mlogloss:1.48908\n",
      "[176]\tvalidation_0-mlogloss:1.48625\n",
      "[177]\tvalidation_0-mlogloss:1.48343\n",
      "[178]\tvalidation_0-mlogloss:1.48171\n",
      "[179]\tvalidation_0-mlogloss:1.48015\n",
      "[180]\tvalidation_0-mlogloss:1.47792\n",
      "[181]\tvalidation_0-mlogloss:1.47613\n",
      "[182]\tvalidation_0-mlogloss:1.47462\n",
      "[183]\tvalidation_0-mlogloss:1.47230\n",
      "[184]\tvalidation_0-mlogloss:1.46991\n",
      "[185]\tvalidation_0-mlogloss:1.46763\n",
      "[186]\tvalidation_0-mlogloss:1.46584\n",
      "[187]\tvalidation_0-mlogloss:1.46424\n",
      "[188]\tvalidation_0-mlogloss:1.46350\n",
      "[189]\tvalidation_0-mlogloss:1.46147\n",
      "[190]\tvalidation_0-mlogloss:1.45848\n",
      "[191]\tvalidation_0-mlogloss:1.45630\n",
      "[192]\tvalidation_0-mlogloss:1.45406\n",
      "[193]\tvalidation_0-mlogloss:1.45220\n",
      "[194]\tvalidation_0-mlogloss:1.45093\n",
      "[195]\tvalidation_0-mlogloss:1.44937\n",
      "[196]\tvalidation_0-mlogloss:1.44781\n",
      "[197]\tvalidation_0-mlogloss:1.44561\n",
      "[198]\tvalidation_0-mlogloss:1.44362\n",
      "[199]\tvalidation_0-mlogloss:1.44164\n",
      "[200]\tvalidation_0-mlogloss:1.43966\n",
      "[201]\tvalidation_0-mlogloss:1.43724\n",
      "[202]\tvalidation_0-mlogloss:1.43588\n",
      "[203]\tvalidation_0-mlogloss:1.43459\n",
      "[204]\tvalidation_0-mlogloss:1.43353\n",
      "[205]\tvalidation_0-mlogloss:1.43249\n",
      "[206]\tvalidation_0-mlogloss:1.43126\n",
      "[207]\tvalidation_0-mlogloss:1.42903\n",
      "[208]\tvalidation_0-mlogloss:1.42792\n",
      "[209]\tvalidation_0-mlogloss:1.42648\n",
      "[210]\tvalidation_0-mlogloss:1.42496\n",
      "[211]\tvalidation_0-mlogloss:1.42279\n",
      "[212]\tvalidation_0-mlogloss:1.42170\n",
      "[213]\tvalidation_0-mlogloss:1.42093\n",
      "[214]\tvalidation_0-mlogloss:1.41987\n",
      "[215]\tvalidation_0-mlogloss:1.41814\n",
      "[216]\tvalidation_0-mlogloss:1.41643\n",
      "[217]\tvalidation_0-mlogloss:1.41452\n",
      "[218]\tvalidation_0-mlogloss:1.41294\n",
      "[219]\tvalidation_0-mlogloss:1.41089\n",
      "[220]\tvalidation_0-mlogloss:1.40886\n",
      "[221]\tvalidation_0-mlogloss:1.40807\n",
      "[222]\tvalidation_0-mlogloss:1.40692\n",
      "[223]\tvalidation_0-mlogloss:1.40572\n",
      "[224]\tvalidation_0-mlogloss:1.40443\n",
      "[225]\tvalidation_0-mlogloss:1.40251\n",
      "[226]\tvalidation_0-mlogloss:1.40146\n",
      "[227]\tvalidation_0-mlogloss:1.40020\n",
      "[228]\tvalidation_0-mlogloss:1.39879\n",
      "[229]\tvalidation_0-mlogloss:1.39752\n",
      "[230]\tvalidation_0-mlogloss:1.39572\n",
      "[231]\tvalidation_0-mlogloss:1.39469\n",
      "[232]\tvalidation_0-mlogloss:1.39299\n",
      "[233]\tvalidation_0-mlogloss:1.39145\n",
      "[234]\tvalidation_0-mlogloss:1.39052\n",
      "[235]\tvalidation_0-mlogloss:1.38967\n",
      "[236]\tvalidation_0-mlogloss:1.38916\n",
      "[237]\tvalidation_0-mlogloss:1.38799\n",
      "[238]\tvalidation_0-mlogloss:1.38764\n",
      "[239]\tvalidation_0-mlogloss:1.38694\n",
      "[240]\tvalidation_0-mlogloss:1.38622\n",
      "[241]\tvalidation_0-mlogloss:1.38530\n",
      "[242]\tvalidation_0-mlogloss:1.38411\n",
      "[243]\tvalidation_0-mlogloss:1.38327\n",
      "[244]\tvalidation_0-mlogloss:1.38247\n",
      "[245]\tvalidation_0-mlogloss:1.38111\n",
      "[246]\tvalidation_0-mlogloss:1.38010\n",
      "[247]\tvalidation_0-mlogloss:1.37890\n",
      "[248]\tvalidation_0-mlogloss:1.37764\n",
      "[249]\tvalidation_0-mlogloss:1.37646\n",
      "[250]\tvalidation_0-mlogloss:1.37495\n",
      "[251]\tvalidation_0-mlogloss:1.37385\n",
      "[252]\tvalidation_0-mlogloss:1.37254\n",
      "[253]\tvalidation_0-mlogloss:1.37233\n",
      "[254]\tvalidation_0-mlogloss:1.37121\n",
      "[255]\tvalidation_0-mlogloss:1.37020\n",
      "[256]\tvalidation_0-mlogloss:1.36902\n",
      "[257]\tvalidation_0-mlogloss:1.36816\n",
      "[258]\tvalidation_0-mlogloss:1.36728\n",
      "[259]\tvalidation_0-mlogloss:1.36670\n",
      "[260]\tvalidation_0-mlogloss:1.36585\n",
      "[261]\tvalidation_0-mlogloss:1.36496\n",
      "[262]\tvalidation_0-mlogloss:1.36360\n",
      "[263]\tvalidation_0-mlogloss:1.36228\n",
      "[264]\tvalidation_0-mlogloss:1.36051\n",
      "[265]\tvalidation_0-mlogloss:1.36000\n",
      "[266]\tvalidation_0-mlogloss:1.35880\n",
      "[267]\tvalidation_0-mlogloss:1.35777\n",
      "[268]\tvalidation_0-mlogloss:1.35614\n",
      "[269]\tvalidation_0-mlogloss:1.35538\n",
      "[270]\tvalidation_0-mlogloss:1.35453\n",
      "[271]\tvalidation_0-mlogloss:1.35386\n",
      "[272]\tvalidation_0-mlogloss:1.35307\n",
      "[273]\tvalidation_0-mlogloss:1.35191\n",
      "[274]\tvalidation_0-mlogloss:1.35135\n",
      "[275]\tvalidation_0-mlogloss:1.35058\n",
      "[276]\tvalidation_0-mlogloss:1.34986\n",
      "[277]\tvalidation_0-mlogloss:1.34915\n",
      "[278]\tvalidation_0-mlogloss:1.34816\n",
      "[279]\tvalidation_0-mlogloss:1.34676\n",
      "[280]\tvalidation_0-mlogloss:1.34541\n",
      "[281]\tvalidation_0-mlogloss:1.34457\n",
      "[282]\tvalidation_0-mlogloss:1.34361\n",
      "[283]\tvalidation_0-mlogloss:1.34271\n",
      "[284]\tvalidation_0-mlogloss:1.34103\n",
      "[285]\tvalidation_0-mlogloss:1.34027\n",
      "[286]\tvalidation_0-mlogloss:1.34018\n",
      "[287]\tvalidation_0-mlogloss:1.33970\n",
      "[288]\tvalidation_0-mlogloss:1.33874\n",
      "[289]\tvalidation_0-mlogloss:1.33803\n",
      "[290]\tvalidation_0-mlogloss:1.33733\n",
      "[291]\tvalidation_0-mlogloss:1.33672\n",
      "[292]\tvalidation_0-mlogloss:1.33612\n",
      "[293]\tvalidation_0-mlogloss:1.33568\n",
      "[294]\tvalidation_0-mlogloss:1.33520\n",
      "[295]\tvalidation_0-mlogloss:1.33551\n",
      "[296]\tvalidation_0-mlogloss:1.33474\n",
      "[297]\tvalidation_0-mlogloss:1.33413\n",
      "[298]\tvalidation_0-mlogloss:1.33314\n",
      "[299]\tvalidation_0-mlogloss:1.33291\n",
      "[300]\tvalidation_0-mlogloss:1.33187\n",
      "[301]\tvalidation_0-mlogloss:1.33096\n",
      "[302]\tvalidation_0-mlogloss:1.33033\n",
      "[303]\tvalidation_0-mlogloss:1.32982\n",
      "[304]\tvalidation_0-mlogloss:1.32843\n",
      "[305]\tvalidation_0-mlogloss:1.32748\n",
      "[306]\tvalidation_0-mlogloss:1.32694\n",
      "[307]\tvalidation_0-mlogloss:1.32589\n",
      "[308]\tvalidation_0-mlogloss:1.32479\n",
      "[309]\tvalidation_0-mlogloss:1.32385\n",
      "[310]\tvalidation_0-mlogloss:1.32241\n",
      "[311]\tvalidation_0-mlogloss:1.32205\n",
      "[312]\tvalidation_0-mlogloss:1.32133\n",
      "[313]\tvalidation_0-mlogloss:1.32043\n",
      "[314]\tvalidation_0-mlogloss:1.31959\n",
      "[315]\tvalidation_0-mlogloss:1.31924\n",
      "[316]\tvalidation_0-mlogloss:1.31861\n",
      "[317]\tvalidation_0-mlogloss:1.31830\n",
      "[318]\tvalidation_0-mlogloss:1.31760\n",
      "[319]\tvalidation_0-mlogloss:1.31673\n",
      "[320]\tvalidation_0-mlogloss:1.31617\n",
      "[321]\tvalidation_0-mlogloss:1.31574\n",
      "[322]\tvalidation_0-mlogloss:1.31555\n",
      "[323]\tvalidation_0-mlogloss:1.31509\n",
      "[324]\tvalidation_0-mlogloss:1.31488\n",
      "[325]\tvalidation_0-mlogloss:1.31448\n",
      "[326]\tvalidation_0-mlogloss:1.31393\n",
      "[327]\tvalidation_0-mlogloss:1.31372\n",
      "[328]\tvalidation_0-mlogloss:1.31396\n",
      "[329]\tvalidation_0-mlogloss:1.31362\n",
      "[330]\tvalidation_0-mlogloss:1.31340\n",
      "[331]\tvalidation_0-mlogloss:1.31291\n",
      "[332]\tvalidation_0-mlogloss:1.31264\n",
      "[333]\tvalidation_0-mlogloss:1.31207\n",
      "[334]\tvalidation_0-mlogloss:1.31157\n",
      "[335]\tvalidation_0-mlogloss:1.31081\n",
      "[336]\tvalidation_0-mlogloss:1.31060\n",
      "[337]\tvalidation_0-mlogloss:1.31033\n",
      "[338]\tvalidation_0-mlogloss:1.30960\n",
      "[339]\tvalidation_0-mlogloss:1.30930\n",
      "[340]\tvalidation_0-mlogloss:1.30861\n",
      "[341]\tvalidation_0-mlogloss:1.30808\n",
      "[342]\tvalidation_0-mlogloss:1.30778\n",
      "[343]\tvalidation_0-mlogloss:1.30698\n",
      "[344]\tvalidation_0-mlogloss:1.30640\n",
      "[345]\tvalidation_0-mlogloss:1.30603\n",
      "[346]\tvalidation_0-mlogloss:1.30500\n",
      "[347]\tvalidation_0-mlogloss:1.30443\n",
      "[348]\tvalidation_0-mlogloss:1.30441\n",
      "[349]\tvalidation_0-mlogloss:1.30425\n",
      "[350]\tvalidation_0-mlogloss:1.30366\n",
      "[351]\tvalidation_0-mlogloss:1.30360\n",
      "[352]\tvalidation_0-mlogloss:1.30331\n",
      "[353]\tvalidation_0-mlogloss:1.30261\n",
      "[354]\tvalidation_0-mlogloss:1.30285\n",
      "[355]\tvalidation_0-mlogloss:1.30231\n",
      "[356]\tvalidation_0-mlogloss:1.30178\n",
      "[357]\tvalidation_0-mlogloss:1.30117\n",
      "[358]\tvalidation_0-mlogloss:1.30103\n",
      "[359]\tvalidation_0-mlogloss:1.30055\n",
      "[360]\tvalidation_0-mlogloss:1.30027\n",
      "[361]\tvalidation_0-mlogloss:1.29986\n",
      "[362]\tvalidation_0-mlogloss:1.29986\n",
      "[363]\tvalidation_0-mlogloss:1.29933\n",
      "[364]\tvalidation_0-mlogloss:1.29953\n",
      "[365]\tvalidation_0-mlogloss:1.29994\n",
      "[366]\tvalidation_0-mlogloss:1.30040\n",
      "[367]\tvalidation_0-mlogloss:1.29992\n",
      "[368]\tvalidation_0-mlogloss:1.29955\n",
      "[369]\tvalidation_0-mlogloss:1.29894\n",
      "[370]\tvalidation_0-mlogloss:1.29849\n",
      "[371]\tvalidation_0-mlogloss:1.29822\n",
      "[372]\tvalidation_0-mlogloss:1.29724\n",
      "[373]\tvalidation_0-mlogloss:1.29682\n",
      "[374]\tvalidation_0-mlogloss:1.29643\n",
      "[375]\tvalidation_0-mlogloss:1.29601\n",
      "[376]\tvalidation_0-mlogloss:1.29531\n",
      "[377]\tvalidation_0-mlogloss:1.29488\n",
      "[378]\tvalidation_0-mlogloss:1.29491\n",
      "[379]\tvalidation_0-mlogloss:1.29438\n",
      "[380]\tvalidation_0-mlogloss:1.29364\n",
      "[381]\tvalidation_0-mlogloss:1.29323\n",
      "[382]\tvalidation_0-mlogloss:1.29263\n",
      "[383]\tvalidation_0-mlogloss:1.29182\n",
      "[384]\tvalidation_0-mlogloss:1.29121\n",
      "[385]\tvalidation_0-mlogloss:1.29072\n",
      "[386]\tvalidation_0-mlogloss:1.29064\n",
      "[387]\tvalidation_0-mlogloss:1.29034\n",
      "[388]\tvalidation_0-mlogloss:1.28987\n",
      "[389]\tvalidation_0-mlogloss:1.28954\n",
      "[390]\tvalidation_0-mlogloss:1.28952\n",
      "[391]\tvalidation_0-mlogloss:1.28928\n",
      "[392]\tvalidation_0-mlogloss:1.28951\n",
      "[393]\tvalidation_0-mlogloss:1.28900\n",
      "[394]\tvalidation_0-mlogloss:1.28881\n",
      "[395]\tvalidation_0-mlogloss:1.28876\n",
      "[396]\tvalidation_0-mlogloss:1.28874\n",
      "[397]\tvalidation_0-mlogloss:1.28832\n",
      "[398]\tvalidation_0-mlogloss:1.28758\n",
      "[399]\tvalidation_0-mlogloss:1.28675\n",
      "[400]\tvalidation_0-mlogloss:1.28671\n",
      "[401]\tvalidation_0-mlogloss:1.28653\n",
      "[402]\tvalidation_0-mlogloss:1.28584\n",
      "[403]\tvalidation_0-mlogloss:1.28523\n",
      "[404]\tvalidation_0-mlogloss:1.28511\n",
      "[405]\tvalidation_0-mlogloss:1.28472\n",
      "[406]\tvalidation_0-mlogloss:1.28448\n",
      "[407]\tvalidation_0-mlogloss:1.28421\n",
      "[408]\tvalidation_0-mlogloss:1.28358\n",
      "[409]\tvalidation_0-mlogloss:1.28313\n",
      "[410]\tvalidation_0-mlogloss:1.28266\n",
      "[411]\tvalidation_0-mlogloss:1.28229\n",
      "[412]\tvalidation_0-mlogloss:1.28230\n",
      "[413]\tvalidation_0-mlogloss:1.28215\n",
      "[414]\tvalidation_0-mlogloss:1.28228\n",
      "[415]\tvalidation_0-mlogloss:1.28134\n",
      "[416]\tvalidation_0-mlogloss:1.28090\n",
      "[417]\tvalidation_0-mlogloss:1.28083\n",
      "[418]\tvalidation_0-mlogloss:1.28054\n",
      "[419]\tvalidation_0-mlogloss:1.28029\n",
      "[420]\tvalidation_0-mlogloss:1.28007\n",
      "[421]\tvalidation_0-mlogloss:1.27996\n",
      "[422]\tvalidation_0-mlogloss:1.28013\n",
      "[423]\tvalidation_0-mlogloss:1.28012\n",
      "[424]\tvalidation_0-mlogloss:1.27983\n",
      "[425]\tvalidation_0-mlogloss:1.27916\n",
      "[426]\tvalidation_0-mlogloss:1.27894\n",
      "[427]\tvalidation_0-mlogloss:1.27840\n",
      "[428]\tvalidation_0-mlogloss:1.27823\n",
      "[429]\tvalidation_0-mlogloss:1.27804\n",
      "[430]\tvalidation_0-mlogloss:1.27784\n",
      "[431]\tvalidation_0-mlogloss:1.27749\n",
      "[432]\tvalidation_0-mlogloss:1.27718\n",
      "[433]\tvalidation_0-mlogloss:1.27713\n",
      "[434]\tvalidation_0-mlogloss:1.27701\n",
      "[435]\tvalidation_0-mlogloss:1.27667\n",
      "[436]\tvalidation_0-mlogloss:1.27598\n",
      "[437]\tvalidation_0-mlogloss:1.27555\n",
      "[438]\tvalidation_0-mlogloss:1.27515\n",
      "[439]\tvalidation_0-mlogloss:1.27465\n",
      "[440]\tvalidation_0-mlogloss:1.27432\n",
      "[441]\tvalidation_0-mlogloss:1.27420\n",
      "[442]\tvalidation_0-mlogloss:1.27397\n",
      "[443]\tvalidation_0-mlogloss:1.27374\n",
      "[444]\tvalidation_0-mlogloss:1.27360\n",
      "[445]\tvalidation_0-mlogloss:1.27332\n",
      "[446]\tvalidation_0-mlogloss:1.27305\n",
      "[447]\tvalidation_0-mlogloss:1.27268\n",
      "[448]\tvalidation_0-mlogloss:1.27219\n",
      "[449]\tvalidation_0-mlogloss:1.27182\n",
      "[450]\tvalidation_0-mlogloss:1.27131\n",
      "[451]\tvalidation_0-mlogloss:1.27092\n",
      "[452]\tvalidation_0-mlogloss:1.27057\n",
      "[453]\tvalidation_0-mlogloss:1.27006\n",
      "[454]\tvalidation_0-mlogloss:1.27027\n",
      "[455]\tvalidation_0-mlogloss:1.27013\n",
      "[456]\tvalidation_0-mlogloss:1.26996\n",
      "[457]\tvalidation_0-mlogloss:1.26980\n",
      "[458]\tvalidation_0-mlogloss:1.26953\n",
      "[459]\tvalidation_0-mlogloss:1.26892\n",
      "[460]\tvalidation_0-mlogloss:1.26865\n",
      "[461]\tvalidation_0-mlogloss:1.26808\n",
      "[462]\tvalidation_0-mlogloss:1.26775\n",
      "[463]\tvalidation_0-mlogloss:1.26754\n",
      "[464]\tvalidation_0-mlogloss:1.26751\n",
      "[465]\tvalidation_0-mlogloss:1.26740\n",
      "[466]\tvalidation_0-mlogloss:1.26715\n",
      "[467]\tvalidation_0-mlogloss:1.26645\n",
      "[468]\tvalidation_0-mlogloss:1.26623\n",
      "[469]\tvalidation_0-mlogloss:1.26614\n",
      "[470]\tvalidation_0-mlogloss:1.26598\n",
      "[471]\tvalidation_0-mlogloss:1.26548\n",
      "[472]\tvalidation_0-mlogloss:1.26528\n",
      "[473]\tvalidation_0-mlogloss:1.26526\n",
      "[474]\tvalidation_0-mlogloss:1.26482\n",
      "[475]\tvalidation_0-mlogloss:1.26433\n",
      "[476]\tvalidation_0-mlogloss:1.26419\n",
      "[477]\tvalidation_0-mlogloss:1.26382\n",
      "[478]\tvalidation_0-mlogloss:1.26327\n",
      "[479]\tvalidation_0-mlogloss:1.26267\n",
      "[480]\tvalidation_0-mlogloss:1.26221\n",
      "[481]\tvalidation_0-mlogloss:1.26169\n",
      "[482]\tvalidation_0-mlogloss:1.26125\n",
      "[483]\tvalidation_0-mlogloss:1.26161\n",
      "[484]\tvalidation_0-mlogloss:1.26175\n",
      "[485]\tvalidation_0-mlogloss:1.26154\n",
      "[486]\tvalidation_0-mlogloss:1.26138\n",
      "[487]\tvalidation_0-mlogloss:1.26120\n",
      "[488]\tvalidation_0-mlogloss:1.26124\n",
      "[489]\tvalidation_0-mlogloss:1.26127\n",
      "[490]\tvalidation_0-mlogloss:1.26077\n",
      "[491]\tvalidation_0-mlogloss:1.26043\n",
      "[492]\tvalidation_0-mlogloss:1.26041\n",
      "[493]\tvalidation_0-mlogloss:1.26059\n",
      "[494]\tvalidation_0-mlogloss:1.26011\n",
      "[495]\tvalidation_0-mlogloss:1.26035\n",
      "[496]\tvalidation_0-mlogloss:1.26000\n",
      "[497]\tvalidation_0-mlogloss:1.25972\n",
      "[498]\tvalidation_0-mlogloss:1.25974\n",
      "[499]\tvalidation_0-mlogloss:1.25964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.62      0.57      1499\n",
      "           1       0.55      0.45      0.49      1499\n",
      "           2       0.74      0.79      0.76      1499\n",
      "           3       0.73      0.31      0.44      1499\n",
      "           4       0.92      0.95      0.94      1499\n",
      "           5       0.92      0.84      0.88      1499\n",
      "           6       0.87      0.93      0.90      1499\n",
      "           7       0.94      0.68      0.79      1499\n",
      "           8       0.66      0.84      0.74      1499\n",
      "           9       0.83      0.70      0.76      1499\n",
      "          10       0.94      0.93      0.93      1499\n",
      "          11       0.94      0.97      0.96      1499\n",
      "          12       0.74      0.90      0.81      1499\n",
      "          13       0.68      0.84      0.76      1499\n",
      "          14       0.48      0.55      0.51      1499\n",
      "          15       0.93      0.97      0.95      1499\n",
      "          16       0.83      0.60      0.69      1499\n",
      "          17       0.66      0.74      0.70      1499\n",
      "          18       0.62      0.87      0.72      1499\n",
      "          19       0.56      0.29      0.38      1499\n",
      "          20       0.88      0.97      0.92      1499\n",
      "          21       0.63      0.80      0.70      1499\n",
      "          22       0.62      0.71      0.66      1499\n",
      "          23       0.95      0.95      0.95      1499\n",
      "          24       0.97      0.91      0.94      1499\n",
      "          25       0.87      0.38      0.53      1499\n",
      "          26       0.71      0.49      0.58      1499\n",
      "          27       0.52      0.37      0.43      1499\n",
      "          28       0.65      0.68      0.66      1499\n",
      "          29       0.75      0.99      0.85      1499\n",
      "          30       0.60      0.68      0.64      1499\n",
      "          31       0.61      0.77      0.68      1499\n",
      "          32       0.64      0.87      0.74      1499\n",
      "          33       0.86      0.62      0.72      1499\n",
      "          34       0.97      0.96      0.97      1499\n",
      "          35       0.73      0.82      0.77      1499\n",
      "          36       0.85      0.93      0.89      1499\n",
      "          37       0.52      0.72      0.61      1499\n",
      "          38       0.67      0.49      0.57      1499\n",
      "          39       0.70      0.57      0.63      1499\n",
      "          40       0.94      0.80      0.86      1499\n",
      "          41       0.86      0.81      0.83      1499\n",
      "          42       0.63      0.41      0.50      1499\n",
      "          43       0.77      0.77      0.77      1499\n",
      "          44       0.73      0.86      0.79      1499\n",
      "          45       0.50      0.61      0.55      1499\n",
      "          46       0.51      0.73      0.60      1499\n",
      "          47       0.86      0.68      0.76      1499\n",
      "          48       0.81      0.92      0.86      1499\n",
      "          49       0.54      0.76      0.63      1499\n",
      "          50       0.70      0.64      0.67      1499\n",
      "          51       0.71      0.50      0.59      1499\n",
      "          52       0.68      0.78      0.72      1499\n",
      "          53       0.71      0.73      0.72      1499\n",
      "          54       0.60      0.60      0.60      1499\n",
      "          55       0.60      0.70      0.65      1499\n",
      "          56       0.55      0.41      0.47      1499\n",
      "          57       0.73      0.81      0.77      1499\n",
      "          58       0.99      0.98      0.98      1499\n",
      "          59       0.75      0.49      0.59      1499\n",
      "          60       0.70      0.59      0.64      1499\n",
      "          61       0.89      0.65      0.75      1499\n",
      "          62       0.89      0.92      0.90      1499\n",
      "          63       0.64      0.52      0.57      1499\n",
      "          64       0.52      0.38      0.44      1499\n",
      "          65       0.70      0.87      0.78      1499\n",
      "          66       0.74      0.78      0.76      1499\n",
      "          67       0.94      0.59      0.72      1499\n",
      "          68       0.72      0.86      0.78      1499\n",
      "          69       0.43      0.61      0.51      1499\n",
      "          70       0.86      0.40      0.55      1499\n",
      "          71       0.50      0.37      0.43      1499\n",
      "          72       0.76      0.87      0.81      1499\n",
      "          73       0.69      0.85      0.76      1499\n",
      "          74       0.37      0.61      0.46      1499\n",
      "\n",
      "    accuracy                           0.71    112425\n",
      "   macro avg       0.72      0.71      0.71    112425\n",
      "weighted avg       0.72      0.71      0.71    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300aa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baab7fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 27s 942us/step - loss: 1.4267 - val_loss: 1.8285\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 26s 930us/step - loss: 0.6363 - val_loss: 1.6128\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 27s 944us/step - loss: 0.4793 - val_loss: 1.7520\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 26s 940us/step - loss: 0.4106 - val_loss: 1.6017\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 27s 953us/step - loss: 0.3693 - val_loss: 1.5204\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 27s 947us/step - loss: 0.3410 - val_loss: 1.4673\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 29s 1ms/step - loss: 0.3202 - val_loss: 1.6347\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 28s 979us/step - loss: 0.3021 - val_loss: 1.8332\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 27s 974us/step - loss: 0.2882 - val_loss: 1.5334\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 27s 947us/step - loss: 0.2758 - val_loss: 1.5679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x30ffae620>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 1s 370us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.61      0.66      1499\n",
      "           1       0.69      0.44      0.54      1499\n",
      "           2       0.70      0.72      0.71      1499\n",
      "           3       0.65      0.44      0.52      1499\n",
      "           4       0.91      0.94      0.92      1499\n",
      "           5       0.96      0.87      0.91      1499\n",
      "           6       0.93      0.93      0.93      1499\n",
      "           7       0.96      0.79      0.87      1499\n",
      "           8       0.58      0.63      0.60      1499\n",
      "           9       0.78      0.66      0.71      1499\n",
      "          10       0.93      0.96      0.95      1499\n",
      "          11       0.87      0.98      0.93      1499\n",
      "          12       0.64      0.87      0.74      1499\n",
      "          13       0.70      0.74      0.72      1499\n",
      "          14       0.57      0.65      0.60      1499\n",
      "          15       0.97      0.86      0.91      1499\n",
      "          16       0.60      0.33      0.42      1499\n",
      "          17       0.83      0.68      0.75      1499\n",
      "          18       0.67      0.93      0.78      1499\n",
      "          19       0.57      0.49      0.53      1499\n",
      "          20       0.89      0.99      0.94      1499\n",
      "          21       0.63      0.86      0.73      1499\n",
      "          22       0.79      0.50      0.61      1499\n",
      "          23       0.91      0.96      0.94      1499\n",
      "          24       0.94      0.86      0.90      1499\n",
      "          25       0.94      0.26      0.41      1499\n",
      "          26       0.44      0.64      0.52      1499\n",
      "          27       0.35      0.36      0.36      1499\n",
      "          28       0.82      0.71      0.76      1499\n",
      "          29       0.67      1.00      0.80      1499\n",
      "          30       0.80      0.66      0.73      1499\n",
      "          31       0.77      0.79      0.78      1499\n",
      "          32       0.75      0.80      0.78      1499\n",
      "          33       0.85      0.56      0.68      1499\n",
      "          34       0.95      0.99      0.97      1499\n",
      "          35       0.69      0.76      0.72      1499\n",
      "          36       0.88      0.80      0.84      1499\n",
      "          37       0.61      0.71      0.65      1499\n",
      "          38       0.72      0.26      0.38      1499\n",
      "          39       0.58      0.51      0.55      1499\n",
      "          40       0.91      0.61      0.73      1499\n",
      "          41       0.80      0.85      0.82      1499\n",
      "          42       0.71      0.43      0.53      1499\n",
      "          43       0.78      0.69      0.73      1499\n",
      "          44       0.68      0.92      0.78      1499\n",
      "          45       0.48      0.80      0.60      1499\n",
      "          46       0.56      0.76      0.64      1499\n",
      "          47       0.68      0.70      0.69      1499\n",
      "          48       0.85      0.92      0.89      1499\n",
      "          49       0.59      0.79      0.68      1499\n",
      "          50       0.73      0.65      0.69      1499\n",
      "          51       0.53      0.48      0.50      1499\n",
      "          52       0.59      0.84      0.70      1499\n",
      "          53       0.86      0.68      0.76      1499\n",
      "          54       0.57      0.66      0.61      1499\n",
      "          55       0.55      0.80      0.65      1499\n",
      "          56       0.62      0.43      0.51      1499\n",
      "          57       0.74      0.81      0.77      1499\n",
      "          58       0.96      0.94      0.95      1499\n",
      "          59       0.91      0.57      0.70      1499\n",
      "          60       0.79      0.63      0.70      1499\n",
      "          61       0.86      0.61      0.71      1499\n",
      "          62       0.93      0.90      0.91      1499\n",
      "          63       0.56      0.42      0.48      1499\n",
      "          64       0.44      0.34      0.39      1499\n",
      "          65       0.81      0.79      0.80      1499\n",
      "          66       0.68      0.76      0.72      1499\n",
      "          67       0.66      0.74      0.70      1499\n",
      "          68       0.77      0.87      0.82      1499\n",
      "          69       0.49      0.69      0.58      1499\n",
      "          70       0.64      0.15      0.25      1499\n",
      "          71       0.42      0.62      0.50      1499\n",
      "          72       0.75      0.89      0.81      1499\n",
      "          73       0.59      0.72      0.65      1499\n",
      "          74       0.36      0.62      0.46      1499\n",
      "\n",
      "    accuracy                           0.70    112425\n",
      "   macro avg       0.72      0.70      0.70    112425\n",
      "weighted avg       0.72      0.70      0.70    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.57980\n",
      "[1]\tvalidation_0-mlogloss:3.30102\n",
      "[2]\tvalidation_0-mlogloss:3.11511\n",
      "[3]\tvalidation_0-mlogloss:2.96973\n",
      "[4]\tvalidation_0-mlogloss:2.84915\n",
      "[5]\tvalidation_0-mlogloss:2.74975\n",
      "[6]\tvalidation_0-mlogloss:2.66319\n",
      "[7]\tvalidation_0-mlogloss:2.58677\n",
      "[8]\tvalidation_0-mlogloss:2.52542\n",
      "[9]\tvalidation_0-mlogloss:2.46079\n",
      "[10]\tvalidation_0-mlogloss:2.40635\n",
      "[11]\tvalidation_0-mlogloss:2.35461\n",
      "[12]\tvalidation_0-mlogloss:2.31139\n",
      "[13]\tvalidation_0-mlogloss:2.27145\n",
      "[14]\tvalidation_0-mlogloss:2.23162\n",
      "[15]\tvalidation_0-mlogloss:2.18988\n",
      "[16]\tvalidation_0-mlogloss:2.15652\n",
      "[17]\tvalidation_0-mlogloss:2.12579\n",
      "[18]\tvalidation_0-mlogloss:2.09738\n",
      "[19]\tvalidation_0-mlogloss:2.06662\n",
      "[20]\tvalidation_0-mlogloss:2.04146\n",
      "[21]\tvalidation_0-mlogloss:2.01777\n",
      "[22]\tvalidation_0-mlogloss:1.99128\n",
      "[23]\tvalidation_0-mlogloss:1.96807\n",
      "[24]\tvalidation_0-mlogloss:1.94619\n",
      "[25]\tvalidation_0-mlogloss:1.92318\n",
      "[26]\tvalidation_0-mlogloss:1.90136\n",
      "[27]\tvalidation_0-mlogloss:1.88164\n",
      "[28]\tvalidation_0-mlogloss:1.86141\n",
      "[29]\tvalidation_0-mlogloss:1.84561\n",
      "[30]\tvalidation_0-mlogloss:1.82758\n",
      "[31]\tvalidation_0-mlogloss:1.81004\n",
      "[32]\tvalidation_0-mlogloss:1.79081\n",
      "[33]\tvalidation_0-mlogloss:1.77629\n",
      "[34]\tvalidation_0-mlogloss:1.75922\n",
      "[35]\tvalidation_0-mlogloss:1.74539\n",
      "[36]\tvalidation_0-mlogloss:1.73055\n",
      "[37]\tvalidation_0-mlogloss:1.71369\n",
      "[38]\tvalidation_0-mlogloss:1.69776\n",
      "[39]\tvalidation_0-mlogloss:1.68521\n",
      "[40]\tvalidation_0-mlogloss:1.67124\n",
      "[41]\tvalidation_0-mlogloss:1.65739\n",
      "[42]\tvalidation_0-mlogloss:1.64274\n",
      "[43]\tvalidation_0-mlogloss:1.63152\n",
      "[44]\tvalidation_0-mlogloss:1.62031\n",
      "[45]\tvalidation_0-mlogloss:1.60826\n",
      "[46]\tvalidation_0-mlogloss:1.59753\n",
      "[47]\tvalidation_0-mlogloss:1.58592\n",
      "[48]\tvalidation_0-mlogloss:1.57362\n",
      "[49]\tvalidation_0-mlogloss:1.55894\n",
      "[50]\tvalidation_0-mlogloss:1.54846\n",
      "[51]\tvalidation_0-mlogloss:1.53816\n",
      "[52]\tvalidation_0-mlogloss:1.52658\n",
      "[53]\tvalidation_0-mlogloss:1.51521\n",
      "[54]\tvalidation_0-mlogloss:1.50489\n",
      "[55]\tvalidation_0-mlogloss:1.49471\n",
      "[56]\tvalidation_0-mlogloss:1.48351\n",
      "[57]\tvalidation_0-mlogloss:1.47511\n",
      "[58]\tvalidation_0-mlogloss:1.46609\n",
      "[59]\tvalidation_0-mlogloss:1.45742\n",
      "[60]\tvalidation_0-mlogloss:1.44826\n",
      "[61]\tvalidation_0-mlogloss:1.43842\n",
      "[62]\tvalidation_0-mlogloss:1.42946\n",
      "[63]\tvalidation_0-mlogloss:1.42145\n",
      "[64]\tvalidation_0-mlogloss:1.41373\n",
      "[65]\tvalidation_0-mlogloss:1.40696\n",
      "[66]\tvalidation_0-mlogloss:1.39880\n",
      "[67]\tvalidation_0-mlogloss:1.39042\n",
      "[68]\tvalidation_0-mlogloss:1.38262\n",
      "[69]\tvalidation_0-mlogloss:1.37677\n",
      "[70]\tvalidation_0-mlogloss:1.36987\n",
      "[71]\tvalidation_0-mlogloss:1.36261\n",
      "[72]\tvalidation_0-mlogloss:1.35688\n",
      "[73]\tvalidation_0-mlogloss:1.35042\n",
      "[74]\tvalidation_0-mlogloss:1.34377\n",
      "[75]\tvalidation_0-mlogloss:1.33661\n",
      "[76]\tvalidation_0-mlogloss:1.32942\n",
      "[77]\tvalidation_0-mlogloss:1.32255\n",
      "[78]\tvalidation_0-mlogloss:1.31642\n",
      "[79]\tvalidation_0-mlogloss:1.31112\n",
      "[80]\tvalidation_0-mlogloss:1.30539\n",
      "[81]\tvalidation_0-mlogloss:1.29943\n",
      "[82]\tvalidation_0-mlogloss:1.29497\n",
      "[83]\tvalidation_0-mlogloss:1.29013\n",
      "[84]\tvalidation_0-mlogloss:1.28541\n",
      "[85]\tvalidation_0-mlogloss:1.27991\n",
      "[86]\tvalidation_0-mlogloss:1.27448\n",
      "[87]\tvalidation_0-mlogloss:1.26805\n",
      "[88]\tvalidation_0-mlogloss:1.26273\n",
      "[89]\tvalidation_0-mlogloss:1.25815\n",
      "[90]\tvalidation_0-mlogloss:1.25386\n",
      "[91]\tvalidation_0-mlogloss:1.24973\n",
      "[92]\tvalidation_0-mlogloss:1.24437\n",
      "[93]\tvalidation_0-mlogloss:1.23928\n",
      "[94]\tvalidation_0-mlogloss:1.23611\n",
      "[95]\tvalidation_0-mlogloss:1.23115\n",
      "[96]\tvalidation_0-mlogloss:1.22595\n",
      "[97]\tvalidation_0-mlogloss:1.22180\n",
      "[98]\tvalidation_0-mlogloss:1.21743\n",
      "[99]\tvalidation_0-mlogloss:1.21242\n",
      "[100]\tvalidation_0-mlogloss:1.20925\n",
      "[101]\tvalidation_0-mlogloss:1.20378\n",
      "[102]\tvalidation_0-mlogloss:1.19997\n",
      "[103]\tvalidation_0-mlogloss:1.19606\n",
      "[104]\tvalidation_0-mlogloss:1.19293\n",
      "[105]\tvalidation_0-mlogloss:1.18829\n",
      "[106]\tvalidation_0-mlogloss:1.18473\n",
      "[107]\tvalidation_0-mlogloss:1.18076\n",
      "[108]\tvalidation_0-mlogloss:1.17633\n",
      "[109]\tvalidation_0-mlogloss:1.17226\n",
      "[110]\tvalidation_0-mlogloss:1.16770\n",
      "[111]\tvalidation_0-mlogloss:1.16382\n",
      "[112]\tvalidation_0-mlogloss:1.15889\n",
      "[113]\tvalidation_0-mlogloss:1.15709\n",
      "[114]\tvalidation_0-mlogloss:1.15317\n",
      "[115]\tvalidation_0-mlogloss:1.14932\n",
      "[116]\tvalidation_0-mlogloss:1.14608\n",
      "[117]\tvalidation_0-mlogloss:1.14131\n",
      "[118]\tvalidation_0-mlogloss:1.13808\n",
      "[119]\tvalidation_0-mlogloss:1.13438\n",
      "[120]\tvalidation_0-mlogloss:1.13097\n",
      "[121]\tvalidation_0-mlogloss:1.12675\n",
      "[122]\tvalidation_0-mlogloss:1.12214\n",
      "[123]\tvalidation_0-mlogloss:1.11949\n",
      "[124]\tvalidation_0-mlogloss:1.11706\n",
      "[125]\tvalidation_0-mlogloss:1.11463\n",
      "[126]\tvalidation_0-mlogloss:1.11289\n",
      "[127]\tvalidation_0-mlogloss:1.11037\n",
      "[128]\tvalidation_0-mlogloss:1.10789\n",
      "[129]\tvalidation_0-mlogloss:1.10545\n",
      "[130]\tvalidation_0-mlogloss:1.10207\n",
      "[131]\tvalidation_0-mlogloss:1.09978\n",
      "[132]\tvalidation_0-mlogloss:1.09710\n",
      "[133]\tvalidation_0-mlogloss:1.09477\n",
      "[134]\tvalidation_0-mlogloss:1.09232\n",
      "[135]\tvalidation_0-mlogloss:1.08888\n",
      "[136]\tvalidation_0-mlogloss:1.08680\n",
      "[137]\tvalidation_0-mlogloss:1.08414\n",
      "[138]\tvalidation_0-mlogloss:1.08079\n",
      "[139]\tvalidation_0-mlogloss:1.07898\n",
      "[140]\tvalidation_0-mlogloss:1.07565\n",
      "[141]\tvalidation_0-mlogloss:1.07332\n",
      "[142]\tvalidation_0-mlogloss:1.07055\n",
      "[143]\tvalidation_0-mlogloss:1.06779\n",
      "[144]\tvalidation_0-mlogloss:1.06489\n",
      "[145]\tvalidation_0-mlogloss:1.06253\n",
      "[146]\tvalidation_0-mlogloss:1.06103\n",
      "[147]\tvalidation_0-mlogloss:1.05883\n",
      "[148]\tvalidation_0-mlogloss:1.05610\n",
      "[149]\tvalidation_0-mlogloss:1.05395\n",
      "[150]\tvalidation_0-mlogloss:1.05106\n",
      "[151]\tvalidation_0-mlogloss:1.04877\n",
      "[152]\tvalidation_0-mlogloss:1.04661\n",
      "[153]\tvalidation_0-mlogloss:1.04474\n",
      "[154]\tvalidation_0-mlogloss:1.04240\n",
      "[155]\tvalidation_0-mlogloss:1.04021\n",
      "[156]\tvalidation_0-mlogloss:1.03842\n",
      "[157]\tvalidation_0-mlogloss:1.03638\n",
      "[158]\tvalidation_0-mlogloss:1.03346\n",
      "[159]\tvalidation_0-mlogloss:1.03119\n",
      "[160]\tvalidation_0-mlogloss:1.02867\n",
      "[161]\tvalidation_0-mlogloss:1.02693\n",
      "[162]\tvalidation_0-mlogloss:1.02473\n",
      "[163]\tvalidation_0-mlogloss:1.02175\n",
      "[164]\tvalidation_0-mlogloss:1.02011\n",
      "[165]\tvalidation_0-mlogloss:1.01779\n",
      "[166]\tvalidation_0-mlogloss:1.01597\n",
      "[167]\tvalidation_0-mlogloss:1.01364\n",
      "[168]\tvalidation_0-mlogloss:1.01145\n",
      "[169]\tvalidation_0-mlogloss:1.00986\n",
      "[170]\tvalidation_0-mlogloss:1.00865\n",
      "[171]\tvalidation_0-mlogloss:1.00627\n",
      "[172]\tvalidation_0-mlogloss:1.00417\n",
      "[173]\tvalidation_0-mlogloss:1.00252\n",
      "[174]\tvalidation_0-mlogloss:1.00147\n",
      "[175]\tvalidation_0-mlogloss:0.99991\n",
      "[176]\tvalidation_0-mlogloss:0.99871\n",
      "[177]\tvalidation_0-mlogloss:0.99638\n",
      "[178]\tvalidation_0-mlogloss:0.99458\n",
      "[179]\tvalidation_0-mlogloss:0.99288\n",
      "[180]\tvalidation_0-mlogloss:0.99175\n",
      "[181]\tvalidation_0-mlogloss:0.99034\n",
      "[182]\tvalidation_0-mlogloss:0.98921\n",
      "[183]\tvalidation_0-mlogloss:0.98745\n",
      "[184]\tvalidation_0-mlogloss:0.98556\n",
      "[185]\tvalidation_0-mlogloss:0.98390\n",
      "[186]\tvalidation_0-mlogloss:0.98288\n",
      "[187]\tvalidation_0-mlogloss:0.98131\n",
      "[188]\tvalidation_0-mlogloss:0.97953\n",
      "[189]\tvalidation_0-mlogloss:0.97830\n",
      "[190]\tvalidation_0-mlogloss:0.97643\n",
      "[191]\tvalidation_0-mlogloss:0.97529\n",
      "[192]\tvalidation_0-mlogloss:0.97386\n",
      "[193]\tvalidation_0-mlogloss:0.97219\n",
      "[194]\tvalidation_0-mlogloss:0.96974\n",
      "[195]\tvalidation_0-mlogloss:0.96850\n",
      "[196]\tvalidation_0-mlogloss:0.96738\n",
      "[197]\tvalidation_0-mlogloss:0.96613\n",
      "[198]\tvalidation_0-mlogloss:0.96549\n",
      "[199]\tvalidation_0-mlogloss:0.96435\n",
      "[200]\tvalidation_0-mlogloss:0.96304\n",
      "[201]\tvalidation_0-mlogloss:0.96187\n",
      "[202]\tvalidation_0-mlogloss:0.96036\n",
      "[203]\tvalidation_0-mlogloss:0.95878\n",
      "[204]\tvalidation_0-mlogloss:0.95797\n",
      "[205]\tvalidation_0-mlogloss:0.95744\n",
      "[206]\tvalidation_0-mlogloss:0.95596\n",
      "[207]\tvalidation_0-mlogloss:0.95457\n",
      "[208]\tvalidation_0-mlogloss:0.95388\n",
      "[209]\tvalidation_0-mlogloss:0.95248\n",
      "[210]\tvalidation_0-mlogloss:0.95164\n",
      "[211]\tvalidation_0-mlogloss:0.95102\n",
      "[212]\tvalidation_0-mlogloss:0.95018\n",
      "[213]\tvalidation_0-mlogloss:0.94931\n",
      "[214]\tvalidation_0-mlogloss:0.94773\n",
      "[215]\tvalidation_0-mlogloss:0.94713\n",
      "[216]\tvalidation_0-mlogloss:0.94603\n",
      "[217]\tvalidation_0-mlogloss:0.94508\n",
      "[218]\tvalidation_0-mlogloss:0.94367\n",
      "[219]\tvalidation_0-mlogloss:0.94233\n",
      "[220]\tvalidation_0-mlogloss:0.94140\n",
      "[221]\tvalidation_0-mlogloss:0.94039\n",
      "[222]\tvalidation_0-mlogloss:0.94009\n",
      "[223]\tvalidation_0-mlogloss:0.93907\n",
      "[224]\tvalidation_0-mlogloss:0.93772\n",
      "[225]\tvalidation_0-mlogloss:0.93697\n",
      "[226]\tvalidation_0-mlogloss:0.93641\n",
      "[227]\tvalidation_0-mlogloss:0.93529\n",
      "[228]\tvalidation_0-mlogloss:0.93413\n",
      "[229]\tvalidation_0-mlogloss:0.93274\n",
      "[230]\tvalidation_0-mlogloss:0.93173\n",
      "[231]\tvalidation_0-mlogloss:0.93067\n",
      "[232]\tvalidation_0-mlogloss:0.92968\n",
      "[233]\tvalidation_0-mlogloss:0.92916\n",
      "[234]\tvalidation_0-mlogloss:0.92806\n",
      "[235]\tvalidation_0-mlogloss:0.92639\n",
      "[236]\tvalidation_0-mlogloss:0.92589\n",
      "[237]\tvalidation_0-mlogloss:0.92537\n",
      "[238]\tvalidation_0-mlogloss:0.92462\n",
      "[239]\tvalidation_0-mlogloss:0.92395\n",
      "[240]\tvalidation_0-mlogloss:0.92286\n",
      "[241]\tvalidation_0-mlogloss:0.92194\n",
      "[242]\tvalidation_0-mlogloss:0.92162\n",
      "[243]\tvalidation_0-mlogloss:0.92085\n",
      "[244]\tvalidation_0-mlogloss:0.91968\n",
      "[245]\tvalidation_0-mlogloss:0.91797\n",
      "[246]\tvalidation_0-mlogloss:0.91676\n",
      "[247]\tvalidation_0-mlogloss:0.91678\n",
      "[248]\tvalidation_0-mlogloss:0.91572\n",
      "[249]\tvalidation_0-mlogloss:0.91467\n",
      "[250]\tvalidation_0-mlogloss:0.91380\n",
      "[251]\tvalidation_0-mlogloss:0.91253\n",
      "[252]\tvalidation_0-mlogloss:0.91147\n",
      "[253]\tvalidation_0-mlogloss:0.91056\n",
      "[254]\tvalidation_0-mlogloss:0.90984\n",
      "[255]\tvalidation_0-mlogloss:0.90877\n",
      "[256]\tvalidation_0-mlogloss:0.90753\n",
      "[257]\tvalidation_0-mlogloss:0.90680\n",
      "[258]\tvalidation_0-mlogloss:0.90556\n",
      "[259]\tvalidation_0-mlogloss:0.90448\n",
      "[260]\tvalidation_0-mlogloss:0.90341\n",
      "[261]\tvalidation_0-mlogloss:0.90230\n",
      "[262]\tvalidation_0-mlogloss:0.90157\n",
      "[263]\tvalidation_0-mlogloss:0.90126\n",
      "[264]\tvalidation_0-mlogloss:0.90031\n",
      "[265]\tvalidation_0-mlogloss:0.90021\n",
      "[266]\tvalidation_0-mlogloss:0.89998\n",
      "[267]\tvalidation_0-mlogloss:0.89908\n",
      "[268]\tvalidation_0-mlogloss:0.89815\n",
      "[269]\tvalidation_0-mlogloss:0.89818\n",
      "[270]\tvalidation_0-mlogloss:0.89748\n",
      "[271]\tvalidation_0-mlogloss:0.89644\n",
      "[272]\tvalidation_0-mlogloss:0.89565\n",
      "[273]\tvalidation_0-mlogloss:0.89490\n",
      "[274]\tvalidation_0-mlogloss:0.89378\n",
      "[275]\tvalidation_0-mlogloss:0.89289\n",
      "[276]\tvalidation_0-mlogloss:0.89245\n",
      "[277]\tvalidation_0-mlogloss:0.89177\n",
      "[278]\tvalidation_0-mlogloss:0.89120\n",
      "[279]\tvalidation_0-mlogloss:0.89019\n",
      "[280]\tvalidation_0-mlogloss:0.88978\n",
      "[281]\tvalidation_0-mlogloss:0.88841\n",
      "[282]\tvalidation_0-mlogloss:0.88743\n",
      "[283]\tvalidation_0-mlogloss:0.88647\n",
      "[284]\tvalidation_0-mlogloss:0.88617\n",
      "[285]\tvalidation_0-mlogloss:0.88617\n",
      "[286]\tvalidation_0-mlogloss:0.88589\n",
      "[287]\tvalidation_0-mlogloss:0.88533\n",
      "[288]\tvalidation_0-mlogloss:0.88462\n",
      "[289]\tvalidation_0-mlogloss:0.88403\n",
      "[290]\tvalidation_0-mlogloss:0.88323\n",
      "[291]\tvalidation_0-mlogloss:0.88260\n",
      "[292]\tvalidation_0-mlogloss:0.88221\n",
      "[293]\tvalidation_0-mlogloss:0.88172\n",
      "[294]\tvalidation_0-mlogloss:0.88139\n",
      "[295]\tvalidation_0-mlogloss:0.88068\n",
      "[296]\tvalidation_0-mlogloss:0.88010\n",
      "[297]\tvalidation_0-mlogloss:0.87997\n",
      "[298]\tvalidation_0-mlogloss:0.87935\n",
      "[299]\tvalidation_0-mlogloss:0.87878\n",
      "[300]\tvalidation_0-mlogloss:0.87835\n",
      "[301]\tvalidation_0-mlogloss:0.87728\n",
      "[302]\tvalidation_0-mlogloss:0.87686\n",
      "[303]\tvalidation_0-mlogloss:0.87597\n",
      "[304]\tvalidation_0-mlogloss:0.87551\n",
      "[305]\tvalidation_0-mlogloss:0.87518\n",
      "[306]\tvalidation_0-mlogloss:0.87419\n",
      "[307]\tvalidation_0-mlogloss:0.87386\n",
      "[308]\tvalidation_0-mlogloss:0.87340\n",
      "[309]\tvalidation_0-mlogloss:0.87231\n",
      "[310]\tvalidation_0-mlogloss:0.87184\n",
      "[311]\tvalidation_0-mlogloss:0.87143\n",
      "[312]\tvalidation_0-mlogloss:0.87098\n",
      "[313]\tvalidation_0-mlogloss:0.87061\n",
      "[314]\tvalidation_0-mlogloss:0.87075\n",
      "[315]\tvalidation_0-mlogloss:0.87041\n",
      "[316]\tvalidation_0-mlogloss:0.86996\n",
      "[317]\tvalidation_0-mlogloss:0.86960\n",
      "[318]\tvalidation_0-mlogloss:0.86929\n",
      "[319]\tvalidation_0-mlogloss:0.86858\n",
      "[320]\tvalidation_0-mlogloss:0.86799\n",
      "[321]\tvalidation_0-mlogloss:0.86740\n",
      "[322]\tvalidation_0-mlogloss:0.86657\n",
      "[323]\tvalidation_0-mlogloss:0.86587\n",
      "[324]\tvalidation_0-mlogloss:0.86530\n",
      "[325]\tvalidation_0-mlogloss:0.86481\n",
      "[326]\tvalidation_0-mlogloss:0.86437\n",
      "[327]\tvalidation_0-mlogloss:0.86393\n",
      "[328]\tvalidation_0-mlogloss:0.86383\n",
      "[329]\tvalidation_0-mlogloss:0.86365\n",
      "[330]\tvalidation_0-mlogloss:0.86338\n",
      "[331]\tvalidation_0-mlogloss:0.86292\n",
      "[332]\tvalidation_0-mlogloss:0.86234\n",
      "[333]\tvalidation_0-mlogloss:0.86151\n",
      "[334]\tvalidation_0-mlogloss:0.86111\n",
      "[335]\tvalidation_0-mlogloss:0.86060\n",
      "[336]\tvalidation_0-mlogloss:0.85997\n",
      "[337]\tvalidation_0-mlogloss:0.85943\n",
      "[338]\tvalidation_0-mlogloss:0.85879\n",
      "[339]\tvalidation_0-mlogloss:0.85804\n",
      "[340]\tvalidation_0-mlogloss:0.85751\n",
      "[341]\tvalidation_0-mlogloss:0.85679\n",
      "[342]\tvalidation_0-mlogloss:0.85634\n",
      "[343]\tvalidation_0-mlogloss:0.85594\n",
      "[344]\tvalidation_0-mlogloss:0.85550\n",
      "[345]\tvalidation_0-mlogloss:0.85535\n",
      "[346]\tvalidation_0-mlogloss:0.85502\n",
      "[347]\tvalidation_0-mlogloss:0.85436\n",
      "[348]\tvalidation_0-mlogloss:0.85392\n",
      "[349]\tvalidation_0-mlogloss:0.85302\n",
      "[350]\tvalidation_0-mlogloss:0.85235\n",
      "[351]\tvalidation_0-mlogloss:0.85158\n",
      "[352]\tvalidation_0-mlogloss:0.85115\n",
      "[353]\tvalidation_0-mlogloss:0.85072\n",
      "[354]\tvalidation_0-mlogloss:0.84994\n",
      "[355]\tvalidation_0-mlogloss:0.84950\n",
      "[356]\tvalidation_0-mlogloss:0.84895\n",
      "[357]\tvalidation_0-mlogloss:0.84837\n",
      "[358]\tvalidation_0-mlogloss:0.84779\n",
      "[359]\tvalidation_0-mlogloss:0.84764\n",
      "[360]\tvalidation_0-mlogloss:0.84768\n",
      "[361]\tvalidation_0-mlogloss:0.84731\n",
      "[362]\tvalidation_0-mlogloss:0.84679\n",
      "[363]\tvalidation_0-mlogloss:0.84621\n",
      "[364]\tvalidation_0-mlogloss:0.84570\n",
      "[365]\tvalidation_0-mlogloss:0.84549\n",
      "[366]\tvalidation_0-mlogloss:0.84510\n",
      "[367]\tvalidation_0-mlogloss:0.84463\n",
      "[368]\tvalidation_0-mlogloss:0.84432\n",
      "[369]\tvalidation_0-mlogloss:0.84403\n",
      "[370]\tvalidation_0-mlogloss:0.84367\n",
      "[371]\tvalidation_0-mlogloss:0.84327\n",
      "[372]\tvalidation_0-mlogloss:0.84323\n",
      "[373]\tvalidation_0-mlogloss:0.84257\n",
      "[374]\tvalidation_0-mlogloss:0.84171\n",
      "[375]\tvalidation_0-mlogloss:0.84148\n",
      "[376]\tvalidation_0-mlogloss:0.84127\n",
      "[377]\tvalidation_0-mlogloss:0.84084\n",
      "[378]\tvalidation_0-mlogloss:0.84036\n",
      "[379]\tvalidation_0-mlogloss:0.84010\n",
      "[380]\tvalidation_0-mlogloss:0.84005\n",
      "[381]\tvalidation_0-mlogloss:0.83957\n",
      "[382]\tvalidation_0-mlogloss:0.83921\n",
      "[383]\tvalidation_0-mlogloss:0.83896\n",
      "[384]\tvalidation_0-mlogloss:0.83861\n",
      "[385]\tvalidation_0-mlogloss:0.83816\n",
      "[386]\tvalidation_0-mlogloss:0.83782\n",
      "[387]\tvalidation_0-mlogloss:0.83781\n",
      "[388]\tvalidation_0-mlogloss:0.83757\n",
      "[389]\tvalidation_0-mlogloss:0.83721\n",
      "[390]\tvalidation_0-mlogloss:0.83679\n",
      "[391]\tvalidation_0-mlogloss:0.83660\n",
      "[392]\tvalidation_0-mlogloss:0.83640\n",
      "[393]\tvalidation_0-mlogloss:0.83581\n",
      "[394]\tvalidation_0-mlogloss:0.83540\n",
      "[395]\tvalidation_0-mlogloss:0.83473\n",
      "[396]\tvalidation_0-mlogloss:0.83446\n",
      "[397]\tvalidation_0-mlogloss:0.83432\n",
      "[398]\tvalidation_0-mlogloss:0.83373\n",
      "[399]\tvalidation_0-mlogloss:0.83344\n",
      "[400]\tvalidation_0-mlogloss:0.83340\n",
      "[401]\tvalidation_0-mlogloss:0.83297\n",
      "[402]\tvalidation_0-mlogloss:0.83246\n",
      "[403]\tvalidation_0-mlogloss:0.83221\n",
      "[404]\tvalidation_0-mlogloss:0.83215\n",
      "[405]\tvalidation_0-mlogloss:0.83186\n",
      "[406]\tvalidation_0-mlogloss:0.83180\n",
      "[407]\tvalidation_0-mlogloss:0.83149\n",
      "[408]\tvalidation_0-mlogloss:0.83094\n",
      "[409]\tvalidation_0-mlogloss:0.83102\n",
      "[410]\tvalidation_0-mlogloss:0.83087\n",
      "[411]\tvalidation_0-mlogloss:0.83080\n",
      "[412]\tvalidation_0-mlogloss:0.83036\n",
      "[413]\tvalidation_0-mlogloss:0.82995\n",
      "[414]\tvalidation_0-mlogloss:0.82922\n",
      "[415]\tvalidation_0-mlogloss:0.82893\n",
      "[416]\tvalidation_0-mlogloss:0.82826\n",
      "[417]\tvalidation_0-mlogloss:0.82765\n",
      "[418]\tvalidation_0-mlogloss:0.82737\n",
      "[419]\tvalidation_0-mlogloss:0.82727\n",
      "[420]\tvalidation_0-mlogloss:0.82690\n",
      "[421]\tvalidation_0-mlogloss:0.82678\n",
      "[422]\tvalidation_0-mlogloss:0.82651\n",
      "[423]\tvalidation_0-mlogloss:0.82620\n",
      "[424]\tvalidation_0-mlogloss:0.82604\n",
      "[425]\tvalidation_0-mlogloss:0.82585\n",
      "[426]\tvalidation_0-mlogloss:0.82558\n",
      "[427]\tvalidation_0-mlogloss:0.82583\n",
      "[428]\tvalidation_0-mlogloss:0.82576\n",
      "[429]\tvalidation_0-mlogloss:0.82578\n",
      "[430]\tvalidation_0-mlogloss:0.82538\n",
      "[431]\tvalidation_0-mlogloss:0.82496\n",
      "[432]\tvalidation_0-mlogloss:0.82487\n",
      "[433]\tvalidation_0-mlogloss:0.82465\n",
      "[434]\tvalidation_0-mlogloss:0.82400\n",
      "[435]\tvalidation_0-mlogloss:0.82395\n",
      "[436]\tvalidation_0-mlogloss:0.82351\n",
      "[437]\tvalidation_0-mlogloss:0.82341\n",
      "[438]\tvalidation_0-mlogloss:0.82313\n",
      "[439]\tvalidation_0-mlogloss:0.82268\n",
      "[440]\tvalidation_0-mlogloss:0.82240\n",
      "[441]\tvalidation_0-mlogloss:0.82207\n",
      "[442]\tvalidation_0-mlogloss:0.82180\n",
      "[443]\tvalidation_0-mlogloss:0.82182\n",
      "[444]\tvalidation_0-mlogloss:0.82160\n",
      "[445]\tvalidation_0-mlogloss:0.82148\n",
      "[446]\tvalidation_0-mlogloss:0.82113\n",
      "[447]\tvalidation_0-mlogloss:0.82059\n",
      "[448]\tvalidation_0-mlogloss:0.82038\n",
      "[449]\tvalidation_0-mlogloss:0.82004\n",
      "[450]\tvalidation_0-mlogloss:0.81990\n",
      "[451]\tvalidation_0-mlogloss:0.81968\n",
      "[452]\tvalidation_0-mlogloss:0.81947\n",
      "[453]\tvalidation_0-mlogloss:0.81943\n",
      "[454]\tvalidation_0-mlogloss:0.81907\n",
      "[455]\tvalidation_0-mlogloss:0.81888\n",
      "[456]\tvalidation_0-mlogloss:0.81853\n",
      "[457]\tvalidation_0-mlogloss:0.81812\n",
      "[458]\tvalidation_0-mlogloss:0.81786\n",
      "[459]\tvalidation_0-mlogloss:0.81751\n",
      "[460]\tvalidation_0-mlogloss:0.81746\n",
      "[461]\tvalidation_0-mlogloss:0.81725\n",
      "[462]\tvalidation_0-mlogloss:0.81696\n",
      "[463]\tvalidation_0-mlogloss:0.81668\n",
      "[464]\tvalidation_0-mlogloss:0.81616\n",
      "[465]\tvalidation_0-mlogloss:0.81609\n",
      "[466]\tvalidation_0-mlogloss:0.81580\n",
      "[467]\tvalidation_0-mlogloss:0.81575\n",
      "[468]\tvalidation_0-mlogloss:0.81571\n",
      "[469]\tvalidation_0-mlogloss:0.81559\n",
      "[470]\tvalidation_0-mlogloss:0.81541\n",
      "[471]\tvalidation_0-mlogloss:0.81549\n",
      "[472]\tvalidation_0-mlogloss:0.81518\n",
      "[473]\tvalidation_0-mlogloss:0.81490\n",
      "[474]\tvalidation_0-mlogloss:0.81463\n",
      "[475]\tvalidation_0-mlogloss:0.81476\n",
      "[476]\tvalidation_0-mlogloss:0.81472\n",
      "[477]\tvalidation_0-mlogloss:0.81453\n",
      "[478]\tvalidation_0-mlogloss:0.81457\n",
      "[479]\tvalidation_0-mlogloss:0.81428\n",
      "[480]\tvalidation_0-mlogloss:0.81413\n",
      "[481]\tvalidation_0-mlogloss:0.81357\n",
      "[482]\tvalidation_0-mlogloss:0.81326\n",
      "[483]\tvalidation_0-mlogloss:0.81297\n",
      "[484]\tvalidation_0-mlogloss:0.81254\n",
      "[485]\tvalidation_0-mlogloss:0.81238\n",
      "[486]\tvalidation_0-mlogloss:0.81208\n",
      "[487]\tvalidation_0-mlogloss:0.81170\n",
      "[488]\tvalidation_0-mlogloss:0.81165\n",
      "[489]\tvalidation_0-mlogloss:0.81152\n",
      "[490]\tvalidation_0-mlogloss:0.81168\n",
      "[491]\tvalidation_0-mlogloss:0.81154\n",
      "[492]\tvalidation_0-mlogloss:0.81149\n",
      "[493]\tvalidation_0-mlogloss:0.81128\n",
      "[494]\tvalidation_0-mlogloss:0.81103\n",
      "[495]\tvalidation_0-mlogloss:0.81074\n",
      "[496]\tvalidation_0-mlogloss:0.81070\n",
      "[497]\tvalidation_0-mlogloss:0.81041\n",
      "[498]\tvalidation_0-mlogloss:0.81031\n",
      "[499]\tvalidation_0-mlogloss:0.81019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81      1499\n",
      "           1       0.95      0.86      0.90      1499\n",
      "           2       0.83      0.91      0.87      1499\n",
      "           3       0.94      0.91      0.93      1499\n",
      "           4       0.99      0.99      0.99      1499\n",
      "           5       0.98      0.98      0.98      1499\n",
      "           6       0.94      0.96      0.95      1499\n",
      "           7       0.94      0.86      0.90      1499\n",
      "           8       0.76      0.90      0.82      1499\n",
      "           9       0.86      0.61      0.71      1499\n",
      "          10       0.91      0.92      0.92      1499\n",
      "          11       0.98      0.99      0.98      1499\n",
      "          12       0.84      0.95      0.89      1499\n",
      "          13       0.85      0.91      0.88      1499\n",
      "          14       0.78      0.86      0.82      1499\n",
      "          15       0.96      0.98      0.97      1499\n",
      "          16       0.87      0.85      0.86      1499\n",
      "          17       0.74      0.91      0.82      1499\n",
      "          18       0.82      0.91      0.86      1499\n",
      "          19       0.85      0.43      0.57      1499\n",
      "          20       0.97      0.99      0.98      1499\n",
      "          21       0.92      0.93      0.92      1499\n",
      "          22       0.76      0.83      0.79      1499\n",
      "          23       0.98      0.99      0.99      1499\n",
      "          24       0.97      0.98      0.98      1499\n",
      "          25       0.92      0.48      0.63      1499\n",
      "          26       0.90      0.57      0.70      1499\n",
      "          27       0.66      0.54      0.59      1499\n",
      "          28       0.66      0.86      0.75      1499\n",
      "          29       0.83      1.00      0.91      1499\n",
      "          30       0.85      0.89      0.87      1499\n",
      "          31       0.78      0.94      0.85      1499\n",
      "          32       0.79      0.96      0.86      1499\n",
      "          33       0.91      0.76      0.83      1499\n",
      "          34       0.99      0.94      0.97      1499\n",
      "          35       0.79      0.90      0.84      1499\n",
      "          36       0.70      0.98      0.82      1499\n",
      "          37       0.77      0.86      0.81      1499\n",
      "          38       0.63      0.49      0.55      1499\n",
      "          39       0.82      0.71      0.76      1499\n",
      "          40       0.97      0.89      0.93      1499\n",
      "          41       0.94      0.91      0.92      1499\n",
      "          42       0.86      0.78      0.81      1499\n",
      "          43       0.91      0.88      0.89      1499\n",
      "          44       0.91      0.98      0.95      1499\n",
      "          45       0.66      0.82      0.73      1499\n",
      "          46       0.63      0.57      0.59      1499\n",
      "          47       0.89      0.67      0.77      1499\n",
      "          48       0.86      0.95      0.90      1499\n",
      "          49       0.65      0.93      0.77      1499\n",
      "          50       0.75      0.72      0.73      1499\n",
      "          51       0.81      0.63      0.71      1499\n",
      "          52       0.82      0.93      0.87      1499\n",
      "          53       0.77      0.87      0.82      1499\n",
      "          54       0.73      0.74      0.74      1499\n",
      "          55       0.86      0.89      0.87      1499\n",
      "          56       0.74      0.63      0.68      1499\n",
      "          57       0.95      0.97      0.96      1499\n",
      "          58       0.99      0.95      0.97      1499\n",
      "          59       0.83      0.66      0.73      1499\n",
      "          60       0.81      0.76      0.78      1499\n",
      "          61       0.95      0.82      0.88      1499\n",
      "          62       0.93      0.98      0.95      1499\n",
      "          63       0.88      0.66      0.75      1499\n",
      "          64       0.80      0.72      0.76      1499\n",
      "          65       0.91      0.99      0.94      1499\n",
      "          66       0.90      0.91      0.91      1499\n",
      "          67       0.89      0.54      0.67      1499\n",
      "          68       0.89      0.96      0.92      1499\n",
      "          69       0.70      0.76      0.73      1499\n",
      "          70       0.85      0.56      0.68      1499\n",
      "          71       0.72      0.58      0.64      1499\n",
      "          72       0.87      0.96      0.91      1499\n",
      "          73       0.87      0.99      0.93      1499\n",
      "          74       0.46      0.75      0.57      1499\n",
      "\n",
      "    accuracy                           0.83    112425\n",
      "   macro avg       0.84      0.83      0.83    112425\n",
      "weighted avg       0.84      0.83      0.83    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3361defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(75, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a896a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 28s 991us/step - loss: 1.0268 - val_loss: 2.0086\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 28s 982us/step - loss: 0.4124 - val_loss: 2.0323\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 27s 966us/step - loss: 0.2943 - val_loss: 1.7633\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 28s 995us/step - loss: 0.2326 - val_loss: 1.6273\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 28s 995us/step - loss: 0.1949 - val_loss: 1.5107\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 28s 988us/step - loss: 0.1687 - val_loss: 1.5476\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 28s 1ms/step - loss: 0.1511 - val_loss: 1.5377\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 29s 1ms/step - loss: 0.1364 - val_loss: 1.6553\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 28s 1ms/step - loss: 0.1251 - val_loss: 1.3805\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 28s 991us/step - loss: 0.1167 - val_loss: 1.4666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x338c1b580>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 1s 385us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.70      0.76      1499\n",
      "           1       0.93      0.81      0.86      1499\n",
      "           2       0.82      0.87      0.84      1499\n",
      "           3       0.91      0.83      0.87      1499\n",
      "           4       1.00      0.98      0.99      1499\n",
      "           5       0.88      0.94      0.90      1499\n",
      "           6       0.88      0.98      0.93      1499\n",
      "           7       0.91      0.89      0.90      1499\n",
      "           8       0.75      0.84      0.79      1499\n",
      "           9       0.70      0.61      0.65      1499\n",
      "          10       0.87      0.71      0.78      1499\n",
      "          11       0.96      0.94      0.95      1499\n",
      "          12       0.88      0.95      0.91      1499\n",
      "          13       0.89      0.85      0.87      1499\n",
      "          14       0.62      0.91      0.74      1499\n",
      "          15       0.93      0.99      0.96      1499\n",
      "          16       0.93      0.77      0.84      1499\n",
      "          17       0.85      0.78      0.81      1499\n",
      "          18       0.83      0.80      0.82      1499\n",
      "          19       0.56      0.39      0.46      1499\n",
      "          20       0.97      0.99      0.98      1499\n",
      "          21       0.91      0.91      0.91      1499\n",
      "          22       0.70      0.70      0.70      1499\n",
      "          23       0.90      1.00      0.95      1499\n",
      "          24       0.95      0.95      0.95      1499\n",
      "          25       0.80      0.50      0.62      1499\n",
      "          26       0.70      0.64      0.67      1499\n",
      "          27       0.71      0.53      0.61      1499\n",
      "          28       0.79      0.85      0.81      1499\n",
      "          29       0.95      0.97      0.96      1499\n",
      "          30       0.87      0.94      0.90      1499\n",
      "          31       0.79      0.94      0.86      1499\n",
      "          32       0.87      0.86      0.87      1499\n",
      "          33       0.75      0.64      0.69      1499\n",
      "          34       0.99      1.00      0.99      1499\n",
      "          35       0.79      0.85      0.82      1499\n",
      "          36       0.66      0.77      0.71      1499\n",
      "          37       0.81      0.80      0.81      1499\n",
      "          38       0.52      0.32      0.39      1499\n",
      "          39       0.68      0.68      0.68      1499\n",
      "          40       0.93      0.79      0.85      1499\n",
      "          41       0.75      0.87      0.81      1499\n",
      "          42       0.57      0.75      0.65      1499\n",
      "          43       0.87      0.88      0.88      1499\n",
      "          44       0.90      0.95      0.92      1499\n",
      "          45       0.59      0.77      0.67      1499\n",
      "          46       0.26      0.34      0.30      1499\n",
      "          47       0.96      0.77      0.86      1499\n",
      "          48       0.78      0.93      0.85      1499\n",
      "          49       0.87      0.80      0.83      1499\n",
      "          50       0.61      0.78      0.68      1499\n",
      "          51       0.77      0.59      0.67      1499\n",
      "          52       0.80      0.92      0.85      1499\n",
      "          53       0.95      0.70      0.81      1499\n",
      "          54       0.83      0.58      0.69      1499\n",
      "          55       0.82      0.86      0.84      1499\n",
      "          56       0.74      0.75      0.75      1499\n",
      "          57       0.87      0.95      0.91      1499\n",
      "          58       0.77      0.99      0.87      1499\n",
      "          59       0.83      0.83      0.83      1499\n",
      "          60       0.68      0.70      0.69      1499\n",
      "          61       0.91      0.74      0.82      1499\n",
      "          62       0.96      0.96      0.96      1499\n",
      "          63       0.93      0.51      0.66      1499\n",
      "          64       0.59      0.74      0.65      1499\n",
      "          65       0.96      0.95      0.96      1499\n",
      "          66       0.83      0.79      0.81      1499\n",
      "          67       0.87      0.63      0.73      1499\n",
      "          68       0.76      0.92      0.84      1499\n",
      "          69       0.77      0.81      0.79      1499\n",
      "          70       0.90      0.34      0.50      1499\n",
      "          71       0.70      0.64      0.67      1499\n",
      "          72       0.98      0.69      0.81      1499\n",
      "          73       0.55      0.83      0.66      1499\n",
      "          74       0.44      0.79      0.57      1499\n",
      "\n",
      "    accuracy                           0.79    112425\n",
      "   macro avg       0.80      0.79      0.79    112425\n",
      "weighted avg       0.80      0.79      0.79    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753946a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
